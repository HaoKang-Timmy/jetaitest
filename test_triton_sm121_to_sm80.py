#!/usr/bin/env python3
"""
æµ‹è¯•tritonåœ¨sm_121æ˜ å°„åˆ°sm_80åæ˜¯å¦èƒ½æ­£å¸¸å·¥ä½œ
"""
import torch
import triton
import triton.language as tl


@triton.jit
def add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):
    """ç®€å•çš„å‘é‡åŠ æ³•kernel"""
    pid = tl.program_id(axis=0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements
    x = tl.load(x_ptr + offsets, mask=mask)
    y = tl.load(y_ptr + offsets, mask=mask)
    output = x + y
    tl.store(output_ptr + offsets, output, mask=mask)


def test_simple_kernel():
    """æµ‹è¯•ç®€å•çš„triton kernel"""
    print("=" * 60)
    print("æµ‹è¯• Triton Kernel: sm_121 -> sm_80 æ¶æ„æ˜ å°„")
    print("=" * 60)
    
    # æ£€æŸ¥è®¾å¤‡ä¿¡æ¯
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨")
        return False
    
    cap = torch.cuda.get_device_capability(0)
    print(f"\nGPUä¿¡æ¯:")
    print(f"  è®¾å¤‡åç§°: {torch.cuda.get_device_name(0)}")
    print(f"  åŸå§‹ Compute Capability: {cap[0]}.{cap[1]} (sm_{cap[0]*10+cap[1]})")
    
    # æ£€æŸ¥triton target
    from triton.runtime import driver as rt_driver
    driver = rt_driver.active
    target = driver.get_current_target()
    print(f"\nTriton Target:")
    print(f"  Backend: {target.backend}")
    print(f"  Architecture: sm_{target.arch} (æ˜ å°„å)")
    print(f"  Warp Size: {target.warp_size}")
    
    if cap[0]*10 + cap[1] == 121 and target.arch == 80:
        print(f"\nâœ… æ¶æ„æ˜ å°„æˆåŠŸ: sm_121 -> sm_80")
    
    # æµ‹è¯•kernel
    print(f"\n{'='*60}")
    print("è¿è¡Œ Triton Kernel æµ‹è¯•...")
    print(f"{'='*60}")
    
    try:
        size = 10240
        x = torch.randn(size, device='cuda', dtype=torch.float32)
        y = torch.randn(size, device='cuda', dtype=torch.float32)
        output = torch.empty(size, device='cuda', dtype=torch.float32)
        
        grid = lambda meta: (triton.cdiv(size, meta['BLOCK_SIZE']),)
        add_kernel[grid](x, y, output, size, BLOCK_SIZE=256)
        
        # éªŒè¯ç»“æœ
        expected = x + y
        max_error = (output - expected).abs().max().item()
        
        if torch.allclose(output, expected, rtol=1e-5, atol=1e-5):
            print(f"\nâœ… Kernelè¿è¡ŒæˆåŠŸï¼")
            print(f"   æµ‹è¯•å¤§å°: {size} ä¸ªå…ƒç´ ")
            print(f"   æœ€å¤§è¯¯å·®: {max_error:.2e}")
            print(f"   Gridå¤§å°: {triton.cdiv(size, 256)} blocks")
            print(f"   Blockå¤§å°: 256 threads")
            return True
        else:
            print(f"\nâŒ ç»“æœä¸åŒ¹é…ï¼Œæœ€å¤§è¯¯å·®: {max_error}")
            return False
            
    except Exception as e:
        print(f"\nâŒ Kernelè¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = test_simple_kernel()
    print("\n" + "=" * 60)
    if success:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼sm_121æ˜ å°„å·¥ä½œæ­£å¸¸ï¼")
    else:
        print("ğŸ’” æµ‹è¯•å¤±è´¥")
    print("=" * 60)

