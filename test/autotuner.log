2025-10-20 06:32:59,752 INFO:Auto-tuning with 0.9 CPU utilizations, 20 CPUs available, 18 CPUs will be used
2025-10-20 06:33:10,729 DEBUG:Compilation failed for config {'block_M': 64, 'block_K': 64, 'block_N': 128, 'num_stages': 4, 'threads': 256} at index 7 with error: Initialization failed: Failed to set the allowed dynamic shared memory size to 115712 with error: invalid argument
#include <tl_templates/cuda/gemm.h>
#include <tl_templates/cuda/copy.h>
#include <tl_templates/cuda/reduce.h>
#include <tl_templates/cuda/ldsm.h>
#include <tl_templates/cuda/threadblock_swizzle.h>
#include <tl_templates/cuda/debug.h>
#ifdef ENABLE_BF16
#include <tl_templates/cuda/cuda_bf16_fallbacks.cuh>
#endif

extern "C" __global__ void main_kernel(__grid_constant__ const CUtensorMap Input_desc, __grid_constant__ const CUtensorMap Output_desc, __grid_constant__ const CUtensorMap W_T_desc);
extern "C" __global__ void __launch_bounds__(384, 1) main_kernel(__grid_constant__ const CUtensorMap Input_desc, __grid_constant__ const CUtensorMap Output_desc, __grid_constant__ const CUtensorMap W_T_desc) {
  extern __shared__ __align__(1024) uchar buf_dyn_shmem[];
  float output_reg[32];
  float squared_reg[32];
  float sum_reg[4];
  __shared__ uint64_t mbarrier_mem[8];
  auto mbarrier = reinterpret_cast<Barrier*>(mbarrier_mem);
  if (tl::tl_shuffle_elect<0>()) {
    tl::prefetch_tma_descriptor(Input_desc);
    tl::prefetch_tma_descriptor(W_T_desc);
    tl::prefetch_tma_descriptor(Output_desc);
    mbarrier[0].init(128);
    mbarrier[1].init(128);
    mbarrier[2].init(128);
    mbarrier[3].init(128);
    mbarrier[4].init(256);
    mbarrier[5].init(256);
    mbarrier[6].init(256);
    mbarrier[7].init(256);
  }
  __syncthreads();
  if (256 <= ((int)threadIdx.x)) {
    for (int w = 0; w < 9; ++w) {
      for (int k = 0; k < 24; ++k) {
        mbarrier[((k & 3) + 4)].wait((((k & 7) >> 2) ^ 1));
        if (tl::tl_shuffle_elect<128>()) {
          mbarrier[(k & 3)].expect_transaction(8192);
          tl::tma_load(Input_desc, mbarrier[(k & 3)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k & 3) * 4096) + 32768)])), (k * 64), 0);
          mbarrier[(k & 3)].expect_transaction(16384);
          tl::tma_load(W_T_desc, mbarrier[(k & 3)], (&(((bfloat16_t*)buf_dyn_shmem)[((k & 3) * 8192)])), (k * 64), (w * 128));
        }
        mbarrier[(k & 3)].arrive();
      }
    }
  } else {
    for (int w_1 = 0; w_1 < 9; ++w_1) {
      #pragma unroll
      for (int i = 0; i < 16; ++i) {
        *(float2*)(output_reg + (i * 2)) = make_float2(0x0p+0f/*0.000000e+00*/, 0x0p+0f/*0.000000e+00*/);
      }
      tl::fence_proxy_async();
      for (int k_1 = 0; k_1 < 24; ++k_1) {
        mbarrier[(k_1 & 3)].wait(((k_1 & 7) >> 2));
        tl::gemm_ss<64, 128, 64, 2, 4, 0, 1, 0, 64, 64, 0, 0>((&(((bfloat16_t*)buf_dyn_shmem)[(((k_1 & 3) * 4096) + 32768)])), (&(((bfloat16_t*)buf_dyn_shmem)[((k_1 & 3) * 8192)])), (&(output_reg[0])));
        mbarrier[((k_1 & 3) + 4)].arrive();
      }
      #pragma unroll
      for (int i_1 = 0; i_1 < 32; ++i_1) {
        output_reg[i_1] = (output_reg[i_1] / (0x1p+0f/*1.000000e+00*/ + expf((output_reg[i_1] * -0x1p+0f/*-1.000000e+00*/))));
      }
      #pragma unroll
      for (int i_2 = 0; i_2 < 32; ++i_2) {
        squared_reg[i_2] = (output_reg[i_2] * output_reg[i_2]);
      }
      tl::__sync_thread_partial<3, 256>();
      #pragma unroll
      for (int i_3 = 0; i_3 < 4; ++i_3) {
        sum_reg[i_3] = 0x0p+0f/*0.000000e+00*/;
        #pragma unroll
        for (int rv = 0; rv < 8; ++rv) {
          sum_reg[i_3] = (sum_reg[i_3] + squared_reg[((((rv & 3) * 8) + (i_3 * 2)) + (rv >> 2))]);
        }
        sum_reg[i_3] = tl::AllReduce<tl::SumOp, 256, 64, 0>::run(sum_reg[i_3], (&(((float*)buf_dyn_shmem)[28672])));
        sum_reg[i_3] = tl::AllReduce<tl::SumOp, 4, 1, 0>::run(sum_reg[i_3]);
      }
      #pragma unroll
      for (int i_4 = 0; i_4 < 32; ++i_4) {
        output_reg[i_4] = (output_reg[i_4] * rsqrtf(sum_reg[((i_4 & 7) >> 1)]));
      }
      #pragma unroll
      for (int i_5 = 0; i_5 < 4; ++i_5) {
        tl::ptx_stmatrix_x4((&(((bfloat16_t*)buf_dyn_shmem)[(((((((((i_5 >> 1) * 4096) + (((((int)threadIdx.x) & 31) >> 4) * 2048)) + (((((int)threadIdx.x) & 63) >> 5) * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + (((((((int)threadIdx.x) & 7) >> 2) + (i_5 & 1)) & 1) * 32)) + ((((((int)threadIdx.x) >> 7) + ((((int)threadIdx.x) & 3) >> 1)) & 1) * 16)) + (((((((int)threadIdx.x) & 127) >> 6) + (((int)threadIdx.x) & 1)) & 1) * 8)) + 49152)])), __pack_half2(((bfloat16_t)output_reg[(i_5 * 8)]), ((bfloat16_t)output_reg[((i_5 * 8) + 1)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 2)]), ((bfloat16_t)output_reg[((i_5 * 8) + 3)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 4)]), ((bfloat16_t)output_reg[((i_5 * 8) + 5)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 6)]), ((bfloat16_t)output_reg[((i_5 * 8) + 7)])));
      }
      tl::fence_proxy_async();
      tl::__sync_thread_partial<3, 256>();
      if (tl::tl_shuffle_elect<256>()) {
        tl::tma_store(Output_desc, (&(((bfloat16_t*)buf_dyn_shmem)[49152])), (w_1 * 128), 0);
        tl::tma_store_arrive();
        tl::tma_store_wait<0>();
        tl::tma_store(Output_desc, (&(((bfloat16_t*)buf_dyn_shmem)[53248])), ((w_1 * 128) + 64), 0);
        tl::tma_store_arrive();
        tl::tma_store_wait<0>();
      }
    }
  }
}


#define ERROR_BUF_SIZE 1024
static char error_buf[ERROR_BUF_SIZE];

extern "C" const char* get_last_error() {
    return error_buf;
}

extern "C" int init() {
    error_buf[0] = '\0';
    
    cudaError_t result_main_kernel = cudaFuncSetAttribute(main_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, 115712);
    if (result_main_kernel != CUDA_SUCCESS) {
        snprintf(error_buf, ERROR_BUF_SIZE, "Failed to set the allowed dynamic shared memory size to %d with error: %s", 115712, cudaGetErrorString(result_main_kernel));
        return -1;
    }

    return 0;
}

extern "C" int call(bfloat16_t* __restrict__ Input, bfloat16_t* __restrict__ W_T, bfloat16_t* __restrict__ Output, cudaStream_t stream=cudaStreamDefault) {

	CUtensorMap Input_desc;
	CUtensorMapDataType Input_desc_type= (CUtensorMapDataType)9;
	cuuint32_t Input_desc_tensorRank= 2;
	void *Input_desc_globalAddress= Input;
	cuuint64_t Input_desc_globalDim[2]= {1536,8};
	cuuint64_t Input_desc_globalStride[2]= {2,3072};
	cuuint32_t Input_desc_boxDim[2]= {64,64};
	cuuint32_t Input_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave Input_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle Input_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion Input_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill Input_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult Input_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &Input_desc, Input_desc_type, Input_desc_tensorRank, Input_desc_globalAddress, Input_desc_globalDim, Input_desc_globalStride + 1, Input_desc_boxDim, Input_desc_elementStrides, Input_desc_interleave, Input_desc_swizzle, Input_desc_l2Promotion, Input_desc_oobFill);

	if (Input_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor Input_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}

	CUtensorMap Output_desc;
	CUtensorMapDataType Output_desc_type= (CUtensorMapDataType)9;
	cuuint32_t Output_desc_tensorRank= 2;
	void *Output_desc_globalAddress= Output;
	cuuint64_t Output_desc_globalDim[2]= {1152,8};
	cuuint64_t Output_desc_globalStride[2]= {2,2304};
	cuuint32_t Output_desc_boxDim[2]= {64,64};
	cuuint32_t Output_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave Output_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle Output_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion Output_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill Output_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult Output_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &Output_desc, Output_desc_type, Output_desc_tensorRank, Output_desc_globalAddress, Output_desc_globalDim, Output_desc_globalStride + 1, Output_desc_boxDim, Output_desc_elementStrides, Output_desc_interleave, Output_desc_swizzle, Output_desc_l2Promotion, Output_desc_oobFill);

	if (Output_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor Output_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}

	CUtensorMap W_T_desc;
	CUtensorMapDataType W_T_desc_type= (CUtensorMapDataType)9;
	cuuint32_t W_T_desc_tensorRank= 2;
	void *W_T_desc_globalAddress= W_T;
	cuuint64_t W_T_desc_globalDim[2]= {1536,1152};
	cuuint64_t W_T_desc_globalStride[2]= {2,3072};
	cuuint32_t W_T_desc_boxDim[2]= {64,128};
	cuuint32_t W_T_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave W_T_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle W_T_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion W_T_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill W_T_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult W_T_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &W_T_desc, W_T_desc_type, W_T_desc_tensorRank, W_T_desc_globalAddress, W_T_desc_globalDim, W_T_desc_globalStride + 1, W_T_desc_boxDim, W_T_desc_elementStrides, W_T_desc_interleave, W_T_desc_swizzle, W_T_desc_l2Promotion, W_T_desc_oobFill);

	if (W_T_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor W_T_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}
	main_kernel<<<dim3(1, 1, 1), dim3(384, 1, 1), 115712, stream>>>(Input_desc, Output_desc, W_T_desc);
	TILELANG_CHECK_LAST_ERROR("main_kernel");

	return 0;
}

2025-10-20 06:33:11,018 DEBUG:Compilation failed for config {'block_M': 64, 'block_K': 128, 'block_N': 128, 'num_stages': 4, 'threads': 128} at index 14 with error: Initialization failed: Failed to set the allowed dynamic shared memory size to 213504 with error: invalid argument
#include <tl_templates/cuda/gemm.h>
#include <tl_templates/cuda/copy.h>
#include <tl_templates/cuda/reduce.h>
#include <tl_templates/cuda/ldsm.h>
#include <tl_templates/cuda/threadblock_swizzle.h>
#include <tl_templates/cuda/debug.h>
#ifdef ENABLE_BF16
#include <tl_templates/cuda/cuda_bf16_fallbacks.cuh>
#endif

extern "C" __global__ void main_kernel(__grid_constant__ const CUtensorMap Input_desc, __grid_constant__ const CUtensorMap Output_desc, __grid_constant__ const CUtensorMap W_T_desc);
extern "C" __global__ void __launch_bounds__(256, 1) main_kernel(__grid_constant__ const CUtensorMap Input_desc, __grid_constant__ const CUtensorMap Output_desc, __grid_constant__ const CUtensorMap W_T_desc) {
  extern __shared__ __align__(1024) uchar buf_dyn_shmem[];
  float output_reg[64];
  float squared_reg[64];
  float sum_reg[4];
  __shared__ uint64_t mbarrier_mem[8];
  auto mbarrier = reinterpret_cast<Barrier*>(mbarrier_mem);
  if (tl::tl_shuffle_elect<0>()) {
    tl::prefetch_tma_descriptor(Input_desc);
    tl::prefetch_tma_descriptor(W_T_desc);
    tl::prefetch_tma_descriptor(Output_desc);
    mbarrier[0].init(128);
    mbarrier[1].init(128);
    mbarrier[2].init(128);
    mbarrier[3].init(128);
    mbarrier[4].init(128);
    mbarrier[5].init(128);
    mbarrier[6].init(128);
    mbarrier[7].init(128);
  }
  __syncthreads();
  if (128 <= ((int)threadIdx.x)) {
    for (int w = 0; w < 9; ++w) {
      for (int k = 0; k < 12; ++k) {
        mbarrier[((k & 3) + 4)].wait(((((w * 3) + (k >> 2)) & 1) ^ 1));
        if (tl::tl_shuffle_elect<128>()) {
          mbarrier[(k & 3)].expect_transaction(16384);
          tl::tma_load(Input_desc, mbarrier[(k & 3)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k & 3) * 8192) + 65536)])), (k * 128), 0);
          tl::tma_load(Input_desc, mbarrier[(k & 3)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k & 3) * 8192) + 69632)])), ((k * 128) + 64), 0);
          mbarrier[(k & 3)].expect_transaction(32768);
          tl::tma_load(W_T_desc, mbarrier[(k & 3)], (&(((bfloat16_t*)buf_dyn_shmem)[((k & 3) * 16384)])), (k * 128), (w * 128));
          tl::tma_load(W_T_desc, mbarrier[(k & 3)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k & 3) * 16384) + 8192)])), ((k * 128) + 64), (w * 128));
        }
        mbarrier[(k & 3)].arrive();
      }
    }
  } else {
    for (int w_1 = 0; w_1 < 9; ++w_1) {
      #pragma unroll
      for (int i = 0; i < 32; ++i) {
        *(float2*)(output_reg + (i * 2)) = make_float2(0x0p+0f/*0.000000e+00*/, 0x0p+0f/*0.000000e+00*/);
      }
      tl::fence_proxy_async();
      for (int k_1 = 0; k_1 < 12; ++k_1) {
        mbarrier[(k_1 & 3)].wait((((w_1 * 3) + (k_1 >> 2)) & 1));
        tl::gemm_ss<64, 128, 128, 2, 2, 0, 1, 0, 128, 128, 0, 0>((&(((bfloat16_t*)buf_dyn_shmem)[(((k_1 & 3) * 8192) + 65536)])), (&(((bfloat16_t*)buf_dyn_shmem)[((k_1 & 3) * 16384)])), (&(output_reg[0])));
        mbarrier[((k_1 & 3) + 4)].arrive();
      }
      #pragma unroll
      for (int i_1 = 0; i_1 < 64; ++i_1) {
        output_reg[i_1] = (output_reg[i_1] / (0x1p+0f/*1.000000e+00*/ + expf((output_reg[i_1] * -0x1p+0f/*-1.000000e+00*/))));
      }
      #pragma unroll
      for (int i_2 = 0; i_2 < 64; ++i_2) {
        squared_reg[i_2] = (output_reg[i_2] * output_reg[i_2]);
      }
      tl::__sync_thread_partial<3, 128>();
      #pragma unroll
      for (int i_3 = 0; i_3 < 4; ++i_3) {
        sum_reg[i_3] = 0x0p+0f/*0.000000e+00*/;
        #pragma unroll
        for (int rv = 0; rv < 16; ++rv) {
          sum_reg[i_3] = (sum_reg[i_3] + squared_reg[((((rv & 7) * 8) + (i_3 * 2)) + (rv >> 3))]);
        }
        sum_reg[i_3] = tl::AllReduce<tl::SumOp, 128, 64, 0>::run(sum_reg[i_3], (&(((float*)buf_dyn_shmem)[53248])));
        sum_reg[i_3] = tl::AllReduce<tl::SumOp, 4, 1, 0>::run(sum_reg[i_3]);
      }
      #pragma unroll
      for (int i_4 = 0; i_4 < 64; ++i_4) {
        output_reg[i_4] = (output_reg[i_4] * rsqrtf(sum_reg[((i_4 & 7) >> 1)]));
      }
      #pragma unroll
      for (int i_5 = 0; i_5 < 8; ++i_5) {
        tl::ptx_stmatrix_x4((&(((bfloat16_t*)buf_dyn_shmem)[(((((((((i_5 >> 2) * 4096) + (((((int)threadIdx.x) & 31) >> 4) * 2048)) + (((((int)threadIdx.x) & 63) >> 5) * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + (((((((int)threadIdx.x) & 7) >> 2) + ((i_5 & 3) >> 1)) & 1) * 32)) + (((((((int)threadIdx.x) & 3) >> 1) + (i_5 & 1)) & 1) * 16)) + ((((((int)threadIdx.x) >> 6) + (((int)threadIdx.x) & 1)) & 1) * 8)) + 98304)])), __pack_half2(((bfloat16_t)output_reg[(i_5 * 8)]), ((bfloat16_t)output_reg[((i_5 * 8) + 1)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 2)]), ((bfloat16_t)output_reg[((i_5 * 8) + 3)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 4)]), ((bfloat16_t)output_reg[((i_5 * 8) + 5)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 6)]), ((bfloat16_t)output_reg[((i_5 * 8) + 7)])));
      }
      tl::fence_proxy_async();
      tl::__sync_thread_partial<3, 128>();
      if (tl::tl_shuffle_elect<128>()) {
        tl::tma_store(Output_desc, (&(((bfloat16_t*)buf_dyn_shmem)[98304])), (w_1 * 128), 0);
        tl::tma_store_arrive();
        tl::tma_store_wait<0>();
        tl::tma_store(Output_desc, (&(((bfloat16_t*)buf_dyn_shmem)[102400])), ((w_1 * 128) + 64), 0);
        tl::tma_store_arrive();
        tl::tma_store_wait<0>();
      }
    }
  }
}


#define ERROR_BUF_SIZE 1024
static char error_buf[ERROR_BUF_SIZE];

extern "C" const char* get_last_error() {
    return error_buf;
}

extern "C" int init() {
    error_buf[0] = '\0';
    
    cudaError_t result_main_kernel = cudaFuncSetAttribute(main_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, 213504);
    if (result_main_kernel != CUDA_SUCCESS) {
        snprintf(error_buf, ERROR_BUF_SIZE, "Failed to set the allowed dynamic shared memory size to %d with error: %s", 213504, cudaGetErrorString(result_main_kernel));
        return -1;
    }

    return 0;
}

extern "C" int call(bfloat16_t* __restrict__ Input, bfloat16_t* __restrict__ W_T, bfloat16_t* __restrict__ Output, cudaStream_t stream=cudaStreamDefault) {

	CUtensorMap Input_desc;
	CUtensorMapDataType Input_desc_type= (CUtensorMapDataType)9;
	cuuint32_t Input_desc_tensorRank= 2;
	void *Input_desc_globalAddress= Input;
	cuuint64_t Input_desc_globalDim[2]= {1536,8};
	cuuint64_t Input_desc_globalStride[2]= {2,3072};
	cuuint32_t Input_desc_boxDim[2]= {64,64};
	cuuint32_t Input_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave Input_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle Input_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion Input_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill Input_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult Input_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &Input_desc, Input_desc_type, Input_desc_tensorRank, Input_desc_globalAddress, Input_desc_globalDim, Input_desc_globalStride + 1, Input_desc_boxDim, Input_desc_elementStrides, Input_desc_interleave, Input_desc_swizzle, Input_desc_l2Promotion, Input_desc_oobFill);

	if (Input_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor Input_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}

	CUtensorMap Output_desc;
	CUtensorMapDataType Output_desc_type= (CUtensorMapDataType)9;
	cuuint32_t Output_desc_tensorRank= 2;
	void *Output_desc_globalAddress= Output;
	cuuint64_t Output_desc_globalDim[2]= {1152,8};
	cuuint64_t Output_desc_globalStride[2]= {2,2304};
	cuuint32_t Output_desc_boxDim[2]= {64,64};
	cuuint32_t Output_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave Output_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle Output_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion Output_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill Output_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult Output_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &Output_desc, Output_desc_type, Output_desc_tensorRank, Output_desc_globalAddress, Output_desc_globalDim, Output_desc_globalStride + 1, Output_desc_boxDim, Output_desc_elementStrides, Output_desc_interleave, Output_desc_swizzle, Output_desc_l2Promotion, Output_desc_oobFill);

	if (Output_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor Output_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}

	CUtensorMap W_T_desc;
	CUtensorMapDataType W_T_desc_type= (CUtensorMapDataType)9;
	cuuint32_t W_T_desc_tensorRank= 2;
	void *W_T_desc_globalAddress= W_T;
	cuuint64_t W_T_desc_globalDim[2]= {1536,1152};
	cuuint64_t W_T_desc_globalStride[2]= {2,3072};
	cuuint32_t W_T_desc_boxDim[2]= {64,128};
	cuuint32_t W_T_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave W_T_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle W_T_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion W_T_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill W_T_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult W_T_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &W_T_desc, W_T_desc_type, W_T_desc_tensorRank, W_T_desc_globalAddress, W_T_desc_globalDim, W_T_desc_globalStride + 1, W_T_desc_boxDim, W_T_desc_elementStrides, W_T_desc_interleave, W_T_desc_swizzle, W_T_desc_l2Promotion, W_T_desc_oobFill);

	if (W_T_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor W_T_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}
	main_kernel<<<dim3(1, 1, 1), dim3(256, 1, 1), 213504, stream>>>(Input_desc, Output_desc, W_T_desc);
	TILELANG_CHECK_LAST_ERROR("main_kernel");

	return 0;
}

2025-10-20 06:33:11,576 DEBUG:Compilation failed for config {'block_M': 64, 'block_K': 128, 'block_N': 128, 'num_stages': 4, 'threads': 256} at index 15 with error: Initialization failed: Failed to set the allowed dynamic shared memory size to 214016 with error: invalid argument
#include <tl_templates/cuda/gemm.h>
#include <tl_templates/cuda/copy.h>
#include <tl_templates/cuda/reduce.h>
#include <tl_templates/cuda/ldsm.h>
#include <tl_templates/cuda/threadblock_swizzle.h>
#include <tl_templates/cuda/debug.h>
#ifdef ENABLE_BF16
#include <tl_templates/cuda/cuda_bf16_fallbacks.cuh>
#endif

extern "C" __global__ void main_kernel(__grid_constant__ const CUtensorMap Input_desc, __grid_constant__ const CUtensorMap Output_desc, __grid_constant__ const CUtensorMap W_T_desc);
extern "C" __global__ void __launch_bounds__(384, 1) main_kernel(__grid_constant__ const CUtensorMap Input_desc, __grid_constant__ const CUtensorMap Output_desc, __grid_constant__ const CUtensorMap W_T_desc) {
  extern __shared__ __align__(1024) uchar buf_dyn_shmem[];
  float output_reg[32];
  float squared_reg[32];
  float sum_reg[4];
  __shared__ uint64_t mbarrier_mem[8];
  auto mbarrier = reinterpret_cast<Barrier*>(mbarrier_mem);
  if (tl::tl_shuffle_elect<0>()) {
    tl::prefetch_tma_descriptor(Input_desc);
    tl::prefetch_tma_descriptor(W_T_desc);
    tl::prefetch_tma_descriptor(Output_desc);
    mbarrier[0].init(128);
    mbarrier[1].init(128);
    mbarrier[2].init(128);
    mbarrier[3].init(128);
    mbarrier[4].init(256);
    mbarrier[5].init(256);
    mbarrier[6].init(256);
    mbarrier[7].init(256);
  }
  __syncthreads();
  if (256 <= ((int)threadIdx.x)) {
    for (int w = 0; w < 9; ++w) {
      for (int k = 0; k < 12; ++k) {
        mbarrier[((k & 3) + 4)].wait(((((w * 3) + (k >> 2)) & 1) ^ 1));
        if (tl::tl_shuffle_elect<128>()) {
          mbarrier[(k & 3)].expect_transaction(16384);
          tl::tma_load(Input_desc, mbarrier[(k & 3)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k & 3) * 8192) + 65536)])), (k * 128), 0);
          tl::tma_load(Input_desc, mbarrier[(k & 3)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k & 3) * 8192) + 69632)])), ((k * 128) + 64), 0);
          mbarrier[(k & 3)].expect_transaction(32768);
          tl::tma_load(W_T_desc, mbarrier[(k & 3)], (&(((bfloat16_t*)buf_dyn_shmem)[((k & 3) * 16384)])), (k * 128), (w * 128));
          tl::tma_load(W_T_desc, mbarrier[(k & 3)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k & 3) * 16384) + 8192)])), ((k * 128) + 64), (w * 128));
        }
        mbarrier[(k & 3)].arrive();
      }
    }
  } else {
    for (int w_1 = 0; w_1 < 9; ++w_1) {
      #pragma unroll
      for (int i = 0; i < 16; ++i) {
        *(float2*)(output_reg + (i * 2)) = make_float2(0x0p+0f/*0.000000e+00*/, 0x0p+0f/*0.000000e+00*/);
      }
      tl::fence_proxy_async();
      for (int k_1 = 0; k_1 < 12; ++k_1) {
        mbarrier[(k_1 & 3)].wait((((w_1 * 3) + (k_1 >> 2)) & 1));
        tl::gemm_ss<64, 128, 128, 2, 4, 0, 1, 0, 128, 128, 0, 0>((&(((bfloat16_t*)buf_dyn_shmem)[(((k_1 & 3) * 8192) + 65536)])), (&(((bfloat16_t*)buf_dyn_shmem)[((k_1 & 3) * 16384)])), (&(output_reg[0])));
        mbarrier[((k_1 & 3) + 4)].arrive();
      }
      #pragma unroll
      for (int i_1 = 0; i_1 < 32; ++i_1) {
        output_reg[i_1] = (output_reg[i_1] / (0x1p+0f/*1.000000e+00*/ + expf((output_reg[i_1] * -0x1p+0f/*-1.000000e+00*/))));
      }
      #pragma unroll
      for (int i_2 = 0; i_2 < 32; ++i_2) {
        squared_reg[i_2] = (output_reg[i_2] * output_reg[i_2]);
      }
      tl::__sync_thread_partial<3, 256>();
      #pragma unroll
      for (int i_3 = 0; i_3 < 4; ++i_3) {
        sum_reg[i_3] = 0x0p+0f/*0.000000e+00*/;
        #pragma unroll
        for (int rv = 0; rv < 8; ++rv) {
          sum_reg[i_3] = (sum_reg[i_3] + squared_reg[((((rv & 3) * 8) + (i_3 * 2)) + (rv >> 2))]);
        }
        sum_reg[i_3] = tl::AllReduce<tl::SumOp, 256, 64, 0>::run(sum_reg[i_3], (&(((float*)buf_dyn_shmem)[53248])));
        sum_reg[i_3] = tl::AllReduce<tl::SumOp, 4, 1, 0>::run(sum_reg[i_3]);
      }
      #pragma unroll
      for (int i_4 = 0; i_4 < 32; ++i_4) {
        output_reg[i_4] = (output_reg[i_4] * rsqrtf(sum_reg[((i_4 & 7) >> 1)]));
      }
      #pragma unroll
      for (int i_5 = 0; i_5 < 4; ++i_5) {
        tl::ptx_stmatrix_x4((&(((bfloat16_t*)buf_dyn_shmem)[(((((((((i_5 >> 1) * 4096) + (((((int)threadIdx.x) & 31) >> 4) * 2048)) + (((((int)threadIdx.x) & 63) >> 5) * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + (((((((int)threadIdx.x) & 7) >> 2) + (i_5 & 1)) & 1) * 32)) + ((((((int)threadIdx.x) >> 7) + ((((int)threadIdx.x) & 3) >> 1)) & 1) * 16)) + (((((((int)threadIdx.x) & 127) >> 6) + (((int)threadIdx.x) & 1)) & 1) * 8)) + 98304)])), __pack_half2(((bfloat16_t)output_reg[(i_5 * 8)]), ((bfloat16_t)output_reg[((i_5 * 8) + 1)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 2)]), ((bfloat16_t)output_reg[((i_5 * 8) + 3)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 4)]), ((bfloat16_t)output_reg[((i_5 * 8) + 5)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 6)]), ((bfloat16_t)output_reg[((i_5 * 8) + 7)])));
      }
      tl::fence_proxy_async();
      tl::__sync_thread_partial<3, 256>();
      if (tl::tl_shuffle_elect<256>()) {
        tl::tma_store(Output_desc, (&(((bfloat16_t*)buf_dyn_shmem)[98304])), (w_1 * 128), 0);
        tl::tma_store_arrive();
        tl::tma_store_wait<0>();
        tl::tma_store(Output_desc, (&(((bfloat16_t*)buf_dyn_shmem)[102400])), ((w_1 * 128) + 64), 0);
        tl::tma_store_arrive();
        tl::tma_store_wait<0>();
      }
    }
  }
}


#define ERROR_BUF_SIZE 1024
static char error_buf[ERROR_BUF_SIZE];

extern "C" const char* get_last_error() {
    return error_buf;
}

extern "C" int init() {
    error_buf[0] = '\0';
    
    cudaError_t result_main_kernel = cudaFuncSetAttribute(main_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, 214016);
    if (result_main_kernel != CUDA_SUCCESS) {
        snprintf(error_buf, ERROR_BUF_SIZE, "Failed to set the allowed dynamic shared memory size to %d with error: %s", 214016, cudaGetErrorString(result_main_kernel));
        return -1;
    }

    return 0;
}

extern "C" int call(bfloat16_t* __restrict__ Input, bfloat16_t* __restrict__ W_T, bfloat16_t* __restrict__ Output, cudaStream_t stream=cudaStreamDefault) {

	CUtensorMap Input_desc;
	CUtensorMapDataType Input_desc_type= (CUtensorMapDataType)9;
	cuuint32_t Input_desc_tensorRank= 2;
	void *Input_desc_globalAddress= Input;
	cuuint64_t Input_desc_globalDim[2]= {1536,8};
	cuuint64_t Input_desc_globalStride[2]= {2,3072};
	cuuint32_t Input_desc_boxDim[2]= {64,64};
	cuuint32_t Input_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave Input_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle Input_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion Input_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill Input_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult Input_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &Input_desc, Input_desc_type, Input_desc_tensorRank, Input_desc_globalAddress, Input_desc_globalDim, Input_desc_globalStride + 1, Input_desc_boxDim, Input_desc_elementStrides, Input_desc_interleave, Input_desc_swizzle, Input_desc_l2Promotion, Input_desc_oobFill);

	if (Input_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor Input_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}

	CUtensorMap Output_desc;
	CUtensorMapDataType Output_desc_type= (CUtensorMapDataType)9;
	cuuint32_t Output_desc_tensorRank= 2;
	void *Output_desc_globalAddress= Output;
	cuuint64_t Output_desc_globalDim[2]= {1152,8};
	cuuint64_t Output_desc_globalStride[2]= {2,2304};
	cuuint32_t Output_desc_boxDim[2]= {64,64};
	cuuint32_t Output_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave Output_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle Output_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion Output_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill Output_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult Output_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &Output_desc, Output_desc_type, Output_desc_tensorRank, Output_desc_globalAddress, Output_desc_globalDim, Output_desc_globalStride + 1, Output_desc_boxDim, Output_desc_elementStrides, Output_desc_interleave, Output_desc_swizzle, Output_desc_l2Promotion, Output_desc_oobFill);

	if (Output_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor Output_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}

	CUtensorMap W_T_desc;
	CUtensorMapDataType W_T_desc_type= (CUtensorMapDataType)9;
	cuuint32_t W_T_desc_tensorRank= 2;
	void *W_T_desc_globalAddress= W_T;
	cuuint64_t W_T_desc_globalDim[2]= {1536,1152};
	cuuint64_t W_T_desc_globalStride[2]= {2,3072};
	cuuint32_t W_T_desc_boxDim[2]= {64,128};
	cuuint32_t W_T_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave W_T_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle W_T_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion W_T_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill W_T_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult W_T_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &W_T_desc, W_T_desc_type, W_T_desc_tensorRank, W_T_desc_globalAddress, W_T_desc_globalDim, W_T_desc_globalStride + 1, W_T_desc_boxDim, W_T_desc_elementStrides, W_T_desc_interleave, W_T_desc_swizzle, W_T_desc_l2Promotion, W_T_desc_oobFill);

	if (W_T_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor W_T_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}
	main_kernel<<<dim3(1, 1, 1), dim3(384, 1, 1), 214016, stream>>>(Input_desc, Output_desc, W_T_desc);
	TILELANG_CHECK_LAST_ERROR("main_kernel");

	return 0;
}

2025-10-20 06:33:11,777 DEBUG:Compilation failed for config {'block_M': 64, 'block_K': 128, 'block_N': 128, 'num_stages': 2, 'threads': 256} at index 11 with error: Initialization failed: Failed to set the allowed dynamic shared memory size to 115712 with error: invalid argument
#include <tl_templates/cuda/gemm.h>
#include <tl_templates/cuda/copy.h>
#include <tl_templates/cuda/reduce.h>
#include <tl_templates/cuda/ldsm.h>
#include <tl_templates/cuda/threadblock_swizzle.h>
#include <tl_templates/cuda/debug.h>
#ifdef ENABLE_BF16
#include <tl_templates/cuda/cuda_bf16_fallbacks.cuh>
#endif

extern "C" __global__ void main_kernel(__grid_constant__ const CUtensorMap Input_desc, __grid_constant__ const CUtensorMap Output_desc, __grid_constant__ const CUtensorMap W_T_desc);
extern "C" __global__ void __launch_bounds__(384, 1) main_kernel(__grid_constant__ const CUtensorMap Input_desc, __grid_constant__ const CUtensorMap Output_desc, __grid_constant__ const CUtensorMap W_T_desc) {
  extern __shared__ __align__(1024) uchar buf_dyn_shmem[];
  float output_reg[32];
  float squared_reg[32];
  float sum_reg[4];
  __shared__ uint64_t mbarrier_mem[4];
  auto mbarrier = reinterpret_cast<Barrier*>(mbarrier_mem);
  if (tl::tl_shuffle_elect<0>()) {
    tl::prefetch_tma_descriptor(Input_desc);
    tl::prefetch_tma_descriptor(W_T_desc);
    tl::prefetch_tma_descriptor(Output_desc);
    mbarrier[0].init(128);
    mbarrier[1].init(128);
    mbarrier[2].init(256);
    mbarrier[3].init(256);
  }
  __syncthreads();
  if (256 <= ((int)threadIdx.x)) {
    for (int w = 0; w < 9; ++w) {
      for (int k = 0; k < 12; ++k) {
        mbarrier[((k & 1) + 2)].wait((((k & 3) >> 1) ^ 1));
        if (tl::tl_shuffle_elect<128>()) {
          mbarrier[(k & 1)].expect_transaction(16384);
          tl::tma_load(Input_desc, mbarrier[(k & 1)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k & 1) * 8192) + 32768)])), (k * 128), 0);
          tl::tma_load(Input_desc, mbarrier[(k & 1)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k & 1) * 8192) + 36864)])), ((k * 128) + 64), 0);
          mbarrier[(k & 1)].expect_transaction(32768);
          tl::tma_load(W_T_desc, mbarrier[(k & 1)], (&(((bfloat16_t*)buf_dyn_shmem)[((k & 1) * 16384)])), (k * 128), (w * 128));
          tl::tma_load(W_T_desc, mbarrier[(k & 1)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k & 1) * 16384) + 8192)])), ((k * 128) + 64), (w * 128));
        }
        mbarrier[(k & 1)].arrive();
      }
    }
  } else {
    for (int w_1 = 0; w_1 < 9; ++w_1) {
      #pragma unroll
      for (int i = 0; i < 16; ++i) {
        *(float2*)(output_reg + (i * 2)) = make_float2(0x0p+0f/*0.000000e+00*/, 0x0p+0f/*0.000000e+00*/);
      }
      tl::fence_proxy_async();
      for (int k_1 = 0; k_1 < 12; ++k_1) {
        mbarrier[(k_1 & 1)].wait(((k_1 & 3) >> 1));
        tl::gemm_ss<64, 128, 128, 2, 4, 0, 1, 0, 128, 128, 0, 0>((&(((bfloat16_t*)buf_dyn_shmem)[(((k_1 & 1) * 8192) + 32768)])), (&(((bfloat16_t*)buf_dyn_shmem)[((k_1 & 1) * 16384)])), (&(output_reg[0])));
        mbarrier[((k_1 & 1) + 2)].arrive();
      }
      #pragma unroll
      for (int i_1 = 0; i_1 < 32; ++i_1) {
        output_reg[i_1] = (output_reg[i_1] / (0x1p+0f/*1.000000e+00*/ + expf((output_reg[i_1] * -0x1p+0f/*-1.000000e+00*/))));
      }
      #pragma unroll
      for (int i_2 = 0; i_2 < 32; ++i_2) {
        squared_reg[i_2] = (output_reg[i_2] * output_reg[i_2]);
      }
      tl::__sync_thread_partial<3, 256>();
      #pragma unroll
      for (int i_3 = 0; i_3 < 4; ++i_3) {
        sum_reg[i_3] = 0x0p+0f/*0.000000e+00*/;
        #pragma unroll
        for (int rv = 0; rv < 8; ++rv) {
          sum_reg[i_3] = (sum_reg[i_3] + squared_reg[((((rv & 3) * 8) + (i_3 * 2)) + (rv >> 2))]);
        }
        sum_reg[i_3] = tl::AllReduce<tl::SumOp, 256, 64, 0>::run(sum_reg[i_3], (&(((float*)buf_dyn_shmem)[28672])));
        sum_reg[i_3] = tl::AllReduce<tl::SumOp, 4, 1, 0>::run(sum_reg[i_3]);
      }
      #pragma unroll
      for (int i_4 = 0; i_4 < 32; ++i_4) {
        output_reg[i_4] = (output_reg[i_4] * rsqrtf(sum_reg[((i_4 & 7) >> 1)]));
      }
      #pragma unroll
      for (int i_5 = 0; i_5 < 4; ++i_5) {
        tl::ptx_stmatrix_x4((&(((bfloat16_t*)buf_dyn_shmem)[(((((((((i_5 >> 1) * 4096) + (((((int)threadIdx.x) & 31) >> 4) * 2048)) + (((((int)threadIdx.x) & 63) >> 5) * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + (((((((int)threadIdx.x) & 7) >> 2) + (i_5 & 1)) & 1) * 32)) + ((((((int)threadIdx.x) >> 7) + ((((int)threadIdx.x) & 3) >> 1)) & 1) * 16)) + (((((((int)threadIdx.x) & 127) >> 6) + (((int)threadIdx.x) & 1)) & 1) * 8)) + 49152)])), __pack_half2(((bfloat16_t)output_reg[(i_5 * 8)]), ((bfloat16_t)output_reg[((i_5 * 8) + 1)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 2)]), ((bfloat16_t)output_reg[((i_5 * 8) + 3)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 4)]), ((bfloat16_t)output_reg[((i_5 * 8) + 5)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 6)]), ((bfloat16_t)output_reg[((i_5 * 8) + 7)])));
      }
      tl::fence_proxy_async();
      tl::__sync_thread_partial<3, 256>();
      if (tl::tl_shuffle_elect<256>()) {
        tl::tma_store(Output_desc, (&(((bfloat16_t*)buf_dyn_shmem)[49152])), (w_1 * 128), 0);
        tl::tma_store_arrive();
        tl::tma_store_wait<0>();
        tl::tma_store(Output_desc, (&(((bfloat16_t*)buf_dyn_shmem)[53248])), ((w_1 * 128) + 64), 0);
        tl::tma_store_arrive();
        tl::tma_store_wait<0>();
      }
    }
  }
}


#define ERROR_BUF_SIZE 1024
static char error_buf[ERROR_BUF_SIZE];

extern "C" const char* get_last_error() {
    return error_buf;
}

extern "C" int init() {
    error_buf[0] = '\0';
    
    cudaError_t result_main_kernel = cudaFuncSetAttribute(main_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, 115712);
    if (result_main_kernel != CUDA_SUCCESS) {
        snprintf(error_buf, ERROR_BUF_SIZE, "Failed to set the allowed dynamic shared memory size to %d with error: %s", 115712, cudaGetErrorString(result_main_kernel));
        return -1;
    }

    return 0;
}

extern "C" int call(bfloat16_t* __restrict__ Input, bfloat16_t* __restrict__ W_T, bfloat16_t* __restrict__ Output, cudaStream_t stream=cudaStreamDefault) {

	CUtensorMap Input_desc;
	CUtensorMapDataType Input_desc_type= (CUtensorMapDataType)9;
	cuuint32_t Input_desc_tensorRank= 2;
	void *Input_desc_globalAddress= Input;
	cuuint64_t Input_desc_globalDim[2]= {1536,8};
	cuuint64_t Input_desc_globalStride[2]= {2,3072};
	cuuint32_t Input_desc_boxDim[2]= {64,64};
	cuuint32_t Input_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave Input_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle Input_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion Input_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill Input_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult Input_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &Input_desc, Input_desc_type, Input_desc_tensorRank, Input_desc_globalAddress, Input_desc_globalDim, Input_desc_globalStride + 1, Input_desc_boxDim, Input_desc_elementStrides, Input_desc_interleave, Input_desc_swizzle, Input_desc_l2Promotion, Input_desc_oobFill);

	if (Input_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor Input_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}

	CUtensorMap Output_desc;
	CUtensorMapDataType Output_desc_type= (CUtensorMapDataType)9;
	cuuint32_t Output_desc_tensorRank= 2;
	void *Output_desc_globalAddress= Output;
	cuuint64_t Output_desc_globalDim[2]= {1152,8};
	cuuint64_t Output_desc_globalStride[2]= {2,2304};
	cuuint32_t Output_desc_boxDim[2]= {64,64};
	cuuint32_t Output_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave Output_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle Output_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion Output_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill Output_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult Output_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &Output_desc, Output_desc_type, Output_desc_tensorRank, Output_desc_globalAddress, Output_desc_globalDim, Output_desc_globalStride + 1, Output_desc_boxDim, Output_desc_elementStrides, Output_desc_interleave, Output_desc_swizzle, Output_desc_l2Promotion, Output_desc_oobFill);

	if (Output_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor Output_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}

	CUtensorMap W_T_desc;
	CUtensorMapDataType W_T_desc_type= (CUtensorMapDataType)9;
	cuuint32_t W_T_desc_tensorRank= 2;
	void *W_T_desc_globalAddress= W_T;
	cuuint64_t W_T_desc_globalDim[2]= {1536,1152};
	cuuint64_t W_T_desc_globalStride[2]= {2,3072};
	cuuint32_t W_T_desc_boxDim[2]= {64,128};
	cuuint32_t W_T_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave W_T_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle W_T_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion W_T_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill W_T_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult W_T_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &W_T_desc, W_T_desc_type, W_T_desc_tensorRank, W_T_desc_globalAddress, W_T_desc_globalDim, W_T_desc_globalStride + 1, W_T_desc_boxDim, W_T_desc_elementStrides, W_T_desc_interleave, W_T_desc_swizzle, W_T_desc_l2Promotion, W_T_desc_oobFill);

	if (W_T_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor W_T_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}
	main_kernel<<<dim3(1, 1, 1), dim3(384, 1, 1), 115712, stream>>>(Input_desc, Output_desc, W_T_desc);
	TILELANG_CHECK_LAST_ERROR("main_kernel");

	return 0;
}

2025-10-20 06:33:12,392 DEBUG:Compilation failed for config {'block_M': 64, 'block_K': 64, 'block_N': 128, 'num_stages': 4, 'threads': 128} at index 6 with error: Initialization failed: Failed to set the allowed dynamic shared memory size to 115200 with error: invalid argument
#include <tl_templates/cuda/gemm.h>
#include <tl_templates/cuda/copy.h>
#include <tl_templates/cuda/reduce.h>
#include <tl_templates/cuda/ldsm.h>
#include <tl_templates/cuda/threadblock_swizzle.h>
#include <tl_templates/cuda/debug.h>
#ifdef ENABLE_BF16
#include <tl_templates/cuda/cuda_bf16_fallbacks.cuh>
#endif

extern "C" __global__ void main_kernel(__grid_constant__ const CUtensorMap Input_desc, __grid_constant__ const CUtensorMap Output_desc, __grid_constant__ const CUtensorMap W_T_desc);
extern "C" __global__ void __launch_bounds__(256, 1) main_kernel(__grid_constant__ const CUtensorMap Input_desc, __grid_constant__ const CUtensorMap Output_desc, __grid_constant__ const CUtensorMap W_T_desc) {
  extern __shared__ __align__(1024) uchar buf_dyn_shmem[];
  float output_reg[64];
  float squared_reg[64];
  float sum_reg[4];
  __shared__ uint64_t mbarrier_mem[8];
  auto mbarrier = reinterpret_cast<Barrier*>(mbarrier_mem);
  if (tl::tl_shuffle_elect<0>()) {
    tl::prefetch_tma_descriptor(Input_desc);
    tl::prefetch_tma_descriptor(W_T_desc);
    tl::prefetch_tma_descriptor(Output_desc);
    mbarrier[0].init(128);
    mbarrier[1].init(128);
    mbarrier[2].init(128);
    mbarrier[3].init(128);
    mbarrier[4].init(128);
    mbarrier[5].init(128);
    mbarrier[6].init(128);
    mbarrier[7].init(128);
  }
  __syncthreads();
  if (128 <= ((int)threadIdx.x)) {
    for (int w = 0; w < 9; ++w) {
      for (int k = 0; k < 24; ++k) {
        mbarrier[((k & 3) + 4)].wait((((k & 7) >> 2) ^ 1));
        if (tl::tl_shuffle_elect<128>()) {
          mbarrier[(k & 3)].expect_transaction(8192);
          tl::tma_load(Input_desc, mbarrier[(k & 3)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k & 3) * 4096) + 32768)])), (k * 64), 0);
          mbarrier[(k & 3)].expect_transaction(16384);
          tl::tma_load(W_T_desc, mbarrier[(k & 3)], (&(((bfloat16_t*)buf_dyn_shmem)[((k & 3) * 8192)])), (k * 64), (w * 128));
        }
        mbarrier[(k & 3)].arrive();
      }
    }
  } else {
    for (int w_1 = 0; w_1 < 9; ++w_1) {
      #pragma unroll
      for (int i = 0; i < 32; ++i) {
        *(float2*)(output_reg + (i * 2)) = make_float2(0x0p+0f/*0.000000e+00*/, 0x0p+0f/*0.000000e+00*/);
      }
      tl::fence_proxy_async();
      for (int k_1 = 0; k_1 < 24; ++k_1) {
        mbarrier[(k_1 & 3)].wait(((k_1 & 7) >> 2));
        tl::gemm_ss<64, 128, 64, 2, 2, 0, 1, 0, 64, 64, 0, 0>((&(((bfloat16_t*)buf_dyn_shmem)[(((k_1 & 3) * 4096) + 32768)])), (&(((bfloat16_t*)buf_dyn_shmem)[((k_1 & 3) * 8192)])), (&(output_reg[0])));
        mbarrier[((k_1 & 3) + 4)].arrive();
      }
      #pragma unroll
      for (int i_1 = 0; i_1 < 64; ++i_1) {
        output_reg[i_1] = (output_reg[i_1] / (0x1p+0f/*1.000000e+00*/ + expf((output_reg[i_1] * -0x1p+0f/*-1.000000e+00*/))));
      }
      #pragma unroll
      for (int i_2 = 0; i_2 < 64; ++i_2) {
        squared_reg[i_2] = (output_reg[i_2] * output_reg[i_2]);
      }
      tl::__sync_thread_partial<3, 128>();
      #pragma unroll
      for (int i_3 = 0; i_3 < 4; ++i_3) {
        sum_reg[i_3] = 0x0p+0f/*0.000000e+00*/;
        #pragma unroll
        for (int rv = 0; rv < 16; ++rv) {
          sum_reg[i_3] = (sum_reg[i_3] + squared_reg[((((rv & 7) * 8) + (i_3 * 2)) + (rv >> 3))]);
        }
        sum_reg[i_3] = tl::AllReduce<tl::SumOp, 128, 64, 0>::run(sum_reg[i_3], (&(((float*)buf_dyn_shmem)[28672])));
        sum_reg[i_3] = tl::AllReduce<tl::SumOp, 4, 1, 0>::run(sum_reg[i_3]);
      }
      #pragma unroll
      for (int i_4 = 0; i_4 < 64; ++i_4) {
        output_reg[i_4] = (output_reg[i_4] * rsqrtf(sum_reg[((i_4 & 7) >> 1)]));
      }
      #pragma unroll
      for (int i_5 = 0; i_5 < 8; ++i_5) {
        tl::ptx_stmatrix_x4((&(((bfloat16_t*)buf_dyn_shmem)[(((((((((i_5 >> 2) * 4096) + (((((int)threadIdx.x) & 31) >> 4) * 2048)) + (((((int)threadIdx.x) & 63) >> 5) * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + (((((((int)threadIdx.x) & 7) >> 2) + ((i_5 & 3) >> 1)) & 1) * 32)) + (((((((int)threadIdx.x) & 3) >> 1) + (i_5 & 1)) & 1) * 16)) + ((((((int)threadIdx.x) >> 6) + (((int)threadIdx.x) & 1)) & 1) * 8)) + 49152)])), __pack_half2(((bfloat16_t)output_reg[(i_5 * 8)]), ((bfloat16_t)output_reg[((i_5 * 8) + 1)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 2)]), ((bfloat16_t)output_reg[((i_5 * 8) + 3)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 4)]), ((bfloat16_t)output_reg[((i_5 * 8) + 5)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 6)]), ((bfloat16_t)output_reg[((i_5 * 8) + 7)])));
      }
      tl::fence_proxy_async();
      tl::__sync_thread_partial<3, 128>();
      if (tl::tl_shuffle_elect<128>()) {
        tl::tma_store(Output_desc, (&(((bfloat16_t*)buf_dyn_shmem)[49152])), (w_1 * 128), 0);
        tl::tma_store_arrive();
        tl::tma_store_wait<0>();
        tl::tma_store(Output_desc, (&(((bfloat16_t*)buf_dyn_shmem)[53248])), ((w_1 * 128) + 64), 0);
        tl::tma_store_arrive();
        tl::tma_store_wait<0>();
      }
    }
  }
}


#define ERROR_BUF_SIZE 1024
static char error_buf[ERROR_BUF_SIZE];

extern "C" const char* get_last_error() {
    return error_buf;
}

extern "C" int init() {
    error_buf[0] = '\0';
    
    cudaError_t result_main_kernel = cudaFuncSetAttribute(main_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, 115200);
    if (result_main_kernel != CUDA_SUCCESS) {
        snprintf(error_buf, ERROR_BUF_SIZE, "Failed to set the allowed dynamic shared memory size to %d with error: %s", 115200, cudaGetErrorString(result_main_kernel));
        return -1;
    }

    return 0;
}

extern "C" int call(bfloat16_t* __restrict__ Input, bfloat16_t* __restrict__ W_T, bfloat16_t* __restrict__ Output, cudaStream_t stream=cudaStreamDefault) {

	CUtensorMap Input_desc;
	CUtensorMapDataType Input_desc_type= (CUtensorMapDataType)9;
	cuuint32_t Input_desc_tensorRank= 2;
	void *Input_desc_globalAddress= Input;
	cuuint64_t Input_desc_globalDim[2]= {1536,8};
	cuuint64_t Input_desc_globalStride[2]= {2,3072};
	cuuint32_t Input_desc_boxDim[2]= {64,64};
	cuuint32_t Input_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave Input_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle Input_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion Input_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill Input_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult Input_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &Input_desc, Input_desc_type, Input_desc_tensorRank, Input_desc_globalAddress, Input_desc_globalDim, Input_desc_globalStride + 1, Input_desc_boxDim, Input_desc_elementStrides, Input_desc_interleave, Input_desc_swizzle, Input_desc_l2Promotion, Input_desc_oobFill);

	if (Input_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor Input_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}

	CUtensorMap Output_desc;
	CUtensorMapDataType Output_desc_type= (CUtensorMapDataType)9;
	cuuint32_t Output_desc_tensorRank= 2;
	void *Output_desc_globalAddress= Output;
	cuuint64_t Output_desc_globalDim[2]= {1152,8};
	cuuint64_t Output_desc_globalStride[2]= {2,2304};
	cuuint32_t Output_desc_boxDim[2]= {64,64};
	cuuint32_t Output_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave Output_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle Output_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion Output_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill Output_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult Output_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &Output_desc, Output_desc_type, Output_desc_tensorRank, Output_desc_globalAddress, Output_desc_globalDim, Output_desc_globalStride + 1, Output_desc_boxDim, Output_desc_elementStrides, Output_desc_interleave, Output_desc_swizzle, Output_desc_l2Promotion, Output_desc_oobFill);

	if (Output_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor Output_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}

	CUtensorMap W_T_desc;
	CUtensorMapDataType W_T_desc_type= (CUtensorMapDataType)9;
	cuuint32_t W_T_desc_tensorRank= 2;
	void *W_T_desc_globalAddress= W_T;
	cuuint64_t W_T_desc_globalDim[2]= {1536,1152};
	cuuint64_t W_T_desc_globalStride[2]= {2,3072};
	cuuint32_t W_T_desc_boxDim[2]= {64,128};
	cuuint32_t W_T_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave W_T_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle W_T_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion W_T_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill W_T_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult W_T_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &W_T_desc, W_T_desc_type, W_T_desc_tensorRank, W_T_desc_globalAddress, W_T_desc_globalDim, W_T_desc_globalStride + 1, W_T_desc_boxDim, W_T_desc_elementStrides, W_T_desc_interleave, W_T_desc_swizzle, W_T_desc_l2Promotion, W_T_desc_oobFill);

	if (W_T_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor W_T_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}
	main_kernel<<<dim3(1, 1, 1), dim3(256, 1, 1), 115200, stream>>>(Input_desc, Output_desc, W_T_desc);
	TILELANG_CHECK_LAST_ERROR("main_kernel");

	return 0;
}

2025-10-20 06:33:13,293 DEBUG:Compilation failed for config {'block_M': 64, 'block_K': 128, 'block_N': 128, 'num_stages': 3, 'threads': 128} at index 12 with error: Initialization failed: Failed to set the allowed dynamic shared memory size to 164352 with error: invalid argument
#include <tl_templates/cuda/gemm.h>
#include <tl_templates/cuda/copy.h>
#include <tl_templates/cuda/reduce.h>
#include <tl_templates/cuda/ldsm.h>
#include <tl_templates/cuda/threadblock_swizzle.h>
#include <tl_templates/cuda/debug.h>
#ifdef ENABLE_BF16
#include <tl_templates/cuda/cuda_bf16_fallbacks.cuh>
#endif

extern "C" __global__ void main_kernel(__grid_constant__ const CUtensorMap Input_desc, __grid_constant__ const CUtensorMap Output_desc, __grid_constant__ const CUtensorMap W_T_desc);
extern "C" __global__ void __launch_bounds__(256, 1) main_kernel(__grid_constant__ const CUtensorMap Input_desc, __grid_constant__ const CUtensorMap Output_desc, __grid_constant__ const CUtensorMap W_T_desc) {
  extern __shared__ __align__(1024) uchar buf_dyn_shmem[];
  float output_reg[64];
  float squared_reg[64];
  float sum_reg[4];
  __shared__ uint64_t mbarrier_mem[6];
  auto mbarrier = reinterpret_cast<Barrier*>(mbarrier_mem);
  if (tl::tl_shuffle_elect<0>()) {
    tl::prefetch_tma_descriptor(Input_desc);
    tl::prefetch_tma_descriptor(W_T_desc);
    tl::prefetch_tma_descriptor(Output_desc);
    mbarrier[0].init(128);
    mbarrier[1].init(128);
    mbarrier[2].init(128);
    mbarrier[3].init(128);
    mbarrier[4].init(128);
    mbarrier[5].init(128);
  }
  __syncthreads();
  if (128 <= ((int)threadIdx.x)) {
    for (int w = 0; w < 9; ++w) {
      for (int k = 0; k < 12; ++k) {
        mbarrier[((k % 3) + 3)].wait((((k % 6) / 3) ^ 1));
        if (tl::tl_shuffle_elect<128>()) {
          mbarrier[(k % 3)].expect_transaction(16384);
          tl::tma_load(Input_desc, mbarrier[(k % 3)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k % 3) * 8192) + 49152)])), (k * 128), 0);
          tl::tma_load(Input_desc, mbarrier[(k % 3)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k % 3) * 8192) + 53248)])), ((k * 128) + 64), 0);
          mbarrier[(k % 3)].expect_transaction(32768);
          tl::tma_load(W_T_desc, mbarrier[(k % 3)], (&(((bfloat16_t*)buf_dyn_shmem)[((k % 3) * 16384)])), (k * 128), (w * 128));
          tl::tma_load(W_T_desc, mbarrier[(k % 3)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k % 3) * 16384) + 8192)])), ((k * 128) + 64), (w * 128));
        }
        mbarrier[(k % 3)].arrive();
      }
    }
  } else {
    for (int w_1 = 0; w_1 < 9; ++w_1) {
      #pragma unroll
      for (int i = 0; i < 32; ++i) {
        *(float2*)(output_reg + (i * 2)) = make_float2(0x0p+0f/*0.000000e+00*/, 0x0p+0f/*0.000000e+00*/);
      }
      tl::fence_proxy_async();
      for (int k_1 = 0; k_1 < 12; ++k_1) {
        mbarrier[(k_1 % 3)].wait(((k_1 % 6) / 3));
        tl::gemm_ss<64, 128, 128, 2, 2, 0, 1, 0, 128, 128, 0, 0>((&(((bfloat16_t*)buf_dyn_shmem)[(((k_1 % 3) * 8192) + 49152)])), (&(((bfloat16_t*)buf_dyn_shmem)[((k_1 % 3) * 16384)])), (&(output_reg[0])));
        mbarrier[((k_1 % 3) + 3)].arrive();
      }
      #pragma unroll
      for (int i_1 = 0; i_1 < 64; ++i_1) {
        output_reg[i_1] = (output_reg[i_1] / (0x1p+0f/*1.000000e+00*/ + expf((output_reg[i_1] * -0x1p+0f/*-1.000000e+00*/))));
      }
      #pragma unroll
      for (int i_2 = 0; i_2 < 64; ++i_2) {
        squared_reg[i_2] = (output_reg[i_2] * output_reg[i_2]);
      }
      tl::__sync_thread_partial<3, 128>();
      #pragma unroll
      for (int i_3 = 0; i_3 < 4; ++i_3) {
        sum_reg[i_3] = 0x0p+0f/*0.000000e+00*/;
        #pragma unroll
        for (int rv = 0; rv < 16; ++rv) {
          sum_reg[i_3] = (sum_reg[i_3] + squared_reg[((((rv & 7) * 8) + (i_3 * 2)) + (rv >> 3))]);
        }
        sum_reg[i_3] = tl::AllReduce<tl::SumOp, 128, 64, 0>::run(sum_reg[i_3], (&(((float*)buf_dyn_shmem)[40960])));
        sum_reg[i_3] = tl::AllReduce<tl::SumOp, 4, 1, 0>::run(sum_reg[i_3]);
      }
      #pragma unroll
      for (int i_4 = 0; i_4 < 64; ++i_4) {
        output_reg[i_4] = (output_reg[i_4] * rsqrtf(sum_reg[((i_4 & 7) >> 1)]));
      }
      #pragma unroll
      for (int i_5 = 0; i_5 < 8; ++i_5) {
        tl::ptx_stmatrix_x4((&(((bfloat16_t*)buf_dyn_shmem)[(((((((((i_5 >> 2) * 4096) + (((((int)threadIdx.x) & 31) >> 4) * 2048)) + (((((int)threadIdx.x) & 63) >> 5) * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + (((((((int)threadIdx.x) & 7) >> 2) + ((i_5 & 3) >> 1)) & 1) * 32)) + (((((((int)threadIdx.x) & 3) >> 1) + (i_5 & 1)) & 1) * 16)) + ((((((int)threadIdx.x) >> 6) + (((int)threadIdx.x) & 1)) & 1) * 8)) + 73728)])), __pack_half2(((bfloat16_t)output_reg[(i_5 * 8)]), ((bfloat16_t)output_reg[((i_5 * 8) + 1)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 2)]), ((bfloat16_t)output_reg[((i_5 * 8) + 3)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 4)]), ((bfloat16_t)output_reg[((i_5 * 8) + 5)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 6)]), ((bfloat16_t)output_reg[((i_5 * 8) + 7)])));
      }
      tl::fence_proxy_async();
      tl::__sync_thread_partial<3, 128>();
      if (tl::tl_shuffle_elect<128>()) {
        tl::tma_store(Output_desc, (&(((bfloat16_t*)buf_dyn_shmem)[73728])), (w_1 * 128), 0);
        tl::tma_store_arrive();
        tl::tma_store_wait<0>();
        tl::tma_store(Output_desc, (&(((bfloat16_t*)buf_dyn_shmem)[77824])), ((w_1 * 128) + 64), 0);
        tl::tma_store_arrive();
        tl::tma_store_wait<0>();
      }
    }
  }
}


#define ERROR_BUF_SIZE 1024
static char error_buf[ERROR_BUF_SIZE];

extern "C" const char* get_last_error() {
    return error_buf;
}

extern "C" int init() {
    error_buf[0] = '\0';
    
    cudaError_t result_main_kernel = cudaFuncSetAttribute(main_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, 164352);
    if (result_main_kernel != CUDA_SUCCESS) {
        snprintf(error_buf, ERROR_BUF_SIZE, "Failed to set the allowed dynamic shared memory size to %d with error: %s", 164352, cudaGetErrorString(result_main_kernel));
        return -1;
    }

    return 0;
}

extern "C" int call(bfloat16_t* __restrict__ Input, bfloat16_t* __restrict__ W_T, bfloat16_t* __restrict__ Output, cudaStream_t stream=cudaStreamDefault) {

	CUtensorMap Input_desc;
	CUtensorMapDataType Input_desc_type= (CUtensorMapDataType)9;
	cuuint32_t Input_desc_tensorRank= 2;
	void *Input_desc_globalAddress= Input;
	cuuint64_t Input_desc_globalDim[2]= {1536,8};
	cuuint64_t Input_desc_globalStride[2]= {2,3072};
	cuuint32_t Input_desc_boxDim[2]= {64,64};
	cuuint32_t Input_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave Input_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle Input_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion Input_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill Input_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult Input_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &Input_desc, Input_desc_type, Input_desc_tensorRank, Input_desc_globalAddress, Input_desc_globalDim, Input_desc_globalStride + 1, Input_desc_boxDim, Input_desc_elementStrides, Input_desc_interleave, Input_desc_swizzle, Input_desc_l2Promotion, Input_desc_oobFill);

	if (Input_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor Input_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}

	CUtensorMap Output_desc;
	CUtensorMapDataType Output_desc_type= (CUtensorMapDataType)9;
	cuuint32_t Output_desc_tensorRank= 2;
	void *Output_desc_globalAddress= Output;
	cuuint64_t Output_desc_globalDim[2]= {1152,8};
	cuuint64_t Output_desc_globalStride[2]= {2,2304};
	cuuint32_t Output_desc_boxDim[2]= {64,64};
	cuuint32_t Output_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave Output_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle Output_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion Output_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill Output_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult Output_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &Output_desc, Output_desc_type, Output_desc_tensorRank, Output_desc_globalAddress, Output_desc_globalDim, Output_desc_globalStride + 1, Output_desc_boxDim, Output_desc_elementStrides, Output_desc_interleave, Output_desc_swizzle, Output_desc_l2Promotion, Output_desc_oobFill);

	if (Output_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor Output_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}

	CUtensorMap W_T_desc;
	CUtensorMapDataType W_T_desc_type= (CUtensorMapDataType)9;
	cuuint32_t W_T_desc_tensorRank= 2;
	void *W_T_desc_globalAddress= W_T;
	cuuint64_t W_T_desc_globalDim[2]= {1536,1152};
	cuuint64_t W_T_desc_globalStride[2]= {2,3072};
	cuuint32_t W_T_desc_boxDim[2]= {64,128};
	cuuint32_t W_T_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave W_T_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle W_T_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion W_T_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill W_T_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult W_T_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &W_T_desc, W_T_desc_type, W_T_desc_tensorRank, W_T_desc_globalAddress, W_T_desc_globalDim, W_T_desc_globalStride + 1, W_T_desc_boxDim, W_T_desc_elementStrides, W_T_desc_interleave, W_T_desc_swizzle, W_T_desc_l2Promotion, W_T_desc_oobFill);

	if (W_T_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor W_T_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}
	main_kernel<<<dim3(1, 1, 1), dim3(256, 1, 1), 164352, stream>>>(Input_desc, Output_desc, W_T_desc);
	TILELANG_CHECK_LAST_ERROR("main_kernel");

	return 0;
}

2025-10-20 06:33:15,934 DEBUG:Compilation failed for config {'block_M': 64, 'block_K': 128, 'block_N': 128, 'num_stages': 3, 'threads': 256} at index 13 with error: Initialization failed: Failed to set the allowed dynamic shared memory size to 164864 with error: invalid argument
#include <tl_templates/cuda/gemm.h>
#include <tl_templates/cuda/copy.h>
#include <tl_templates/cuda/reduce.h>
#include <tl_templates/cuda/ldsm.h>
#include <tl_templates/cuda/threadblock_swizzle.h>
#include <tl_templates/cuda/debug.h>
#ifdef ENABLE_BF16
#include <tl_templates/cuda/cuda_bf16_fallbacks.cuh>
#endif

extern "C" __global__ void main_kernel(__grid_constant__ const CUtensorMap Input_desc, __grid_constant__ const CUtensorMap Output_desc, __grid_constant__ const CUtensorMap W_T_desc);
extern "C" __global__ void __launch_bounds__(384, 1) main_kernel(__grid_constant__ const CUtensorMap Input_desc, __grid_constant__ const CUtensorMap Output_desc, __grid_constant__ const CUtensorMap W_T_desc) {
  extern __shared__ __align__(1024) uchar buf_dyn_shmem[];
  float output_reg[32];
  float squared_reg[32];
  float sum_reg[4];
  __shared__ uint64_t mbarrier_mem[6];
  auto mbarrier = reinterpret_cast<Barrier*>(mbarrier_mem);
  if (tl::tl_shuffle_elect<0>()) {
    tl::prefetch_tma_descriptor(Input_desc);
    tl::prefetch_tma_descriptor(W_T_desc);
    tl::prefetch_tma_descriptor(Output_desc);
    mbarrier[0].init(128);
    mbarrier[1].init(128);
    mbarrier[2].init(128);
    mbarrier[3].init(256);
    mbarrier[4].init(256);
    mbarrier[5].init(256);
  }
  __syncthreads();
  if (256 <= ((int)threadIdx.x)) {
    for (int w = 0; w < 9; ++w) {
      for (int k = 0; k < 12; ++k) {
        mbarrier[((k % 3) + 3)].wait((((k % 6) / 3) ^ 1));
        if (tl::tl_shuffle_elect<128>()) {
          mbarrier[(k % 3)].expect_transaction(16384);
          tl::tma_load(Input_desc, mbarrier[(k % 3)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k % 3) * 8192) + 49152)])), (k * 128), 0);
          tl::tma_load(Input_desc, mbarrier[(k % 3)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k % 3) * 8192) + 53248)])), ((k * 128) + 64), 0);
          mbarrier[(k % 3)].expect_transaction(32768);
          tl::tma_load(W_T_desc, mbarrier[(k % 3)], (&(((bfloat16_t*)buf_dyn_shmem)[((k % 3) * 16384)])), (k * 128), (w * 128));
          tl::tma_load(W_T_desc, mbarrier[(k % 3)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k % 3) * 16384) + 8192)])), ((k * 128) + 64), (w * 128));
        }
        mbarrier[(k % 3)].arrive();
      }
    }
  } else {
    for (int w_1 = 0; w_1 < 9; ++w_1) {
      #pragma unroll
      for (int i = 0; i < 16; ++i) {
        *(float2*)(output_reg + (i * 2)) = make_float2(0x0p+0f/*0.000000e+00*/, 0x0p+0f/*0.000000e+00*/);
      }
      tl::fence_proxy_async();
      for (int k_1 = 0; k_1 < 12; ++k_1) {
        mbarrier[(k_1 % 3)].wait(((k_1 % 6) / 3));
        tl::gemm_ss<64, 128, 128, 2, 4, 0, 1, 0, 128, 128, 0, 0>((&(((bfloat16_t*)buf_dyn_shmem)[(((k_1 % 3) * 8192) + 49152)])), (&(((bfloat16_t*)buf_dyn_shmem)[((k_1 % 3) * 16384)])), (&(output_reg[0])));
        mbarrier[((k_1 % 3) + 3)].arrive();
      }
      #pragma unroll
      for (int i_1 = 0; i_1 < 32; ++i_1) {
        output_reg[i_1] = (output_reg[i_1] / (0x1p+0f/*1.000000e+00*/ + expf((output_reg[i_1] * -0x1p+0f/*-1.000000e+00*/))));
      }
      #pragma unroll
      for (int i_2 = 0; i_2 < 32; ++i_2) {
        squared_reg[i_2] = (output_reg[i_2] * output_reg[i_2]);
      }
      tl::__sync_thread_partial<3, 256>();
      #pragma unroll
      for (int i_3 = 0; i_3 < 4; ++i_3) {
        sum_reg[i_3] = 0x0p+0f/*0.000000e+00*/;
        #pragma unroll
        for (int rv = 0; rv < 8; ++rv) {
          sum_reg[i_3] = (sum_reg[i_3] + squared_reg[((((rv & 3) * 8) + (i_3 * 2)) + (rv >> 2))]);
        }
        sum_reg[i_3] = tl::AllReduce<tl::SumOp, 256, 64, 0>::run(sum_reg[i_3], (&(((float*)buf_dyn_shmem)[40960])));
        sum_reg[i_3] = tl::AllReduce<tl::SumOp, 4, 1, 0>::run(sum_reg[i_3]);
      }
      #pragma unroll
      for (int i_4 = 0; i_4 < 32; ++i_4) {
        output_reg[i_4] = (output_reg[i_4] * rsqrtf(sum_reg[((i_4 & 7) >> 1)]));
      }
      #pragma unroll
      for (int i_5 = 0; i_5 < 4; ++i_5) {
        tl::ptx_stmatrix_x4((&(((bfloat16_t*)buf_dyn_shmem)[(((((((((i_5 >> 1) * 4096) + (((((int)threadIdx.x) & 31) >> 4) * 2048)) + (((((int)threadIdx.x) & 63) >> 5) * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + (((((((int)threadIdx.x) & 7) >> 2) + (i_5 & 1)) & 1) * 32)) + ((((((int)threadIdx.x) >> 7) + ((((int)threadIdx.x) & 3) >> 1)) & 1) * 16)) + (((((((int)threadIdx.x) & 127) >> 6) + (((int)threadIdx.x) & 1)) & 1) * 8)) + 73728)])), __pack_half2(((bfloat16_t)output_reg[(i_5 * 8)]), ((bfloat16_t)output_reg[((i_5 * 8) + 1)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 2)]), ((bfloat16_t)output_reg[((i_5 * 8) + 3)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 4)]), ((bfloat16_t)output_reg[((i_5 * 8) + 5)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 6)]), ((bfloat16_t)output_reg[((i_5 * 8) + 7)])));
      }
      tl::fence_proxy_async();
      tl::__sync_thread_partial<3, 256>();
      if (tl::tl_shuffle_elect<256>()) {
        tl::tma_store(Output_desc, (&(((bfloat16_t*)buf_dyn_shmem)[73728])), (w_1 * 128), 0);
        tl::tma_store_arrive();
        tl::tma_store_wait<0>();
        tl::tma_store(Output_desc, (&(((bfloat16_t*)buf_dyn_shmem)[77824])), ((w_1 * 128) + 64), 0);
        tl::tma_store_arrive();
        tl::tma_store_wait<0>();
      }
    }
  }
}


#define ERROR_BUF_SIZE 1024
static char error_buf[ERROR_BUF_SIZE];

extern "C" const char* get_last_error() {
    return error_buf;
}

extern "C" int init() {
    error_buf[0] = '\0';
    
    cudaError_t result_main_kernel = cudaFuncSetAttribute(main_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, 164864);
    if (result_main_kernel != CUDA_SUCCESS) {
        snprintf(error_buf, ERROR_BUF_SIZE, "Failed to set the allowed dynamic shared memory size to %d with error: %s", 164864, cudaGetErrorString(result_main_kernel));
        return -1;
    }

    return 0;
}

extern "C" int call(bfloat16_t* __restrict__ Input, bfloat16_t* __restrict__ W_T, bfloat16_t* __restrict__ Output, cudaStream_t stream=cudaStreamDefault) {

	CUtensorMap Input_desc;
	CUtensorMapDataType Input_desc_type= (CUtensorMapDataType)9;
	cuuint32_t Input_desc_tensorRank= 2;
	void *Input_desc_globalAddress= Input;
	cuuint64_t Input_desc_globalDim[2]= {1536,8};
	cuuint64_t Input_desc_globalStride[2]= {2,3072};
	cuuint32_t Input_desc_boxDim[2]= {64,64};
	cuuint32_t Input_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave Input_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle Input_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion Input_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill Input_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult Input_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &Input_desc, Input_desc_type, Input_desc_tensorRank, Input_desc_globalAddress, Input_desc_globalDim, Input_desc_globalStride + 1, Input_desc_boxDim, Input_desc_elementStrides, Input_desc_interleave, Input_desc_swizzle, Input_desc_l2Promotion, Input_desc_oobFill);

	if (Input_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor Input_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}

	CUtensorMap Output_desc;
	CUtensorMapDataType Output_desc_type= (CUtensorMapDataType)9;
	cuuint32_t Output_desc_tensorRank= 2;
	void *Output_desc_globalAddress= Output;
	cuuint64_t Output_desc_globalDim[2]= {1152,8};
	cuuint64_t Output_desc_globalStride[2]= {2,2304};
	cuuint32_t Output_desc_boxDim[2]= {64,64};
	cuuint32_t Output_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave Output_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle Output_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion Output_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill Output_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult Output_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &Output_desc, Output_desc_type, Output_desc_tensorRank, Output_desc_globalAddress, Output_desc_globalDim, Output_desc_globalStride + 1, Output_desc_boxDim, Output_desc_elementStrides, Output_desc_interleave, Output_desc_swizzle, Output_desc_l2Promotion, Output_desc_oobFill);

	if (Output_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor Output_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}

	CUtensorMap W_T_desc;
	CUtensorMapDataType W_T_desc_type= (CUtensorMapDataType)9;
	cuuint32_t W_T_desc_tensorRank= 2;
	void *W_T_desc_globalAddress= W_T;
	cuuint64_t W_T_desc_globalDim[2]= {1536,1152};
	cuuint64_t W_T_desc_globalStride[2]= {2,3072};
	cuuint32_t W_T_desc_boxDim[2]= {64,128};
	cuuint32_t W_T_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave W_T_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle W_T_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion W_T_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill W_T_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult W_T_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &W_T_desc, W_T_desc_type, W_T_desc_tensorRank, W_T_desc_globalAddress, W_T_desc_globalDim, W_T_desc_globalStride + 1, W_T_desc_boxDim, W_T_desc_elementStrides, W_T_desc_interleave, W_T_desc_swizzle, W_T_desc_l2Promotion, W_T_desc_oobFill);

	if (W_T_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor W_T_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}
	main_kernel<<<dim3(1, 1, 1), dim3(384, 1, 1), 164864, stream>>>(Input_desc, Output_desc, W_T_desc);
	TILELANG_CHECK_LAST_ERROR("main_kernel");

	return 0;
}

2025-10-20 06:33:17,510 DEBUG:Compilation failed for config {'block_M': 64, 'block_K': 128, 'block_N': 128, 'num_stages': 2, 'threads': 128} at index 10 with error: Initialization failed: Failed to set the allowed dynamic shared memory size to 115200 with error: invalid argument
#include <tl_templates/cuda/gemm.h>
#include <tl_templates/cuda/copy.h>
#include <tl_templates/cuda/reduce.h>
#include <tl_templates/cuda/ldsm.h>
#include <tl_templates/cuda/threadblock_swizzle.h>
#include <tl_templates/cuda/debug.h>
#ifdef ENABLE_BF16
#include <tl_templates/cuda/cuda_bf16_fallbacks.cuh>
#endif

extern "C" __global__ void main_kernel(__grid_constant__ const CUtensorMap Input_desc, __grid_constant__ const CUtensorMap Output_desc, __grid_constant__ const CUtensorMap W_T_desc);
extern "C" __global__ void __launch_bounds__(256, 1) main_kernel(__grid_constant__ const CUtensorMap Input_desc, __grid_constant__ const CUtensorMap Output_desc, __grid_constant__ const CUtensorMap W_T_desc) {
  extern __shared__ __align__(1024) uchar buf_dyn_shmem[];
  float output_reg[64];
  float squared_reg[64];
  float sum_reg[4];
  __shared__ uint64_t mbarrier_mem[4];
  auto mbarrier = reinterpret_cast<Barrier*>(mbarrier_mem);
  if (tl::tl_shuffle_elect<0>()) {
    tl::prefetch_tma_descriptor(Input_desc);
    tl::prefetch_tma_descriptor(W_T_desc);
    tl::prefetch_tma_descriptor(Output_desc);
    mbarrier[0].init(128);
    mbarrier[1].init(128);
    mbarrier[2].init(128);
    mbarrier[3].init(128);
  }
  __syncthreads();
  if (128 <= ((int)threadIdx.x)) {
    for (int w = 0; w < 9; ++w) {
      for (int k = 0; k < 12; ++k) {
        mbarrier[((k & 1) + 2)].wait((((k & 3) >> 1) ^ 1));
        if (tl::tl_shuffle_elect<128>()) {
          mbarrier[(k & 1)].expect_transaction(16384);
          tl::tma_load(Input_desc, mbarrier[(k & 1)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k & 1) * 8192) + 32768)])), (k * 128), 0);
          tl::tma_load(Input_desc, mbarrier[(k & 1)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k & 1) * 8192) + 36864)])), ((k * 128) + 64), 0);
          mbarrier[(k & 1)].expect_transaction(32768);
          tl::tma_load(W_T_desc, mbarrier[(k & 1)], (&(((bfloat16_t*)buf_dyn_shmem)[((k & 1) * 16384)])), (k * 128), (w * 128));
          tl::tma_load(W_T_desc, mbarrier[(k & 1)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k & 1) * 16384) + 8192)])), ((k * 128) + 64), (w * 128));
        }
        mbarrier[(k & 1)].arrive();
      }
    }
  } else {
    for (int w_1 = 0; w_1 < 9; ++w_1) {
      #pragma unroll
      for (int i = 0; i < 32; ++i) {
        *(float2*)(output_reg + (i * 2)) = make_float2(0x0p+0f/*0.000000e+00*/, 0x0p+0f/*0.000000e+00*/);
      }
      tl::fence_proxy_async();
      for (int k_1 = 0; k_1 < 12; ++k_1) {
        mbarrier[(k_1 & 1)].wait(((k_1 & 3) >> 1));
        tl::gemm_ss<64, 128, 128, 2, 2, 0, 1, 0, 128, 128, 0, 0>((&(((bfloat16_t*)buf_dyn_shmem)[(((k_1 & 1) * 8192) + 32768)])), (&(((bfloat16_t*)buf_dyn_shmem)[((k_1 & 1) * 16384)])), (&(output_reg[0])));
        mbarrier[((k_1 & 1) + 2)].arrive();
      }
      #pragma unroll
      for (int i_1 = 0; i_1 < 64; ++i_1) {
        output_reg[i_1] = (output_reg[i_1] / (0x1p+0f/*1.000000e+00*/ + expf((output_reg[i_1] * -0x1p+0f/*-1.000000e+00*/))));
      }
      #pragma unroll
      for (int i_2 = 0; i_2 < 64; ++i_2) {
        squared_reg[i_2] = (output_reg[i_2] * output_reg[i_2]);
      }
      tl::__sync_thread_partial<3, 128>();
      #pragma unroll
      for (int i_3 = 0; i_3 < 4; ++i_3) {
        sum_reg[i_3] = 0x0p+0f/*0.000000e+00*/;
        #pragma unroll
        for (int rv = 0; rv < 16; ++rv) {
          sum_reg[i_3] = (sum_reg[i_3] + squared_reg[((((rv & 7) * 8) + (i_3 * 2)) + (rv >> 3))]);
        }
        sum_reg[i_3] = tl::AllReduce<tl::SumOp, 128, 64, 0>::run(sum_reg[i_3], (&(((float*)buf_dyn_shmem)[28672])));
        sum_reg[i_3] = tl::AllReduce<tl::SumOp, 4, 1, 0>::run(sum_reg[i_3]);
      }
      #pragma unroll
      for (int i_4 = 0; i_4 < 64; ++i_4) {
        output_reg[i_4] = (output_reg[i_4] * rsqrtf(sum_reg[((i_4 & 7) >> 1)]));
      }
      #pragma unroll
      for (int i_5 = 0; i_5 < 8; ++i_5) {
        tl::ptx_stmatrix_x4((&(((bfloat16_t*)buf_dyn_shmem)[(((((((((i_5 >> 2) * 4096) + (((((int)threadIdx.x) & 31) >> 4) * 2048)) + (((((int)threadIdx.x) & 63) >> 5) * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + (((((((int)threadIdx.x) & 7) >> 2) + ((i_5 & 3) >> 1)) & 1) * 32)) + (((((((int)threadIdx.x) & 3) >> 1) + (i_5 & 1)) & 1) * 16)) + ((((((int)threadIdx.x) >> 6) + (((int)threadIdx.x) & 1)) & 1) * 8)) + 49152)])), __pack_half2(((bfloat16_t)output_reg[(i_5 * 8)]), ((bfloat16_t)output_reg[((i_5 * 8) + 1)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 2)]), ((bfloat16_t)output_reg[((i_5 * 8) + 3)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 4)]), ((bfloat16_t)output_reg[((i_5 * 8) + 5)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 6)]), ((bfloat16_t)output_reg[((i_5 * 8) + 7)])));
      }
      tl::fence_proxy_async();
      tl::__sync_thread_partial<3, 128>();
      if (tl::tl_shuffle_elect<128>()) {
        tl::tma_store(Output_desc, (&(((bfloat16_t*)buf_dyn_shmem)[49152])), (w_1 * 128), 0);
        tl::tma_store_arrive();
        tl::tma_store_wait<0>();
        tl::tma_store(Output_desc, (&(((bfloat16_t*)buf_dyn_shmem)[53248])), ((w_1 * 128) + 64), 0);
        tl::tma_store_arrive();
        tl::tma_store_wait<0>();
      }
    }
  }
}


#define ERROR_BUF_SIZE 1024
static char error_buf[ERROR_BUF_SIZE];

extern "C" const char* get_last_error() {
    return error_buf;
}

extern "C" int init() {
    error_buf[0] = '\0';
    
    cudaError_t result_main_kernel = cudaFuncSetAttribute(main_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, 115200);
    if (result_main_kernel != CUDA_SUCCESS) {
        snprintf(error_buf, ERROR_BUF_SIZE, "Failed to set the allowed dynamic shared memory size to %d with error: %s", 115200, cudaGetErrorString(result_main_kernel));
        return -1;
    }

    return 0;
}

extern "C" int call(bfloat16_t* __restrict__ Input, bfloat16_t* __restrict__ W_T, bfloat16_t* __restrict__ Output, cudaStream_t stream=cudaStreamDefault) {

	CUtensorMap Input_desc;
	CUtensorMapDataType Input_desc_type= (CUtensorMapDataType)9;
	cuuint32_t Input_desc_tensorRank= 2;
	void *Input_desc_globalAddress= Input;
	cuuint64_t Input_desc_globalDim[2]= {1536,8};
	cuuint64_t Input_desc_globalStride[2]= {2,3072};
	cuuint32_t Input_desc_boxDim[2]= {64,64};
	cuuint32_t Input_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave Input_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle Input_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion Input_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill Input_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult Input_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &Input_desc, Input_desc_type, Input_desc_tensorRank, Input_desc_globalAddress, Input_desc_globalDim, Input_desc_globalStride + 1, Input_desc_boxDim, Input_desc_elementStrides, Input_desc_interleave, Input_desc_swizzle, Input_desc_l2Promotion, Input_desc_oobFill);

	if (Input_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor Input_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}

	CUtensorMap Output_desc;
	CUtensorMapDataType Output_desc_type= (CUtensorMapDataType)9;
	cuuint32_t Output_desc_tensorRank= 2;
	void *Output_desc_globalAddress= Output;
	cuuint64_t Output_desc_globalDim[2]= {1152,8};
	cuuint64_t Output_desc_globalStride[2]= {2,2304};
	cuuint32_t Output_desc_boxDim[2]= {64,64};
	cuuint32_t Output_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave Output_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle Output_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion Output_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill Output_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult Output_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &Output_desc, Output_desc_type, Output_desc_tensorRank, Output_desc_globalAddress, Output_desc_globalDim, Output_desc_globalStride + 1, Output_desc_boxDim, Output_desc_elementStrides, Output_desc_interleave, Output_desc_swizzle, Output_desc_l2Promotion, Output_desc_oobFill);

	if (Output_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor Output_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}

	CUtensorMap W_T_desc;
	CUtensorMapDataType W_T_desc_type= (CUtensorMapDataType)9;
	cuuint32_t W_T_desc_tensorRank= 2;
	void *W_T_desc_globalAddress= W_T;
	cuuint64_t W_T_desc_globalDim[2]= {1536,1152};
	cuuint64_t W_T_desc_globalStride[2]= {2,3072};
	cuuint32_t W_T_desc_boxDim[2]= {64,128};
	cuuint32_t W_T_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave W_T_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle W_T_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion W_T_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill W_T_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult W_T_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &W_T_desc, W_T_desc_type, W_T_desc_tensorRank, W_T_desc_globalAddress, W_T_desc_globalDim, W_T_desc_globalStride + 1, W_T_desc_boxDim, W_T_desc_elementStrides, W_T_desc_interleave, W_T_desc_swizzle, W_T_desc_l2Promotion, W_T_desc_oobFill);

	if (W_T_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor W_T_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}
	main_kernel<<<dim3(1, 1, 1), dim3(256, 1, 1), 115200, stream>>>(Input_desc, Output_desc, W_T_desc);
	TILELANG_CHECK_LAST_ERROR("main_kernel");

	return 0;
}

2025-10-20 06:33:22,130 DEBUG:Compilation failed for config {'block_M': 128, 'block_K': 64, 'block_N': 128, 'num_stages': 3, 'threads': 128} at index 20 with error: Initialization failed: Failed to set the allowed dynamic shared memory size to 131584 with error: invalid argument
#include <tl_templates/cuda/gemm.h>
#include <tl_templates/cuda/copy.h>
#include <tl_templates/cuda/reduce.h>
#include <tl_templates/cuda/ldsm.h>
#include <tl_templates/cuda/threadblock_swizzle.h>
#include <tl_templates/cuda/debug.h>
#ifdef ENABLE_BF16
#include <tl_templates/cuda/cuda_bf16_fallbacks.cuh>
#endif

extern "C" __global__ void main_kernel(__grid_constant__ const CUtensorMap Input_desc, __grid_constant__ const CUtensorMap Output_desc, __grid_constant__ const CUtensorMap W_T_desc);
extern "C" __global__ void __launch_bounds__(256, 1) main_kernel(__grid_constant__ const CUtensorMap Input_desc, __grid_constant__ const CUtensorMap Output_desc, __grid_constant__ const CUtensorMap W_T_desc) {
  extern __shared__ __align__(1024) uchar buf_dyn_shmem[];
  float output_reg[128];
  float squared_reg[128];
  float sum_reg[8];
  __shared__ uint64_t mbarrier_mem[6];
  auto mbarrier = reinterpret_cast<Barrier*>(mbarrier_mem);
  if (tl::tl_shuffle_elect<0>()) {
    tl::prefetch_tma_descriptor(Input_desc);
    tl::prefetch_tma_descriptor(W_T_desc);
    tl::prefetch_tma_descriptor(Output_desc);
    mbarrier[0].init(128);
    mbarrier[1].init(128);
    mbarrier[2].init(128);
    mbarrier[3].init(128);
    mbarrier[4].init(128);
    mbarrier[5].init(128);
  }
  __syncthreads();
  if (128 <= ((int)threadIdx.x)) {
    for (int w = 0; w < 9; ++w) {
      for (int k = 0; k < 24; ++k) {
        mbarrier[((k % 3) + 3)].wait((((k % 6) / 3) ^ 1));
        if (tl::tl_shuffle_elect<128>()) {
          mbarrier[(k % 3)].expect_transaction(16384);
          tl::tma_load(Input_desc, mbarrier[(k % 3)], (&(((bfloat16_t*)buf_dyn_shmem)[((k % 3) * 8192)])), (k * 64), 0);
          mbarrier[(k % 3)].expect_transaction(16384);
          tl::tma_load(W_T_desc, mbarrier[(k % 3)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k % 3) * 8192) + 24576)])), (k * 64), (w * 128));
        }
        mbarrier[(k % 3)].arrive();
      }
    }
  } else {
    for (int w_1 = 0; w_1 < 9; ++w_1) {
      #pragma unroll
      for (int i = 0; i < 64; ++i) {
        *(float2*)(output_reg + (i * 2)) = make_float2(0x0p+0f/*0.000000e+00*/, 0x0p+0f/*0.000000e+00*/);
      }
      tl::fence_proxy_async();
      for (int k_1 = 0; k_1 < 24; ++k_1) {
        mbarrier[(k_1 % 3)].wait(((k_1 % 6) / 3));
        tl::gemm_ss<128, 128, 64, 2, 2, 0, 1, 0, 64, 64, 0, 0>((&(((bfloat16_t*)buf_dyn_shmem)[((k_1 % 3) * 8192)])), (&(((bfloat16_t*)buf_dyn_shmem)[(((k_1 % 3) * 8192) + 24576)])), (&(output_reg[0])));
        mbarrier[((k_1 % 3) + 3)].arrive();
      }
      #pragma unroll
      for (int i_1 = 0; i_1 < 128; ++i_1) {
        output_reg[i_1] = (output_reg[i_1] / (0x1p+0f/*1.000000e+00*/ + expf((output_reg[i_1] * -0x1p+0f/*-1.000000e+00*/))));
      }
      #pragma unroll
      for (int i_2 = 0; i_2 < 128; ++i_2) {
        squared_reg[i_2] = (output_reg[i_2] * output_reg[i_2]);
      }
      tl::__sync_thread_partial<3, 128>();
      #pragma unroll
      for (int i_3 = 0; i_3 < 8; ++i_3) {
        sum_reg[i_3] = 0x0p+0f/*0.000000e+00*/;
        #pragma unroll
        for (int rv = 0; rv < 16; ++rv) {
          sum_reg[i_3] = (sum_reg[i_3] + squared_reg[((((rv & 7) * 16) + (i_3 * 2)) + (rv >> 3))]);
        }
        sum_reg[i_3] = tl::AllReduce<tl::SumOp, 128, 64, 0>::run(sum_reg[i_3], (&(((float*)buf_dyn_shmem)[32768])));
        sum_reg[i_3] = tl::AllReduce<tl::SumOp, 4, 1, 0>::run(sum_reg[i_3]);
      }
      #pragma unroll
      for (int i_4 = 0; i_4 < 128; ++i_4) {
        output_reg[i_4] = (output_reg[i_4] * rsqrtf(sum_reg[((i_4 & 15) >> 1)]));
      }
      #pragma unroll
      for (int i_5 = 0; i_5 < 16; ++i_5) {
        tl::ptx_stmatrix_x4((&(((bfloat16_t*)buf_dyn_shmem)[((((((((((i_5 >> 3) * 8192) + ((i_5 & 1) * 4096)) + (((((int)threadIdx.x) & 31) >> 4) * 2048)) + (((((int)threadIdx.x) & 63) >> 5) * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + (((((i_5 & 7) >> 2) + ((((int)threadIdx.x) & 7) >> 2)) & 1) * 32)) + (((((i_5 & 3) >> 1) + ((((int)threadIdx.x) & 3) >> 1)) & 1) * 16)) + ((((((int)threadIdx.x) >> 6) + (((int)threadIdx.x) & 1)) & 1) * 8)) + 49152)])), __pack_half2(((bfloat16_t)output_reg[(i_5 * 8)]), ((bfloat16_t)output_reg[((i_5 * 8) + 1)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 2)]), ((bfloat16_t)output_reg[((i_5 * 8) + 3)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 4)]), ((bfloat16_t)output_reg[((i_5 * 8) + 5)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 6)]), ((bfloat16_t)output_reg[((i_5 * 8) + 7)])));
      }
      tl::fence_proxy_async();
      tl::__sync_thread_partial<3, 128>();
      if (tl::tl_shuffle_elect<128>()) {
        tl::tma_store(Output_desc, (&(((bfloat16_t*)buf_dyn_shmem)[49152])), (w_1 * 128), 0);
        tl::tma_store_arrive();
        tl::tma_store_wait<0>();
        tl::tma_store(Output_desc, (&(((bfloat16_t*)buf_dyn_shmem)[57344])), ((w_1 * 128) + 64), 0);
        tl::tma_store_arrive();
        tl::tma_store_wait<0>();
      }
    }
  }
}


#define ERROR_BUF_SIZE 1024
static char error_buf[ERROR_BUF_SIZE];

extern "C" const char* get_last_error() {
    return error_buf;
}

extern "C" int init() {
    error_buf[0] = '\0';
    
    cudaError_t result_main_kernel = cudaFuncSetAttribute(main_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, 131584);
    if (result_main_kernel != CUDA_SUCCESS) {
        snprintf(error_buf, ERROR_BUF_SIZE, "Failed to set the allowed dynamic shared memory size to %d with error: %s", 131584, cudaGetErrorString(result_main_kernel));
        return -1;
    }

    return 0;
}

extern "C" int call(bfloat16_t* __restrict__ Input, bfloat16_t* __restrict__ W_T, bfloat16_t* __restrict__ Output, cudaStream_t stream=cudaStreamDefault) {

	CUtensorMap Input_desc;
	CUtensorMapDataType Input_desc_type= (CUtensorMapDataType)9;
	cuuint32_t Input_desc_tensorRank= 2;
	void *Input_desc_globalAddress= Input;
	cuuint64_t Input_desc_globalDim[2]= {1536,8};
	cuuint64_t Input_desc_globalStride[2]= {2,3072};
	cuuint32_t Input_desc_boxDim[2]= {64,128};
	cuuint32_t Input_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave Input_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle Input_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion Input_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill Input_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult Input_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &Input_desc, Input_desc_type, Input_desc_tensorRank, Input_desc_globalAddress, Input_desc_globalDim, Input_desc_globalStride + 1, Input_desc_boxDim, Input_desc_elementStrides, Input_desc_interleave, Input_desc_swizzle, Input_desc_l2Promotion, Input_desc_oobFill);

	if (Input_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor Input_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}

	CUtensorMap Output_desc;
	CUtensorMapDataType Output_desc_type= (CUtensorMapDataType)9;
	cuuint32_t Output_desc_tensorRank= 2;
	void *Output_desc_globalAddress= Output;
	cuuint64_t Output_desc_globalDim[2]= {1152,8};
	cuuint64_t Output_desc_globalStride[2]= {2,2304};
	cuuint32_t Output_desc_boxDim[2]= {64,128};
	cuuint32_t Output_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave Output_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle Output_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion Output_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill Output_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult Output_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &Output_desc, Output_desc_type, Output_desc_tensorRank, Output_desc_globalAddress, Output_desc_globalDim, Output_desc_globalStride + 1, Output_desc_boxDim, Output_desc_elementStrides, Output_desc_interleave, Output_desc_swizzle, Output_desc_l2Promotion, Output_desc_oobFill);

	if (Output_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor Output_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}

	CUtensorMap W_T_desc;
	CUtensorMapDataType W_T_desc_type= (CUtensorMapDataType)9;
	cuuint32_t W_T_desc_tensorRank= 2;
	void *W_T_desc_globalAddress= W_T;
	cuuint64_t W_T_desc_globalDim[2]= {1536,1152};
	cuuint64_t W_T_desc_globalStride[2]= {2,3072};
	cuuint32_t W_T_desc_boxDim[2]= {64,128};
	cuuint32_t W_T_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave W_T_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle W_T_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion W_T_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill W_T_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult W_T_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &W_T_desc, W_T_desc_type, W_T_desc_tensorRank, W_T_desc_globalAddress, W_T_desc_globalDim, W_T_desc_globalStride + 1, W_T_desc_boxDim, W_T_desc_elementStrides, W_T_desc_interleave, W_T_desc_swizzle, W_T_desc_l2Promotion, W_T_desc_oobFill);

	if (W_T_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor W_T_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}
	main_kernel<<<dim3(1, 1, 1), dim3(256, 1, 1), 131584, stream>>>(Input_desc, Output_desc, W_T_desc);
	TILELANG_CHECK_LAST_ERROR("main_kernel");

	return 0;
}

2025-10-20 06:33:22,166 DEBUG:Compilation failed for config {'block_M': 128, 'block_K': 64, 'block_N': 128, 'num_stages': 4, 'threads': 256} at index 23 with error: Initialization failed: Failed to set the allowed dynamic shared memory size to 164864 with error: invalid argument
#include <tl_templates/cuda/gemm.h>
#include <tl_templates/cuda/copy.h>
#include <tl_templates/cuda/reduce.h>
#include <tl_templates/cuda/ldsm.h>
#include <tl_templates/cuda/threadblock_swizzle.h>
#include <tl_templates/cuda/debug.h>
#ifdef ENABLE_BF16
#include <tl_templates/cuda/cuda_bf16_fallbacks.cuh>
#endif

extern "C" __global__ void main_kernel(__grid_constant__ const CUtensorMap Input_desc, __grid_constant__ const CUtensorMap Output_desc, __grid_constant__ const CUtensorMap W_T_desc);
extern "C" __global__ void __launch_bounds__(384, 1) main_kernel(__grid_constant__ const CUtensorMap Input_desc, __grid_constant__ const CUtensorMap Output_desc, __grid_constant__ const CUtensorMap W_T_desc) {
  extern __shared__ __align__(1024) uchar buf_dyn_shmem[];
  float output_reg[64];
  float squared_reg[64];
  float sum_reg[8];
  __shared__ uint64_t mbarrier_mem[8];
  auto mbarrier = reinterpret_cast<Barrier*>(mbarrier_mem);
  if (tl::tl_shuffle_elect<0>()) {
    tl::prefetch_tma_descriptor(Input_desc);
    tl::prefetch_tma_descriptor(W_T_desc);
    tl::prefetch_tma_descriptor(Output_desc);
    mbarrier[0].init(128);
    mbarrier[1].init(128);
    mbarrier[2].init(128);
    mbarrier[3].init(128);
    mbarrier[4].init(256);
    mbarrier[5].init(256);
    mbarrier[6].init(256);
    mbarrier[7].init(256);
  }
  __syncthreads();
  if (256 <= ((int)threadIdx.x)) {
    for (int w = 0; w < 9; ++w) {
      for (int k = 0; k < 24; ++k) {
        mbarrier[((k & 3) + 4)].wait((((k & 7) >> 2) ^ 1));
        if (tl::tl_shuffle_elect<128>()) {
          mbarrier[(k & 3)].expect_transaction(16384);
          tl::tma_load(Input_desc, mbarrier[(k & 3)], (&(((bfloat16_t*)buf_dyn_shmem)[((k & 3) * 8192)])), (k * 64), 0);
          mbarrier[(k & 3)].expect_transaction(16384);
          tl::tma_load(W_T_desc, mbarrier[(k & 3)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k & 3) * 8192) + 32768)])), (k * 64), (w * 128));
        }
        mbarrier[(k & 3)].arrive();
      }
    }
  } else {
    for (int w_1 = 0; w_1 < 9; ++w_1) {
      #pragma unroll
      for (int i = 0; i < 32; ++i) {
        *(float2*)(output_reg + (i * 2)) = make_float2(0x0p+0f/*0.000000e+00*/, 0x0p+0f/*0.000000e+00*/);
      }
      tl::fence_proxy_async();
      for (int k_1 = 0; k_1 < 24; ++k_1) {
        mbarrier[(k_1 & 3)].wait(((k_1 & 7) >> 2));
        tl::gemm_ss<128, 128, 64, 2, 4, 0, 1, 0, 64, 64, 0, 0>((&(((bfloat16_t*)buf_dyn_shmem)[((k_1 & 3) * 8192)])), (&(((bfloat16_t*)buf_dyn_shmem)[(((k_1 & 3) * 8192) + 32768)])), (&(output_reg[0])));
        mbarrier[((k_1 & 3) + 4)].arrive();
      }
      #pragma unroll
      for (int i_1 = 0; i_1 < 64; ++i_1) {
        output_reg[i_1] = (output_reg[i_1] / (0x1p+0f/*1.000000e+00*/ + expf((output_reg[i_1] * -0x1p+0f/*-1.000000e+00*/))));
      }
      #pragma unroll
      for (int i_2 = 0; i_2 < 64; ++i_2) {
        squared_reg[i_2] = (output_reg[i_2] * output_reg[i_2]);
      }
      tl::__sync_thread_partial<3, 256>();
      #pragma unroll
      for (int i_3 = 0; i_3 < 8; ++i_3) {
        sum_reg[i_3] = 0x0p+0f/*0.000000e+00*/;
        #pragma unroll
        for (int rv = 0; rv < 8; ++rv) {
          sum_reg[i_3] = (sum_reg[i_3] + squared_reg[((((rv & 3) * 16) + (i_3 * 2)) + (rv >> 2))]);
        }
        sum_reg[i_3] = tl::AllReduce<tl::SumOp, 256, 64, 0>::run(sum_reg[i_3], (&(((float*)buf_dyn_shmem)[40960])));
        sum_reg[i_3] = tl::AllReduce<tl::SumOp, 4, 1, 0>::run(sum_reg[i_3]);
      }
      #pragma unroll
      for (int i_4 = 0; i_4 < 64; ++i_4) {
        output_reg[i_4] = (output_reg[i_4] * rsqrtf(sum_reg[((i_4 & 15) >> 1)]));
      }
      #pragma unroll
      for (int i_5 = 0; i_5 < 8; ++i_5) {
        tl::ptx_stmatrix_x4((&(((bfloat16_t*)buf_dyn_shmem)[((((((((((i_5 >> 2) * 8192) + ((i_5 & 1) * 4096)) + (((((int)threadIdx.x) & 31) >> 4) * 2048)) + (((((int)threadIdx.x) & 63) >> 5) * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + (((((((int)threadIdx.x) & 7) >> 2) + ((i_5 & 3) >> 1)) & 1) * 32)) + ((((((int)threadIdx.x) >> 7) + ((((int)threadIdx.x) & 3) >> 1)) & 1) * 16)) + (((((((int)threadIdx.x) & 127) >> 6) + (((int)threadIdx.x) & 1)) & 1) * 8)) + 65536)])), __pack_half2(((bfloat16_t)output_reg[(i_5 * 8)]), ((bfloat16_t)output_reg[((i_5 * 8) + 1)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 2)]), ((bfloat16_t)output_reg[((i_5 * 8) + 3)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 4)]), ((bfloat16_t)output_reg[((i_5 * 8) + 5)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 6)]), ((bfloat16_t)output_reg[((i_5 * 8) + 7)])));
      }
      tl::fence_proxy_async();
      tl::__sync_thread_partial<3, 256>();
      if (tl::tl_shuffle_elect<256>()) {
        tl::tma_store(Output_desc, (&(((bfloat16_t*)buf_dyn_shmem)[65536])), (w_1 * 128), 0);
        tl::tma_store_arrive();
        tl::tma_store_wait<0>();
        tl::tma_store(Output_desc, (&(((bfloat16_t*)buf_dyn_shmem)[73728])), ((w_1 * 128) + 64), 0);
        tl::tma_store_arrive();
        tl::tma_store_wait<0>();
      }
    }
  }
}


#define ERROR_BUF_SIZE 1024
static char error_buf[ERROR_BUF_SIZE];

extern "C" const char* get_last_error() {
    return error_buf;
}

extern "C" int init() {
    error_buf[0] = '\0';
    
    cudaError_t result_main_kernel = cudaFuncSetAttribute(main_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, 164864);
    if (result_main_kernel != CUDA_SUCCESS) {
        snprintf(error_buf, ERROR_BUF_SIZE, "Failed to set the allowed dynamic shared memory size to %d with error: %s", 164864, cudaGetErrorString(result_main_kernel));
        return -1;
    }

    return 0;
}

extern "C" int call(bfloat16_t* __restrict__ Input, bfloat16_t* __restrict__ W_T, bfloat16_t* __restrict__ Output, cudaStream_t stream=cudaStreamDefault) {

	CUtensorMap Input_desc;
	CUtensorMapDataType Input_desc_type= (CUtensorMapDataType)9;
	cuuint32_t Input_desc_tensorRank= 2;
	void *Input_desc_globalAddress= Input;
	cuuint64_t Input_desc_globalDim[2]= {1536,8};
	cuuint64_t Input_desc_globalStride[2]= {2,3072};
	cuuint32_t Input_desc_boxDim[2]= {64,128};
	cuuint32_t Input_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave Input_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle Input_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion Input_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill Input_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult Input_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &Input_desc, Input_desc_type, Input_desc_tensorRank, Input_desc_globalAddress, Input_desc_globalDim, Input_desc_globalStride + 1, Input_desc_boxDim, Input_desc_elementStrides, Input_desc_interleave, Input_desc_swizzle, Input_desc_l2Promotion, Input_desc_oobFill);

	if (Input_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor Input_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}

	CUtensorMap Output_desc;
	CUtensorMapDataType Output_desc_type= (CUtensorMapDataType)9;
	cuuint32_t Output_desc_tensorRank= 2;
	void *Output_desc_globalAddress= Output;
	cuuint64_t Output_desc_globalDim[2]= {1152,8};
	cuuint64_t Output_desc_globalStride[2]= {2,2304};
	cuuint32_t Output_desc_boxDim[2]= {64,128};
	cuuint32_t Output_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave Output_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle Output_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion Output_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill Output_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult Output_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &Output_desc, Output_desc_type, Output_desc_tensorRank, Output_desc_globalAddress, Output_desc_globalDim, Output_desc_globalStride + 1, Output_desc_boxDim, Output_desc_elementStrides, Output_desc_interleave, Output_desc_swizzle, Output_desc_l2Promotion, Output_desc_oobFill);

	if (Output_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor Output_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}

	CUtensorMap W_T_desc;
	CUtensorMapDataType W_T_desc_type= (CUtensorMapDataType)9;
	cuuint32_t W_T_desc_tensorRank= 2;
	void *W_T_desc_globalAddress= W_T;
	cuuint64_t W_T_desc_globalDim[2]= {1536,1152};
	cuuint64_t W_T_desc_globalStride[2]= {2,3072};
	cuuint32_t W_T_desc_boxDim[2]= {64,128};
	cuuint32_t W_T_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave W_T_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle W_T_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion W_T_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill W_T_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult W_T_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &W_T_desc, W_T_desc_type, W_T_desc_tensorRank, W_T_desc_globalAddress, W_T_desc_globalDim, W_T_desc_globalStride + 1, W_T_desc_boxDim, W_T_desc_elementStrides, W_T_desc_interleave, W_T_desc_swizzle, W_T_desc_l2Promotion, W_T_desc_oobFill);

	if (W_T_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor W_T_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}
	main_kernel<<<dim3(1, 1, 1), dim3(384, 1, 1), 164864, stream>>>(Input_desc, Output_desc, W_T_desc);
	TILELANG_CHECK_LAST_ERROR("main_kernel");

	return 0;
}

2025-10-20 06:33:22,599 DEBUG:Compilation failed for config {'block_M': 128, 'block_K': 64, 'block_N': 128, 'num_stages': 4, 'threads': 128} at index 22 with error: Initialization failed: Failed to set the allowed dynamic shared memory size to 164352 with error: invalid argument
#include <tl_templates/cuda/gemm.h>
#include <tl_templates/cuda/copy.h>
#include <tl_templates/cuda/reduce.h>
#include <tl_templates/cuda/ldsm.h>
#include <tl_templates/cuda/threadblock_swizzle.h>
#include <tl_templates/cuda/debug.h>
#ifdef ENABLE_BF16
#include <tl_templates/cuda/cuda_bf16_fallbacks.cuh>
#endif

extern "C" __global__ void main_kernel(__grid_constant__ const CUtensorMap Input_desc, __grid_constant__ const CUtensorMap Output_desc, __grid_constant__ const CUtensorMap W_T_desc);
extern "C" __global__ void __launch_bounds__(256, 1) main_kernel(__grid_constant__ const CUtensorMap Input_desc, __grid_constant__ const CUtensorMap Output_desc, __grid_constant__ const CUtensorMap W_T_desc) {
  extern __shared__ __align__(1024) uchar buf_dyn_shmem[];
  float output_reg[128];
  float squared_reg[128];
  float sum_reg[8];
  __shared__ uint64_t mbarrier_mem[8];
  auto mbarrier = reinterpret_cast<Barrier*>(mbarrier_mem);
  if (tl::tl_shuffle_elect<0>()) {
    tl::prefetch_tma_descriptor(Input_desc);
    tl::prefetch_tma_descriptor(W_T_desc);
    tl::prefetch_tma_descriptor(Output_desc);
    mbarrier[0].init(128);
    mbarrier[1].init(128);
    mbarrier[2].init(128);
    mbarrier[3].init(128);
    mbarrier[4].init(128);
    mbarrier[5].init(128);
    mbarrier[6].init(128);
    mbarrier[7].init(128);
  }
  __syncthreads();
  if (128 <= ((int)threadIdx.x)) {
    for (int w = 0; w < 9; ++w) {
      for (int k = 0; k < 24; ++k) {
        mbarrier[((k & 3) + 4)].wait((((k & 7) >> 2) ^ 1));
        if (tl::tl_shuffle_elect<128>()) {
          mbarrier[(k & 3)].expect_transaction(16384);
          tl::tma_load(Input_desc, mbarrier[(k & 3)], (&(((bfloat16_t*)buf_dyn_shmem)[((k & 3) * 8192)])), (k * 64), 0);
          mbarrier[(k & 3)].expect_transaction(16384);
          tl::tma_load(W_T_desc, mbarrier[(k & 3)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k & 3) * 8192) + 32768)])), (k * 64), (w * 128));
        }
        mbarrier[(k & 3)].arrive();
      }
    }
  } else {
    for (int w_1 = 0; w_1 < 9; ++w_1) {
      #pragma unroll
      for (int i = 0; i < 64; ++i) {
        *(float2*)(output_reg + (i * 2)) = make_float2(0x0p+0f/*0.000000e+00*/, 0x0p+0f/*0.000000e+00*/);
      }
      tl::fence_proxy_async();
      for (int k_1 = 0; k_1 < 24; ++k_1) {
        mbarrier[(k_1 & 3)].wait(((k_1 & 7) >> 2));
        tl::gemm_ss<128, 128, 64, 2, 2, 0, 1, 0, 64, 64, 0, 0>((&(((bfloat16_t*)buf_dyn_shmem)[((k_1 & 3) * 8192)])), (&(((bfloat16_t*)buf_dyn_shmem)[(((k_1 & 3) * 8192) + 32768)])), (&(output_reg[0])));
        mbarrier[((k_1 & 3) + 4)].arrive();
      }
      #pragma unroll
      for (int i_1 = 0; i_1 < 128; ++i_1) {
        output_reg[i_1] = (output_reg[i_1] / (0x1p+0f/*1.000000e+00*/ + expf((output_reg[i_1] * -0x1p+0f/*-1.000000e+00*/))));
      }
      #pragma unroll
      for (int i_2 = 0; i_2 < 128; ++i_2) {
        squared_reg[i_2] = (output_reg[i_2] * output_reg[i_2]);
      }
      tl::__sync_thread_partial<3, 128>();
      #pragma unroll
      for (int i_3 = 0; i_3 < 8; ++i_3) {
        sum_reg[i_3] = 0x0p+0f/*0.000000e+00*/;
        #pragma unroll
        for (int rv = 0; rv < 16; ++rv) {
          sum_reg[i_3] = (sum_reg[i_3] + squared_reg[((((rv & 7) * 16) + (i_3 * 2)) + (rv >> 3))]);
        }
        sum_reg[i_3] = tl::AllReduce<tl::SumOp, 128, 64, 0>::run(sum_reg[i_3], (&(((float*)buf_dyn_shmem)[40960])));
        sum_reg[i_3] = tl::AllReduce<tl::SumOp, 4, 1, 0>::run(sum_reg[i_3]);
      }
      #pragma unroll
      for (int i_4 = 0; i_4 < 128; ++i_4) {
        output_reg[i_4] = (output_reg[i_4] * rsqrtf(sum_reg[((i_4 & 15) >> 1)]));
      }
      #pragma unroll
      for (int i_5 = 0; i_5 < 16; ++i_5) {
        tl::ptx_stmatrix_x4((&(((bfloat16_t*)buf_dyn_shmem)[((((((((((i_5 >> 3) * 8192) + ((i_5 & 1) * 4096)) + (((((int)threadIdx.x) & 31) >> 4) * 2048)) + (((((int)threadIdx.x) & 63) >> 5) * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + (((((i_5 & 7) >> 2) + ((((int)threadIdx.x) & 7) >> 2)) & 1) * 32)) + (((((i_5 & 3) >> 1) + ((((int)threadIdx.x) & 3) >> 1)) & 1) * 16)) + ((((((int)threadIdx.x) >> 6) + (((int)threadIdx.x) & 1)) & 1) * 8)) + 65536)])), __pack_half2(((bfloat16_t)output_reg[(i_5 * 8)]), ((bfloat16_t)output_reg[((i_5 * 8) + 1)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 2)]), ((bfloat16_t)output_reg[((i_5 * 8) + 3)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 4)]), ((bfloat16_t)output_reg[((i_5 * 8) + 5)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 6)]), ((bfloat16_t)output_reg[((i_5 * 8) + 7)])));
      }
      tl::fence_proxy_async();
      tl::__sync_thread_partial<3, 128>();
      if (tl::tl_shuffle_elect<128>()) {
        tl::tma_store(Output_desc, (&(((bfloat16_t*)buf_dyn_shmem)[65536])), (w_1 * 128), 0);
        tl::tma_store_arrive();
        tl::tma_store_wait<0>();
        tl::tma_store(Output_desc, (&(((bfloat16_t*)buf_dyn_shmem)[73728])), ((w_1 * 128) + 64), 0);
        tl::tma_store_arrive();
        tl::tma_store_wait<0>();
      }
    }
  }
}


#define ERROR_BUF_SIZE 1024
static char error_buf[ERROR_BUF_SIZE];

extern "C" const char* get_last_error() {
    return error_buf;
}

extern "C" int init() {
    error_buf[0] = '\0';
    
    cudaError_t result_main_kernel = cudaFuncSetAttribute(main_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, 164352);
    if (result_main_kernel != CUDA_SUCCESS) {
        snprintf(error_buf, ERROR_BUF_SIZE, "Failed to set the allowed dynamic shared memory size to %d with error: %s", 164352, cudaGetErrorString(result_main_kernel));
        return -1;
    }

    return 0;
}

extern "C" int call(bfloat16_t* __restrict__ Input, bfloat16_t* __restrict__ W_T, bfloat16_t* __restrict__ Output, cudaStream_t stream=cudaStreamDefault) {

	CUtensorMap Input_desc;
	CUtensorMapDataType Input_desc_type= (CUtensorMapDataType)9;
	cuuint32_t Input_desc_tensorRank= 2;
	void *Input_desc_globalAddress= Input;
	cuuint64_t Input_desc_globalDim[2]= {1536,8};
	cuuint64_t Input_desc_globalStride[2]= {2,3072};
	cuuint32_t Input_desc_boxDim[2]= {64,128};
	cuuint32_t Input_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave Input_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle Input_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion Input_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill Input_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult Input_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &Input_desc, Input_desc_type, Input_desc_tensorRank, Input_desc_globalAddress, Input_desc_globalDim, Input_desc_globalStride + 1, Input_desc_boxDim, Input_desc_elementStrides, Input_desc_interleave, Input_desc_swizzle, Input_desc_l2Promotion, Input_desc_oobFill);

	if (Input_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor Input_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}

	CUtensorMap Output_desc;
	CUtensorMapDataType Output_desc_type= (CUtensorMapDataType)9;
	cuuint32_t Output_desc_tensorRank= 2;
	void *Output_desc_globalAddress= Output;
	cuuint64_t Output_desc_globalDim[2]= {1152,8};
	cuuint64_t Output_desc_globalStride[2]= {2,2304};
	cuuint32_t Output_desc_boxDim[2]= {64,128};
	cuuint32_t Output_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave Output_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle Output_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion Output_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill Output_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult Output_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &Output_desc, Output_desc_type, Output_desc_tensorRank, Output_desc_globalAddress, Output_desc_globalDim, Output_desc_globalStride + 1, Output_desc_boxDim, Output_desc_elementStrides, Output_desc_interleave, Output_desc_swizzle, Output_desc_l2Promotion, Output_desc_oobFill);

	if (Output_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor Output_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}

	CUtensorMap W_T_desc;
	CUtensorMapDataType W_T_desc_type= (CUtensorMapDataType)9;
	cuuint32_t W_T_desc_tensorRank= 2;
	void *W_T_desc_globalAddress= W_T;
	cuuint64_t W_T_desc_globalDim[2]= {1536,1152};
	cuuint64_t W_T_desc_globalStride[2]= {2,3072};
	cuuint32_t W_T_desc_boxDim[2]= {64,128};
	cuuint32_t W_T_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave W_T_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle W_T_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion W_T_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill W_T_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult W_T_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &W_T_desc, W_T_desc_type, W_T_desc_tensorRank, W_T_desc_globalAddress, W_T_desc_globalDim, W_T_desc_globalStride + 1, W_T_desc_boxDim, W_T_desc_elementStrides, W_T_desc_interleave, W_T_desc_swizzle, W_T_desc_l2Promotion, W_T_desc_oobFill);

	if (W_T_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor W_T_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}
	main_kernel<<<dim3(1, 1, 1), dim3(256, 1, 1), 164352, stream>>>(Input_desc, Output_desc, W_T_desc);
	TILELANG_CHECK_LAST_ERROR("main_kernel");

	return 0;
}

2025-10-20 06:33:22,962 DEBUG:Compilation failed for config {'block_M': 128, 'block_K': 128, 'block_N': 128, 'num_stages': 2, 'threads': 128} at index 26 with error: Initialization failed: Failed to set the allowed dynamic shared memory size to 164352 with error: invalid argument
#include <tl_templates/cuda/gemm.h>
#include <tl_templates/cuda/copy.h>
#include <tl_templates/cuda/reduce.h>
#include <tl_templates/cuda/ldsm.h>
#include <tl_templates/cuda/threadblock_swizzle.h>
#include <tl_templates/cuda/debug.h>
#ifdef ENABLE_BF16
#include <tl_templates/cuda/cuda_bf16_fallbacks.cuh>
#endif

extern "C" __global__ void main_kernel(__grid_constant__ const CUtensorMap Input_desc, __grid_constant__ const CUtensorMap Output_desc, __grid_constant__ const CUtensorMap W_T_desc);
extern "C" __global__ void __launch_bounds__(256, 1) main_kernel(__grid_constant__ const CUtensorMap Input_desc, __grid_constant__ const CUtensorMap Output_desc, __grid_constant__ const CUtensorMap W_T_desc) {
  extern __shared__ __align__(1024) uchar buf_dyn_shmem[];
  float output_reg[128];
  float squared_reg[128];
  float sum_reg[8];
  __shared__ uint64_t mbarrier_mem[4];
  auto mbarrier = reinterpret_cast<Barrier*>(mbarrier_mem);
  if (tl::tl_shuffle_elect<0>()) {
    tl::prefetch_tma_descriptor(Input_desc);
    tl::prefetch_tma_descriptor(W_T_desc);
    tl::prefetch_tma_descriptor(Output_desc);
    mbarrier[0].init(128);
    mbarrier[1].init(128);
    mbarrier[2].init(128);
    mbarrier[3].init(128);
  }
  __syncthreads();
  if (128 <= ((int)threadIdx.x)) {
    for (int w = 0; w < 9; ++w) {
      for (int k = 0; k < 12; ++k) {
        mbarrier[((k & 1) + 2)].wait((((k & 3) >> 1) ^ 1));
        if (tl::tl_shuffle_elect<128>()) {
          mbarrier[(k & 1)].expect_transaction(32768);
          tl::tma_load(Input_desc, mbarrier[(k & 1)], (&(((bfloat16_t*)buf_dyn_shmem)[((k & 1) * 16384)])), (k * 128), 0);
          tl::tma_load(Input_desc, mbarrier[(k & 1)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k & 1) * 16384) + 8192)])), ((k * 128) + 64), 0);
          mbarrier[(k & 1)].expect_transaction(32768);
          tl::tma_load(W_T_desc, mbarrier[(k & 1)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k & 1) * 16384) + 32768)])), (k * 128), (w * 128));
          tl::tma_load(W_T_desc, mbarrier[(k & 1)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k & 1) * 16384) + 40960)])), ((k * 128) + 64), (w * 128));
        }
        mbarrier[(k & 1)].arrive();
      }
    }
  } else {
    for (int w_1 = 0; w_1 < 9; ++w_1) {
      #pragma unroll
      for (int i = 0; i < 64; ++i) {
        *(float2*)(output_reg + (i * 2)) = make_float2(0x0p+0f/*0.000000e+00*/, 0x0p+0f/*0.000000e+00*/);
      }
      tl::fence_proxy_async();
      for (int k_1 = 0; k_1 < 12; ++k_1) {
        mbarrier[(k_1 & 1)].wait(((k_1 & 3) >> 1));
        tl::gemm_ss<128, 128, 128, 2, 2, 0, 1, 0, 128, 128, 0, 0>((&(((bfloat16_t*)buf_dyn_shmem)[((k_1 & 1) * 16384)])), (&(((bfloat16_t*)buf_dyn_shmem)[(((k_1 & 1) * 16384) + 32768)])), (&(output_reg[0])));
        mbarrier[((k_1 & 1) + 2)].arrive();
      }
      #pragma unroll
      for (int i_1 = 0; i_1 < 128; ++i_1) {
        output_reg[i_1] = (output_reg[i_1] / (0x1p+0f/*1.000000e+00*/ + expf((output_reg[i_1] * -0x1p+0f/*-1.000000e+00*/))));
      }
      #pragma unroll
      for (int i_2 = 0; i_2 < 128; ++i_2) {
        squared_reg[i_2] = (output_reg[i_2] * output_reg[i_2]);
      }
      tl::__sync_thread_partial<3, 128>();
      #pragma unroll
      for (int i_3 = 0; i_3 < 8; ++i_3) {
        sum_reg[i_3] = 0x0p+0f/*0.000000e+00*/;
        #pragma unroll
        for (int rv = 0; rv < 16; ++rv) {
          sum_reg[i_3] = (sum_reg[i_3] + squared_reg[((((rv & 7) * 16) + (i_3 * 2)) + (rv >> 3))]);
        }
        sum_reg[i_3] = tl::AllReduce<tl::SumOp, 128, 64, 0>::run(sum_reg[i_3], (&(((float*)buf_dyn_shmem)[40960])));
        sum_reg[i_3] = tl::AllReduce<tl::SumOp, 4, 1, 0>::run(sum_reg[i_3]);
      }
      #pragma unroll
      for (int i_4 = 0; i_4 < 128; ++i_4) {
        output_reg[i_4] = (output_reg[i_4] * rsqrtf(sum_reg[((i_4 & 15) >> 1)]));
      }
      #pragma unroll
      for (int i_5 = 0; i_5 < 16; ++i_5) {
        tl::ptx_stmatrix_x4((&(((bfloat16_t*)buf_dyn_shmem)[((((((((((i_5 >> 3) * 8192) + ((i_5 & 1) * 4096)) + (((((int)threadIdx.x) & 31) >> 4) * 2048)) + (((((int)threadIdx.x) & 63) >> 5) * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + (((((i_5 & 7) >> 2) + ((((int)threadIdx.x) & 7) >> 2)) & 1) * 32)) + (((((i_5 & 3) >> 1) + ((((int)threadIdx.x) & 3) >> 1)) & 1) * 16)) + ((((((int)threadIdx.x) >> 6) + (((int)threadIdx.x) & 1)) & 1) * 8)) + 65536)])), __pack_half2(((bfloat16_t)output_reg[(i_5 * 8)]), ((bfloat16_t)output_reg[((i_5 * 8) + 1)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 2)]), ((bfloat16_t)output_reg[((i_5 * 8) + 3)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 4)]), ((bfloat16_t)output_reg[((i_5 * 8) + 5)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 6)]), ((bfloat16_t)output_reg[((i_5 * 8) + 7)])));
      }
      tl::fence_proxy_async();
      tl::__sync_thread_partial<3, 128>();
      if (tl::tl_shuffle_elect<128>()) {
        tl::tma_store(Output_desc, (&(((bfloat16_t*)buf_dyn_shmem)[65536])), (w_1 * 128), 0);
        tl::tma_store_arrive();
        tl::tma_store_wait<0>();
        tl::tma_store(Output_desc, (&(((bfloat16_t*)buf_dyn_shmem)[73728])), ((w_1 * 128) + 64), 0);
        tl::tma_store_arrive();
        tl::tma_store_wait<0>();
      }
    }
  }
}


#define ERROR_BUF_SIZE 1024
static char error_buf[ERROR_BUF_SIZE];

extern "C" const char* get_last_error() {
    return error_buf;
}

extern "C" int init() {
    error_buf[0] = '\0';
    
    cudaError_t result_main_kernel = cudaFuncSetAttribute(main_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, 164352);
    if (result_main_kernel != CUDA_SUCCESS) {
        snprintf(error_buf, ERROR_BUF_SIZE, "Failed to set the allowed dynamic shared memory size to %d with error: %s", 164352, cudaGetErrorString(result_main_kernel));
        return -1;
    }

    return 0;
}

extern "C" int call(bfloat16_t* __restrict__ Input, bfloat16_t* __restrict__ W_T, bfloat16_t* __restrict__ Output, cudaStream_t stream=cudaStreamDefault) {

	CUtensorMap Input_desc;
	CUtensorMapDataType Input_desc_type= (CUtensorMapDataType)9;
	cuuint32_t Input_desc_tensorRank= 2;
	void *Input_desc_globalAddress= Input;
	cuuint64_t Input_desc_globalDim[2]= {1536,8};
	cuuint64_t Input_desc_globalStride[2]= {2,3072};
	cuuint32_t Input_desc_boxDim[2]= {64,128};
	cuuint32_t Input_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave Input_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle Input_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion Input_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill Input_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult Input_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &Input_desc, Input_desc_type, Input_desc_tensorRank, Input_desc_globalAddress, Input_desc_globalDim, Input_desc_globalStride + 1, Input_desc_boxDim, Input_desc_elementStrides, Input_desc_interleave, Input_desc_swizzle, Input_desc_l2Promotion, Input_desc_oobFill);

	if (Input_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor Input_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}

	CUtensorMap Output_desc;
	CUtensorMapDataType Output_desc_type= (CUtensorMapDataType)9;
	cuuint32_t Output_desc_tensorRank= 2;
	void *Output_desc_globalAddress= Output;
	cuuint64_t Output_desc_globalDim[2]= {1152,8};
	cuuint64_t Output_desc_globalStride[2]= {2,2304};
	cuuint32_t Output_desc_boxDim[2]= {64,128};
	cuuint32_t Output_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave Output_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle Output_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion Output_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill Output_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult Output_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &Output_desc, Output_desc_type, Output_desc_tensorRank, Output_desc_globalAddress, Output_desc_globalDim, Output_desc_globalStride + 1, Output_desc_boxDim, Output_desc_elementStrides, Output_desc_interleave, Output_desc_swizzle, Output_desc_l2Promotion, Output_desc_oobFill);

	if (Output_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor Output_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}

	CUtensorMap W_T_desc;
	CUtensorMapDataType W_T_desc_type= (CUtensorMapDataType)9;
	cuuint32_t W_T_desc_tensorRank= 2;
	void *W_T_desc_globalAddress= W_T;
	cuuint64_t W_T_desc_globalDim[2]= {1536,1152};
	cuuint64_t W_T_desc_globalStride[2]= {2,3072};
	cuuint32_t W_T_desc_boxDim[2]= {64,128};
	cuuint32_t W_T_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave W_T_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle W_T_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion W_T_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill W_T_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult W_T_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &W_T_desc, W_T_desc_type, W_T_desc_tensorRank, W_T_desc_globalAddress, W_T_desc_globalDim, W_T_desc_globalStride + 1, W_T_desc_boxDim, W_T_desc_elementStrides, W_T_desc_interleave, W_T_desc_swizzle, W_T_desc_l2Promotion, W_T_desc_oobFill);

	if (W_T_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor W_T_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}
	main_kernel<<<dim3(1, 1, 1), dim3(256, 1, 1), 164352, stream>>>(Input_desc, Output_desc, W_T_desc);
	TILELANG_CHECK_LAST_ERROR("main_kernel");

	return 0;
}

2025-10-20 06:33:23,186 DEBUG:Compilation failed for config {'block_M': 128, 'block_K': 128, 'block_N': 128, 'num_stages': 2, 'threads': 256} at index 27 with error: Initialization failed: Failed to set the allowed dynamic shared memory size to 164864 with error: invalid argument
#include <tl_templates/cuda/gemm.h>
#include <tl_templates/cuda/copy.h>
#include <tl_templates/cuda/reduce.h>
#include <tl_templates/cuda/ldsm.h>
#include <tl_templates/cuda/threadblock_swizzle.h>
#include <tl_templates/cuda/debug.h>
#ifdef ENABLE_BF16
#include <tl_templates/cuda/cuda_bf16_fallbacks.cuh>
#endif

extern "C" __global__ void main_kernel(__grid_constant__ const CUtensorMap Input_desc, __grid_constant__ const CUtensorMap Output_desc, __grid_constant__ const CUtensorMap W_T_desc);
extern "C" __global__ void __launch_bounds__(384, 1) main_kernel(__grid_constant__ const CUtensorMap Input_desc, __grid_constant__ const CUtensorMap Output_desc, __grid_constant__ const CUtensorMap W_T_desc) {
  extern __shared__ __align__(1024) uchar buf_dyn_shmem[];
  float output_reg[64];
  float squared_reg[64];
  float sum_reg[8];
  __shared__ uint64_t mbarrier_mem[4];
  auto mbarrier = reinterpret_cast<Barrier*>(mbarrier_mem);
  if (tl::tl_shuffle_elect<0>()) {
    tl::prefetch_tma_descriptor(Input_desc);
    tl::prefetch_tma_descriptor(W_T_desc);
    tl::prefetch_tma_descriptor(Output_desc);
    mbarrier[0].init(128);
    mbarrier[1].init(128);
    mbarrier[2].init(256);
    mbarrier[3].init(256);
  }
  __syncthreads();
  if (256 <= ((int)threadIdx.x)) {
    for (int w = 0; w < 9; ++w) {
      for (int k = 0; k < 12; ++k) {
        mbarrier[((k & 1) + 2)].wait((((k & 3) >> 1) ^ 1));
        if (tl::tl_shuffle_elect<128>()) {
          mbarrier[(k & 1)].expect_transaction(32768);
          tl::tma_load(Input_desc, mbarrier[(k & 1)], (&(((bfloat16_t*)buf_dyn_shmem)[((k & 1) * 16384)])), (k * 128), 0);
          tl::tma_load(Input_desc, mbarrier[(k & 1)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k & 1) * 16384) + 8192)])), ((k * 128) + 64), 0);
          mbarrier[(k & 1)].expect_transaction(32768);
          tl::tma_load(W_T_desc, mbarrier[(k & 1)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k & 1) * 16384) + 32768)])), (k * 128), (w * 128));
          tl::tma_load(W_T_desc, mbarrier[(k & 1)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k & 1) * 16384) + 40960)])), ((k * 128) + 64), (w * 128));
        }
        mbarrier[(k & 1)].arrive();
      }
    }
  } else {
    for (int w_1 = 0; w_1 < 9; ++w_1) {
      #pragma unroll
      for (int i = 0; i < 32; ++i) {
        *(float2*)(output_reg + (i * 2)) = make_float2(0x0p+0f/*0.000000e+00*/, 0x0p+0f/*0.000000e+00*/);
      }
      tl::fence_proxy_async();
      for (int k_1 = 0; k_1 < 12; ++k_1) {
        mbarrier[(k_1 & 1)].wait(((k_1 & 3) >> 1));
        tl::gemm_ss<128, 128, 128, 2, 4, 0, 1, 0, 128, 128, 0, 0>((&(((bfloat16_t*)buf_dyn_shmem)[((k_1 & 1) * 16384)])), (&(((bfloat16_t*)buf_dyn_shmem)[(((k_1 & 1) * 16384) + 32768)])), (&(output_reg[0])));
        mbarrier[((k_1 & 1) + 2)].arrive();
      }
      #pragma unroll
      for (int i_1 = 0; i_1 < 64; ++i_1) {
        output_reg[i_1] = (output_reg[i_1] / (0x1p+0f/*1.000000e+00*/ + expf((output_reg[i_1] * -0x1p+0f/*-1.000000e+00*/))));
      }
      #pragma unroll
      for (int i_2 = 0; i_2 < 64; ++i_2) {
        squared_reg[i_2] = (output_reg[i_2] * output_reg[i_2]);
      }
      tl::__sync_thread_partial<3, 256>();
      #pragma unroll
      for (int i_3 = 0; i_3 < 8; ++i_3) {
        sum_reg[i_3] = 0x0p+0f/*0.000000e+00*/;
        #pragma unroll
        for (int rv = 0; rv < 8; ++rv) {
          sum_reg[i_3] = (sum_reg[i_3] + squared_reg[((((rv & 3) * 16) + (i_3 * 2)) + (rv >> 2))]);
        }
        sum_reg[i_3] = tl::AllReduce<tl::SumOp, 256, 64, 0>::run(sum_reg[i_3], (&(((float*)buf_dyn_shmem)[40960])));
        sum_reg[i_3] = tl::AllReduce<tl::SumOp, 4, 1, 0>::run(sum_reg[i_3]);
      }
      #pragma unroll
      for (int i_4 = 0; i_4 < 64; ++i_4) {
        output_reg[i_4] = (output_reg[i_4] * rsqrtf(sum_reg[((i_4 & 15) >> 1)]));
      }
      #pragma unroll
      for (int i_5 = 0; i_5 < 8; ++i_5) {
        tl::ptx_stmatrix_x4((&(((bfloat16_t*)buf_dyn_shmem)[((((((((((i_5 >> 2) * 8192) + ((i_5 & 1) * 4096)) + (((((int)threadIdx.x) & 31) >> 4) * 2048)) + (((((int)threadIdx.x) & 63) >> 5) * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + (((((((int)threadIdx.x) & 7) >> 2) + ((i_5 & 3) >> 1)) & 1) * 32)) + ((((((int)threadIdx.x) >> 7) + ((((int)threadIdx.x) & 3) >> 1)) & 1) * 16)) + (((((((int)threadIdx.x) & 127) >> 6) + (((int)threadIdx.x) & 1)) & 1) * 8)) + 65536)])), __pack_half2(((bfloat16_t)output_reg[(i_5 * 8)]), ((bfloat16_t)output_reg[((i_5 * 8) + 1)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 2)]), ((bfloat16_t)output_reg[((i_5 * 8) + 3)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 4)]), ((bfloat16_t)output_reg[((i_5 * 8) + 5)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 6)]), ((bfloat16_t)output_reg[((i_5 * 8) + 7)])));
      }
      tl::fence_proxy_async();
      tl::__sync_thread_partial<3, 256>();
      if (tl::tl_shuffle_elect<256>()) {
        tl::tma_store(Output_desc, (&(((bfloat16_t*)buf_dyn_shmem)[65536])), (w_1 * 128), 0);
        tl::tma_store_arrive();
        tl::tma_store_wait<0>();
        tl::tma_store(Output_desc, (&(((bfloat16_t*)buf_dyn_shmem)[73728])), ((w_1 * 128) + 64), 0);
        tl::tma_store_arrive();
        tl::tma_store_wait<0>();
      }
    }
  }
}


#define ERROR_BUF_SIZE 1024
static char error_buf[ERROR_BUF_SIZE];

extern "C" const char* get_last_error() {
    return error_buf;
}

extern "C" int init() {
    error_buf[0] = '\0';
    
    cudaError_t result_main_kernel = cudaFuncSetAttribute(main_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, 164864);
    if (result_main_kernel != CUDA_SUCCESS) {
        snprintf(error_buf, ERROR_BUF_SIZE, "Failed to set the allowed dynamic shared memory size to %d with error: %s", 164864, cudaGetErrorString(result_main_kernel));
        return -1;
    }

    return 0;
}

extern "C" int call(bfloat16_t* __restrict__ Input, bfloat16_t* __restrict__ W_T, bfloat16_t* __restrict__ Output, cudaStream_t stream=cudaStreamDefault) {

	CUtensorMap Input_desc;
	CUtensorMapDataType Input_desc_type= (CUtensorMapDataType)9;
	cuuint32_t Input_desc_tensorRank= 2;
	void *Input_desc_globalAddress= Input;
	cuuint64_t Input_desc_globalDim[2]= {1536,8};
	cuuint64_t Input_desc_globalStride[2]= {2,3072};
	cuuint32_t Input_desc_boxDim[2]= {64,128};
	cuuint32_t Input_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave Input_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle Input_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion Input_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill Input_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult Input_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &Input_desc, Input_desc_type, Input_desc_tensorRank, Input_desc_globalAddress, Input_desc_globalDim, Input_desc_globalStride + 1, Input_desc_boxDim, Input_desc_elementStrides, Input_desc_interleave, Input_desc_swizzle, Input_desc_l2Promotion, Input_desc_oobFill);

	if (Input_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor Input_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}

	CUtensorMap Output_desc;
	CUtensorMapDataType Output_desc_type= (CUtensorMapDataType)9;
	cuuint32_t Output_desc_tensorRank= 2;
	void *Output_desc_globalAddress= Output;
	cuuint64_t Output_desc_globalDim[2]= {1152,8};
	cuuint64_t Output_desc_globalStride[2]= {2,2304};
	cuuint32_t Output_desc_boxDim[2]= {64,128};
	cuuint32_t Output_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave Output_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle Output_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion Output_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill Output_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult Output_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &Output_desc, Output_desc_type, Output_desc_tensorRank, Output_desc_globalAddress, Output_desc_globalDim, Output_desc_globalStride + 1, Output_desc_boxDim, Output_desc_elementStrides, Output_desc_interleave, Output_desc_swizzle, Output_desc_l2Promotion, Output_desc_oobFill);

	if (Output_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor Output_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}

	CUtensorMap W_T_desc;
	CUtensorMapDataType W_T_desc_type= (CUtensorMapDataType)9;
	cuuint32_t W_T_desc_tensorRank= 2;
	void *W_T_desc_globalAddress= W_T;
	cuuint64_t W_T_desc_globalDim[2]= {1536,1152};
	cuuint64_t W_T_desc_globalStride[2]= {2,3072};
	cuuint32_t W_T_desc_boxDim[2]= {64,128};
	cuuint32_t W_T_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave W_T_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle W_T_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion W_T_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill W_T_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult W_T_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &W_T_desc, W_T_desc_type, W_T_desc_tensorRank, W_T_desc_globalAddress, W_T_desc_globalDim, W_T_desc_globalStride + 1, W_T_desc_boxDim, W_T_desc_elementStrides, W_T_desc_interleave, W_T_desc_swizzle, W_T_desc_l2Promotion, W_T_desc_oobFill);

	if (W_T_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor W_T_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}
	main_kernel<<<dim3(1, 1, 1), dim3(384, 1, 1), 164864, stream>>>(Input_desc, Output_desc, W_T_desc);
	TILELANG_CHECK_LAST_ERROR("main_kernel");

	return 0;
}

2025-10-20 06:33:24,689 DEBUG:Compilation failed for config {'block_M': 128, 'block_K': 64, 'block_N': 128, 'num_stages': 3, 'threads': 256} at index 21 with error: Initialization failed: Failed to set the allowed dynamic shared memory size to 132096 with error: invalid argument
#include <tl_templates/cuda/gemm.h>
#include <tl_templates/cuda/copy.h>
#include <tl_templates/cuda/reduce.h>
#include <tl_templates/cuda/ldsm.h>
#include <tl_templates/cuda/threadblock_swizzle.h>
#include <tl_templates/cuda/debug.h>
#ifdef ENABLE_BF16
#include <tl_templates/cuda/cuda_bf16_fallbacks.cuh>
#endif

extern "C" __global__ void main_kernel(__grid_constant__ const CUtensorMap Input_desc, __grid_constant__ const CUtensorMap Output_desc, __grid_constant__ const CUtensorMap W_T_desc);
extern "C" __global__ void __launch_bounds__(384, 1) main_kernel(__grid_constant__ const CUtensorMap Input_desc, __grid_constant__ const CUtensorMap Output_desc, __grid_constant__ const CUtensorMap W_T_desc) {
  extern __shared__ __align__(1024) uchar buf_dyn_shmem[];
  float output_reg[64];
  float squared_reg[64];
  float sum_reg[8];
  __shared__ uint64_t mbarrier_mem[6];
  auto mbarrier = reinterpret_cast<Barrier*>(mbarrier_mem);
  if (tl::tl_shuffle_elect<0>()) {
    tl::prefetch_tma_descriptor(Input_desc);
    tl::prefetch_tma_descriptor(W_T_desc);
    tl::prefetch_tma_descriptor(Output_desc);
    mbarrier[0].init(128);
    mbarrier[1].init(128);
    mbarrier[2].init(128);
    mbarrier[3].init(256);
    mbarrier[4].init(256);
    mbarrier[5].init(256);
  }
  __syncthreads();
  if (256 <= ((int)threadIdx.x)) {
    for (int w = 0; w < 9; ++w) {
      for (int k = 0; k < 24; ++k) {
        mbarrier[((k % 3) + 3)].wait((((k % 6) / 3) ^ 1));
        if (tl::tl_shuffle_elect<128>()) {
          mbarrier[(k % 3)].expect_transaction(16384);
          tl::tma_load(Input_desc, mbarrier[(k % 3)], (&(((bfloat16_t*)buf_dyn_shmem)[((k % 3) * 8192)])), (k * 64), 0);
          mbarrier[(k % 3)].expect_transaction(16384);
          tl::tma_load(W_T_desc, mbarrier[(k % 3)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k % 3) * 8192) + 24576)])), (k * 64), (w * 128));
        }
        mbarrier[(k % 3)].arrive();
      }
    }
  } else {
    for (int w_1 = 0; w_1 < 9; ++w_1) {
      #pragma unroll
      for (int i = 0; i < 32; ++i) {
        *(float2*)(output_reg + (i * 2)) = make_float2(0x0p+0f/*0.000000e+00*/, 0x0p+0f/*0.000000e+00*/);
      }
      tl::fence_proxy_async();
      for (int k_1 = 0; k_1 < 24; ++k_1) {
        mbarrier[(k_1 % 3)].wait(((k_1 % 6) / 3));
        tl::gemm_ss<128, 128, 64, 2, 4, 0, 1, 0, 64, 64, 0, 0>((&(((bfloat16_t*)buf_dyn_shmem)[((k_1 % 3) * 8192)])), (&(((bfloat16_t*)buf_dyn_shmem)[(((k_1 % 3) * 8192) + 24576)])), (&(output_reg[0])));
        mbarrier[((k_1 % 3) + 3)].arrive();
      }
      #pragma unroll
      for (int i_1 = 0; i_1 < 64; ++i_1) {
        output_reg[i_1] = (output_reg[i_1] / (0x1p+0f/*1.000000e+00*/ + expf((output_reg[i_1] * -0x1p+0f/*-1.000000e+00*/))));
      }
      #pragma unroll
      for (int i_2 = 0; i_2 < 64; ++i_2) {
        squared_reg[i_2] = (output_reg[i_2] * output_reg[i_2]);
      }
      tl::__sync_thread_partial<3, 256>();
      #pragma unroll
      for (int i_3 = 0; i_3 < 8; ++i_3) {
        sum_reg[i_3] = 0x0p+0f/*0.000000e+00*/;
        #pragma unroll
        for (int rv = 0; rv < 8; ++rv) {
          sum_reg[i_3] = (sum_reg[i_3] + squared_reg[((((rv & 3) * 16) + (i_3 * 2)) + (rv >> 2))]);
        }
        sum_reg[i_3] = tl::AllReduce<tl::SumOp, 256, 64, 0>::run(sum_reg[i_3], (&(((float*)buf_dyn_shmem)[32768])));
        sum_reg[i_3] = tl::AllReduce<tl::SumOp, 4, 1, 0>::run(sum_reg[i_3]);
      }
      #pragma unroll
      for (int i_4 = 0; i_4 < 64; ++i_4) {
        output_reg[i_4] = (output_reg[i_4] * rsqrtf(sum_reg[((i_4 & 15) >> 1)]));
      }
      #pragma unroll
      for (int i_5 = 0; i_5 < 8; ++i_5) {
        tl::ptx_stmatrix_x4((&(((bfloat16_t*)buf_dyn_shmem)[((((((((((i_5 >> 2) * 8192) + ((i_5 & 1) * 4096)) + (((((int)threadIdx.x) & 31) >> 4) * 2048)) + (((((int)threadIdx.x) & 63) >> 5) * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + (((((((int)threadIdx.x) & 7) >> 2) + ((i_5 & 3) >> 1)) & 1) * 32)) + ((((((int)threadIdx.x) >> 7) + ((((int)threadIdx.x) & 3) >> 1)) & 1) * 16)) + (((((((int)threadIdx.x) & 127) >> 6) + (((int)threadIdx.x) & 1)) & 1) * 8)) + 49152)])), __pack_half2(((bfloat16_t)output_reg[(i_5 * 8)]), ((bfloat16_t)output_reg[((i_5 * 8) + 1)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 2)]), ((bfloat16_t)output_reg[((i_5 * 8) + 3)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 4)]), ((bfloat16_t)output_reg[((i_5 * 8) + 5)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 6)]), ((bfloat16_t)output_reg[((i_5 * 8) + 7)])));
      }
      tl::fence_proxy_async();
      tl::__sync_thread_partial<3, 256>();
      if (tl::tl_shuffle_elect<256>()) {
        tl::tma_store(Output_desc, (&(((bfloat16_t*)buf_dyn_shmem)[49152])), (w_1 * 128), 0);
        tl::tma_store_arrive();
        tl::tma_store_wait<0>();
        tl::tma_store(Output_desc, (&(((bfloat16_t*)buf_dyn_shmem)[57344])), ((w_1 * 128) + 64), 0);
        tl::tma_store_arrive();
        tl::tma_store_wait<0>();
      }
    }
  }
}


#define ERROR_BUF_SIZE 1024
static char error_buf[ERROR_BUF_SIZE];

extern "C" const char* get_last_error() {
    return error_buf;
}

extern "C" int init() {
    error_buf[0] = '\0';
    
    cudaError_t result_main_kernel = cudaFuncSetAttribute(main_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, 132096);
    if (result_main_kernel != CUDA_SUCCESS) {
        snprintf(error_buf, ERROR_BUF_SIZE, "Failed to set the allowed dynamic shared memory size to %d with error: %s", 132096, cudaGetErrorString(result_main_kernel));
        return -1;
    }

    return 0;
}

extern "C" int call(bfloat16_t* __restrict__ Input, bfloat16_t* __restrict__ W_T, bfloat16_t* __restrict__ Output, cudaStream_t stream=cudaStreamDefault) {

	CUtensorMap Input_desc;
	CUtensorMapDataType Input_desc_type= (CUtensorMapDataType)9;
	cuuint32_t Input_desc_tensorRank= 2;
	void *Input_desc_globalAddress= Input;
	cuuint64_t Input_desc_globalDim[2]= {1536,8};
	cuuint64_t Input_desc_globalStride[2]= {2,3072};
	cuuint32_t Input_desc_boxDim[2]= {64,128};
	cuuint32_t Input_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave Input_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle Input_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion Input_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill Input_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult Input_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &Input_desc, Input_desc_type, Input_desc_tensorRank, Input_desc_globalAddress, Input_desc_globalDim, Input_desc_globalStride + 1, Input_desc_boxDim, Input_desc_elementStrides, Input_desc_interleave, Input_desc_swizzle, Input_desc_l2Promotion, Input_desc_oobFill);

	if (Input_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor Input_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}

	CUtensorMap Output_desc;
	CUtensorMapDataType Output_desc_type= (CUtensorMapDataType)9;
	cuuint32_t Output_desc_tensorRank= 2;
	void *Output_desc_globalAddress= Output;
	cuuint64_t Output_desc_globalDim[2]= {1152,8};
	cuuint64_t Output_desc_globalStride[2]= {2,2304};
	cuuint32_t Output_desc_boxDim[2]= {64,128};
	cuuint32_t Output_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave Output_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle Output_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion Output_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill Output_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult Output_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &Output_desc, Output_desc_type, Output_desc_tensorRank, Output_desc_globalAddress, Output_desc_globalDim, Output_desc_globalStride + 1, Output_desc_boxDim, Output_desc_elementStrides, Output_desc_interleave, Output_desc_swizzle, Output_desc_l2Promotion, Output_desc_oobFill);

	if (Output_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor Output_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}

	CUtensorMap W_T_desc;
	CUtensorMapDataType W_T_desc_type= (CUtensorMapDataType)9;
	cuuint32_t W_T_desc_tensorRank= 2;
	void *W_T_desc_globalAddress= W_T;
	cuuint64_t W_T_desc_globalDim[2]= {1536,1152};
	cuuint64_t W_T_desc_globalStride[2]= {2,3072};
	cuuint32_t W_T_desc_boxDim[2]= {64,128};
	cuuint32_t W_T_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave W_T_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle W_T_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion W_T_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill W_T_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult W_T_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &W_T_desc, W_T_desc_type, W_T_desc_tensorRank, W_T_desc_globalAddress, W_T_desc_globalDim, W_T_desc_globalStride + 1, W_T_desc_boxDim, W_T_desc_elementStrides, W_T_desc_interleave, W_T_desc_swizzle, W_T_desc_l2Promotion, W_T_desc_oobFill);

	if (W_T_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor W_T_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}
	main_kernel<<<dim3(1, 1, 1), dim3(384, 1, 1), 132096, stream>>>(Input_desc, Output_desc, W_T_desc);
	TILELANG_CHECK_LAST_ERROR("main_kernel");

	return 0;
}

2025-10-20 06:33:25,587 DEBUG:Compilation failed for config {'block_M': 128, 'block_K': 128, 'block_N': 128, 'num_stages': 3, 'threads': 256} at index 29 with error: Initialization failed: Failed to set the allowed dynamic shared memory size to 230400 with error: invalid argument
#include <tl_templates/cuda/gemm.h>
#include <tl_templates/cuda/copy.h>
#include <tl_templates/cuda/reduce.h>
#include <tl_templates/cuda/ldsm.h>
#include <tl_templates/cuda/threadblock_swizzle.h>
#include <tl_templates/cuda/debug.h>
#ifdef ENABLE_BF16
#include <tl_templates/cuda/cuda_bf16_fallbacks.cuh>
#endif

extern "C" __global__ void main_kernel(__grid_constant__ const CUtensorMap Input_desc, __grid_constant__ const CUtensorMap Output_desc, __grid_constant__ const CUtensorMap W_T_desc);
extern "C" __global__ void __launch_bounds__(384, 1) main_kernel(__grid_constant__ const CUtensorMap Input_desc, __grid_constant__ const CUtensorMap Output_desc, __grid_constant__ const CUtensorMap W_T_desc) {
  extern __shared__ __align__(1024) uchar buf_dyn_shmem[];
  float output_reg[64];
  float squared_reg[64];
  float sum_reg[8];
  __shared__ uint64_t mbarrier_mem[6];
  auto mbarrier = reinterpret_cast<Barrier*>(mbarrier_mem);
  if (tl::tl_shuffle_elect<0>()) {
    tl::prefetch_tma_descriptor(Input_desc);
    tl::prefetch_tma_descriptor(W_T_desc);
    tl::prefetch_tma_descriptor(Output_desc);
    mbarrier[0].init(128);
    mbarrier[1].init(128);
    mbarrier[2].init(128);
    mbarrier[3].init(256);
    mbarrier[4].init(256);
    mbarrier[5].init(256);
  }
  __syncthreads();
  if (256 <= ((int)threadIdx.x)) {
    for (int w = 0; w < 9; ++w) {
      for (int k = 0; k < 12; ++k) {
        mbarrier[((k % 3) + 3)].wait((((k % 6) / 3) ^ 1));
        if (tl::tl_shuffle_elect<128>()) {
          mbarrier[(k % 3)].expect_transaction(32768);
          tl::tma_load(Input_desc, mbarrier[(k % 3)], (&(((bfloat16_t*)buf_dyn_shmem)[((k % 3) * 16384)])), (k * 128), 0);
          tl::tma_load(Input_desc, mbarrier[(k % 3)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k % 3) * 16384) + 8192)])), ((k * 128) + 64), 0);
          mbarrier[(k % 3)].expect_transaction(32768);
          tl::tma_load(W_T_desc, mbarrier[(k % 3)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k % 3) * 16384) + 49152)])), (k * 128), (w * 128));
          tl::tma_load(W_T_desc, mbarrier[(k % 3)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k % 3) * 16384) + 57344)])), ((k * 128) + 64), (w * 128));
        }
        mbarrier[(k % 3)].arrive();
      }
    }
  } else {
    for (int w_1 = 0; w_1 < 9; ++w_1) {
      #pragma unroll
      for (int i = 0; i < 32; ++i) {
        *(float2*)(output_reg + (i * 2)) = make_float2(0x0p+0f/*0.000000e+00*/, 0x0p+0f/*0.000000e+00*/);
      }
      tl::fence_proxy_async();
      for (int k_1 = 0; k_1 < 12; ++k_1) {
        mbarrier[(k_1 % 3)].wait(((k_1 % 6) / 3));
        tl::gemm_ss<128, 128, 128, 2, 4, 0, 1, 0, 128, 128, 0, 0>((&(((bfloat16_t*)buf_dyn_shmem)[((k_1 % 3) * 16384)])), (&(((bfloat16_t*)buf_dyn_shmem)[(((k_1 % 3) * 16384) + 49152)])), (&(output_reg[0])));
        mbarrier[((k_1 % 3) + 3)].arrive();
      }
      #pragma unroll
      for (int i_1 = 0; i_1 < 64; ++i_1) {
        output_reg[i_1] = (output_reg[i_1] / (0x1p+0f/*1.000000e+00*/ + expf((output_reg[i_1] * -0x1p+0f/*-1.000000e+00*/))));
      }
      #pragma unroll
      for (int i_2 = 0; i_2 < 64; ++i_2) {
        squared_reg[i_2] = (output_reg[i_2] * output_reg[i_2]);
      }
      tl::__sync_thread_partial<3, 256>();
      #pragma unroll
      for (int i_3 = 0; i_3 < 8; ++i_3) {
        sum_reg[i_3] = 0x0p+0f/*0.000000e+00*/;
        #pragma unroll
        for (int rv = 0; rv < 8; ++rv) {
          sum_reg[i_3] = (sum_reg[i_3] + squared_reg[((((rv & 3) * 16) + (i_3 * 2)) + (rv >> 2))]);
        }
        sum_reg[i_3] = tl::AllReduce<tl::SumOp, 256, 64, 0>::run(sum_reg[i_3], (&(((float*)buf_dyn_shmem)[57344])));
        sum_reg[i_3] = tl::AllReduce<tl::SumOp, 4, 1, 0>::run(sum_reg[i_3]);
      }
      #pragma unroll
      for (int i_4 = 0; i_4 < 64; ++i_4) {
        output_reg[i_4] = (output_reg[i_4] * rsqrtf(sum_reg[((i_4 & 15) >> 1)]));
      }
      #pragma unroll
      for (int i_5 = 0; i_5 < 8; ++i_5) {
        tl::ptx_stmatrix_x4((&(((bfloat16_t*)buf_dyn_shmem)[((((((((((i_5 >> 2) * 8192) + ((i_5 & 1) * 4096)) + (((((int)threadIdx.x) & 31) >> 4) * 2048)) + (((((int)threadIdx.x) & 63) >> 5) * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + (((((((int)threadIdx.x) & 7) >> 2) + ((i_5 & 3) >> 1)) & 1) * 32)) + ((((((int)threadIdx.x) >> 7) + ((((int)threadIdx.x) & 3) >> 1)) & 1) * 16)) + (((((((int)threadIdx.x) & 127) >> 6) + (((int)threadIdx.x) & 1)) & 1) * 8)) + 98304)])), __pack_half2(((bfloat16_t)output_reg[(i_5 * 8)]), ((bfloat16_t)output_reg[((i_5 * 8) + 1)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 2)]), ((bfloat16_t)output_reg[((i_5 * 8) + 3)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 4)]), ((bfloat16_t)output_reg[((i_5 * 8) + 5)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 6)]), ((bfloat16_t)output_reg[((i_5 * 8) + 7)])));
      }
      tl::fence_proxy_async();
      tl::__sync_thread_partial<3, 256>();
      if (tl::tl_shuffle_elect<256>()) {
        tl::tma_store(Output_desc, (&(((bfloat16_t*)buf_dyn_shmem)[98304])), (w_1 * 128), 0);
        tl::tma_store_arrive();
        tl::tma_store_wait<0>();
        tl::tma_store(Output_desc, (&(((bfloat16_t*)buf_dyn_shmem)[106496])), ((w_1 * 128) + 64), 0);
        tl::tma_store_arrive();
        tl::tma_store_wait<0>();
      }
    }
  }
}


#define ERROR_BUF_SIZE 1024
static char error_buf[ERROR_BUF_SIZE];

extern "C" const char* get_last_error() {
    return error_buf;
}

extern "C" int init() {
    error_buf[0] = '\0';
    
    cudaError_t result_main_kernel = cudaFuncSetAttribute(main_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, 230400);
    if (result_main_kernel != CUDA_SUCCESS) {
        snprintf(error_buf, ERROR_BUF_SIZE, "Failed to set the allowed dynamic shared memory size to %d with error: %s", 230400, cudaGetErrorString(result_main_kernel));
        return -1;
    }

    return 0;
}

extern "C" int call(bfloat16_t* __restrict__ Input, bfloat16_t* __restrict__ W_T, bfloat16_t* __restrict__ Output, cudaStream_t stream=cudaStreamDefault) {

	CUtensorMap Input_desc;
	CUtensorMapDataType Input_desc_type= (CUtensorMapDataType)9;
	cuuint32_t Input_desc_tensorRank= 2;
	void *Input_desc_globalAddress= Input;
	cuuint64_t Input_desc_globalDim[2]= {1536,8};
	cuuint64_t Input_desc_globalStride[2]= {2,3072};
	cuuint32_t Input_desc_boxDim[2]= {64,128};
	cuuint32_t Input_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave Input_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle Input_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion Input_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill Input_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult Input_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &Input_desc, Input_desc_type, Input_desc_tensorRank, Input_desc_globalAddress, Input_desc_globalDim, Input_desc_globalStride + 1, Input_desc_boxDim, Input_desc_elementStrides, Input_desc_interleave, Input_desc_swizzle, Input_desc_l2Promotion, Input_desc_oobFill);

	if (Input_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor Input_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}

	CUtensorMap Output_desc;
	CUtensorMapDataType Output_desc_type= (CUtensorMapDataType)9;
	cuuint32_t Output_desc_tensorRank= 2;
	void *Output_desc_globalAddress= Output;
	cuuint64_t Output_desc_globalDim[2]= {1152,8};
	cuuint64_t Output_desc_globalStride[2]= {2,2304};
	cuuint32_t Output_desc_boxDim[2]= {64,128};
	cuuint32_t Output_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave Output_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle Output_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion Output_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill Output_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult Output_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &Output_desc, Output_desc_type, Output_desc_tensorRank, Output_desc_globalAddress, Output_desc_globalDim, Output_desc_globalStride + 1, Output_desc_boxDim, Output_desc_elementStrides, Output_desc_interleave, Output_desc_swizzle, Output_desc_l2Promotion, Output_desc_oobFill);

	if (Output_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor Output_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}

	CUtensorMap W_T_desc;
	CUtensorMapDataType W_T_desc_type= (CUtensorMapDataType)9;
	cuuint32_t W_T_desc_tensorRank= 2;
	void *W_T_desc_globalAddress= W_T;
	cuuint64_t W_T_desc_globalDim[2]= {1536,1152};
	cuuint64_t W_T_desc_globalStride[2]= {2,3072};
	cuuint32_t W_T_desc_boxDim[2]= {64,128};
	cuuint32_t W_T_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave W_T_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle W_T_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion W_T_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill W_T_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult W_T_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &W_T_desc, W_T_desc_type, W_T_desc_tensorRank, W_T_desc_globalAddress, W_T_desc_globalDim, W_T_desc_globalStride + 1, W_T_desc_boxDim, W_T_desc_elementStrides, W_T_desc_interleave, W_T_desc_swizzle, W_T_desc_l2Promotion, W_T_desc_oobFill);

	if (W_T_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor W_T_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}
	main_kernel<<<dim3(1, 1, 1), dim3(384, 1, 1), 230400, stream>>>(Input_desc, Output_desc, W_T_desc);
	TILELANG_CHECK_LAST_ERROR("main_kernel");

	return 0;
}

2025-10-20 06:33:25,761 DEBUG:Compilation failed for config {'block_M': 128, 'block_K': 128, 'block_N': 128, 'num_stages': 3, 'threads': 128} at index 28 with error: Initialization failed: Failed to set the allowed dynamic shared memory size to 229888 with error: invalid argument
#include <tl_templates/cuda/gemm.h>
#include <tl_templates/cuda/copy.h>
#include <tl_templates/cuda/reduce.h>
#include <tl_templates/cuda/ldsm.h>
#include <tl_templates/cuda/threadblock_swizzle.h>
#include <tl_templates/cuda/debug.h>
#ifdef ENABLE_BF16
#include <tl_templates/cuda/cuda_bf16_fallbacks.cuh>
#endif

extern "C" __global__ void main_kernel(__grid_constant__ const CUtensorMap Input_desc, __grid_constant__ const CUtensorMap Output_desc, __grid_constant__ const CUtensorMap W_T_desc);
extern "C" __global__ void __launch_bounds__(256, 1) main_kernel(__grid_constant__ const CUtensorMap Input_desc, __grid_constant__ const CUtensorMap Output_desc, __grid_constant__ const CUtensorMap W_T_desc) {
  extern __shared__ __align__(1024) uchar buf_dyn_shmem[];
  float output_reg[128];
  float squared_reg[128];
  float sum_reg[8];
  __shared__ uint64_t mbarrier_mem[6];
  auto mbarrier = reinterpret_cast<Barrier*>(mbarrier_mem);
  if (tl::tl_shuffle_elect<0>()) {
    tl::prefetch_tma_descriptor(Input_desc);
    tl::prefetch_tma_descriptor(W_T_desc);
    tl::prefetch_tma_descriptor(Output_desc);
    mbarrier[0].init(128);
    mbarrier[1].init(128);
    mbarrier[2].init(128);
    mbarrier[3].init(128);
    mbarrier[4].init(128);
    mbarrier[5].init(128);
  }
  __syncthreads();
  if (128 <= ((int)threadIdx.x)) {
    for (int w = 0; w < 9; ++w) {
      for (int k = 0; k < 12; ++k) {
        mbarrier[((k % 3) + 3)].wait((((k % 6) / 3) ^ 1));
        if (tl::tl_shuffle_elect<128>()) {
          mbarrier[(k % 3)].expect_transaction(32768);
          tl::tma_load(Input_desc, mbarrier[(k % 3)], (&(((bfloat16_t*)buf_dyn_shmem)[((k % 3) * 16384)])), (k * 128), 0);
          tl::tma_load(Input_desc, mbarrier[(k % 3)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k % 3) * 16384) + 8192)])), ((k * 128) + 64), 0);
          mbarrier[(k % 3)].expect_transaction(32768);
          tl::tma_load(W_T_desc, mbarrier[(k % 3)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k % 3) * 16384) + 49152)])), (k * 128), (w * 128));
          tl::tma_load(W_T_desc, mbarrier[(k % 3)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k % 3) * 16384) + 57344)])), ((k * 128) + 64), (w * 128));
        }
        mbarrier[(k % 3)].arrive();
      }
    }
  } else {
    for (int w_1 = 0; w_1 < 9; ++w_1) {
      #pragma unroll
      for (int i = 0; i < 64; ++i) {
        *(float2*)(output_reg + (i * 2)) = make_float2(0x0p+0f/*0.000000e+00*/, 0x0p+0f/*0.000000e+00*/);
      }
      tl::fence_proxy_async();
      for (int k_1 = 0; k_1 < 12; ++k_1) {
        mbarrier[(k_1 % 3)].wait(((k_1 % 6) / 3));
        tl::gemm_ss<128, 128, 128, 2, 2, 0, 1, 0, 128, 128, 0, 0>((&(((bfloat16_t*)buf_dyn_shmem)[((k_1 % 3) * 16384)])), (&(((bfloat16_t*)buf_dyn_shmem)[(((k_1 % 3) * 16384) + 49152)])), (&(output_reg[0])));
        mbarrier[((k_1 % 3) + 3)].arrive();
      }
      #pragma unroll
      for (int i_1 = 0; i_1 < 128; ++i_1) {
        output_reg[i_1] = (output_reg[i_1] / (0x1p+0f/*1.000000e+00*/ + expf((output_reg[i_1] * -0x1p+0f/*-1.000000e+00*/))));
      }
      #pragma unroll
      for (int i_2 = 0; i_2 < 128; ++i_2) {
        squared_reg[i_2] = (output_reg[i_2] * output_reg[i_2]);
      }
      tl::__sync_thread_partial<3, 128>();
      #pragma unroll
      for (int i_3 = 0; i_3 < 8; ++i_3) {
        sum_reg[i_3] = 0x0p+0f/*0.000000e+00*/;
        #pragma unroll
        for (int rv = 0; rv < 16; ++rv) {
          sum_reg[i_3] = (sum_reg[i_3] + squared_reg[((((rv & 7) * 16) + (i_3 * 2)) + (rv >> 3))]);
        }
        sum_reg[i_3] = tl::AllReduce<tl::SumOp, 128, 64, 0>::run(sum_reg[i_3], (&(((float*)buf_dyn_shmem)[57344])));
        sum_reg[i_3] = tl::AllReduce<tl::SumOp, 4, 1, 0>::run(sum_reg[i_3]);
      }
      #pragma unroll
      for (int i_4 = 0; i_4 < 128; ++i_4) {
        output_reg[i_4] = (output_reg[i_4] * rsqrtf(sum_reg[((i_4 & 15) >> 1)]));
      }
      #pragma unroll
      for (int i_5 = 0; i_5 < 16; ++i_5) {
        tl::ptx_stmatrix_x4((&(((bfloat16_t*)buf_dyn_shmem)[((((((((((i_5 >> 3) * 8192) + ((i_5 & 1) * 4096)) + (((((int)threadIdx.x) & 31) >> 4) * 2048)) + (((((int)threadIdx.x) & 63) >> 5) * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + (((((i_5 & 7) >> 2) + ((((int)threadIdx.x) & 7) >> 2)) & 1) * 32)) + (((((i_5 & 3) >> 1) + ((((int)threadIdx.x) & 3) >> 1)) & 1) * 16)) + ((((((int)threadIdx.x) >> 6) + (((int)threadIdx.x) & 1)) & 1) * 8)) + 98304)])), __pack_half2(((bfloat16_t)output_reg[(i_5 * 8)]), ((bfloat16_t)output_reg[((i_5 * 8) + 1)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 2)]), ((bfloat16_t)output_reg[((i_5 * 8) + 3)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 4)]), ((bfloat16_t)output_reg[((i_5 * 8) + 5)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 6)]), ((bfloat16_t)output_reg[((i_5 * 8) + 7)])));
      }
      tl::fence_proxy_async();
      tl::__sync_thread_partial<3, 128>();
      if (tl::tl_shuffle_elect<128>()) {
        tl::tma_store(Output_desc, (&(((bfloat16_t*)buf_dyn_shmem)[98304])), (w_1 * 128), 0);
        tl::tma_store_arrive();
        tl::tma_store_wait<0>();
        tl::tma_store(Output_desc, (&(((bfloat16_t*)buf_dyn_shmem)[106496])), ((w_1 * 128) + 64), 0);
        tl::tma_store_arrive();
        tl::tma_store_wait<0>();
      }
    }
  }
}


#define ERROR_BUF_SIZE 1024
static char error_buf[ERROR_BUF_SIZE];

extern "C" const char* get_last_error() {
    return error_buf;
}

extern "C" int init() {
    error_buf[0] = '\0';
    
    cudaError_t result_main_kernel = cudaFuncSetAttribute(main_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, 229888);
    if (result_main_kernel != CUDA_SUCCESS) {
        snprintf(error_buf, ERROR_BUF_SIZE, "Failed to set the allowed dynamic shared memory size to %d with error: %s", 229888, cudaGetErrorString(result_main_kernel));
        return -1;
    }

    return 0;
}

extern "C" int call(bfloat16_t* __restrict__ Input, bfloat16_t* __restrict__ W_T, bfloat16_t* __restrict__ Output, cudaStream_t stream=cudaStreamDefault) {

	CUtensorMap Input_desc;
	CUtensorMapDataType Input_desc_type= (CUtensorMapDataType)9;
	cuuint32_t Input_desc_tensorRank= 2;
	void *Input_desc_globalAddress= Input;
	cuuint64_t Input_desc_globalDim[2]= {1536,8};
	cuuint64_t Input_desc_globalStride[2]= {2,3072};
	cuuint32_t Input_desc_boxDim[2]= {64,128};
	cuuint32_t Input_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave Input_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle Input_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion Input_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill Input_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult Input_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &Input_desc, Input_desc_type, Input_desc_tensorRank, Input_desc_globalAddress, Input_desc_globalDim, Input_desc_globalStride + 1, Input_desc_boxDim, Input_desc_elementStrides, Input_desc_interleave, Input_desc_swizzle, Input_desc_l2Promotion, Input_desc_oobFill);

	if (Input_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor Input_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}

	CUtensorMap Output_desc;
	CUtensorMapDataType Output_desc_type= (CUtensorMapDataType)9;
	cuuint32_t Output_desc_tensorRank= 2;
	void *Output_desc_globalAddress= Output;
	cuuint64_t Output_desc_globalDim[2]= {1152,8};
	cuuint64_t Output_desc_globalStride[2]= {2,2304};
	cuuint32_t Output_desc_boxDim[2]= {64,128};
	cuuint32_t Output_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave Output_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle Output_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion Output_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill Output_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult Output_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &Output_desc, Output_desc_type, Output_desc_tensorRank, Output_desc_globalAddress, Output_desc_globalDim, Output_desc_globalStride + 1, Output_desc_boxDim, Output_desc_elementStrides, Output_desc_interleave, Output_desc_swizzle, Output_desc_l2Promotion, Output_desc_oobFill);

	if (Output_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor Output_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}

	CUtensorMap W_T_desc;
	CUtensorMapDataType W_T_desc_type= (CUtensorMapDataType)9;
	cuuint32_t W_T_desc_tensorRank= 2;
	void *W_T_desc_globalAddress= W_T;
	cuuint64_t W_T_desc_globalDim[2]= {1536,1152};
	cuuint64_t W_T_desc_globalStride[2]= {2,3072};
	cuuint32_t W_T_desc_boxDim[2]= {64,128};
	cuuint32_t W_T_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave W_T_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle W_T_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion W_T_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill W_T_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult W_T_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &W_T_desc, W_T_desc_type, W_T_desc_tensorRank, W_T_desc_globalAddress, W_T_desc_globalDim, W_T_desc_globalStride + 1, W_T_desc_boxDim, W_T_desc_elementStrides, W_T_desc_interleave, W_T_desc_swizzle, W_T_desc_l2Promotion, W_T_desc_oobFill);

	if (W_T_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor W_T_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}
	main_kernel<<<dim3(1, 1, 1), dim3(256, 1, 1), 229888, stream>>>(Input_desc, Output_desc, W_T_desc);
	TILELANG_CHECK_LAST_ERROR("main_kernel");

	return 0;
}

2025-10-20 06:33:26,649 DEBUG:Compilation failed for config {'block_M': 128, 'block_K': 128, 'block_N': 128, 'num_stages': 4, 'threads': 128} at index 30 with error: Initialization failed: Failed to set the allowed dynamic shared memory size to 295424 with error: invalid argument
#include <tl_templates/cuda/gemm.h>
#include <tl_templates/cuda/copy.h>
#include <tl_templates/cuda/reduce.h>
#include <tl_templates/cuda/ldsm.h>
#include <tl_templates/cuda/threadblock_swizzle.h>
#include <tl_templates/cuda/debug.h>
#ifdef ENABLE_BF16
#include <tl_templates/cuda/cuda_bf16_fallbacks.cuh>
#endif

extern "C" __global__ void main_kernel(__grid_constant__ const CUtensorMap Input_desc, __grid_constant__ const CUtensorMap Output_desc, __grid_constant__ const CUtensorMap W_T_desc);
extern "C" __global__ void __launch_bounds__(256, 1) main_kernel(__grid_constant__ const CUtensorMap Input_desc, __grid_constant__ const CUtensorMap Output_desc, __grid_constant__ const CUtensorMap W_T_desc) {
  extern __shared__ __align__(1024) uchar buf_dyn_shmem[];
  float output_reg[128];
  float squared_reg[128];
  float sum_reg[8];
  __shared__ uint64_t mbarrier_mem[8];
  auto mbarrier = reinterpret_cast<Barrier*>(mbarrier_mem);
  if (tl::tl_shuffle_elect<0>()) {
    tl::prefetch_tma_descriptor(Input_desc);
    tl::prefetch_tma_descriptor(W_T_desc);
    tl::prefetch_tma_descriptor(Output_desc);
    mbarrier[0].init(128);
    mbarrier[1].init(128);
    mbarrier[2].init(128);
    mbarrier[3].init(128);
    mbarrier[4].init(128);
    mbarrier[5].init(128);
    mbarrier[6].init(128);
    mbarrier[7].init(128);
  }
  __syncthreads();
  if (128 <= ((int)threadIdx.x)) {
    for (int w = 0; w < 9; ++w) {
      for (int k = 0; k < 12; ++k) {
        mbarrier[((k & 3) + 4)].wait(((((w * 3) + (k >> 2)) & 1) ^ 1));
        if (tl::tl_shuffle_elect<128>()) {
          mbarrier[(k & 3)].expect_transaction(32768);
          tl::tma_load(Input_desc, mbarrier[(k & 3)], (&(((bfloat16_t*)buf_dyn_shmem)[((k & 3) * 16384)])), (k * 128), 0);
          tl::tma_load(Input_desc, mbarrier[(k & 3)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k & 3) * 16384) + 8192)])), ((k * 128) + 64), 0);
          mbarrier[(k & 3)].expect_transaction(32768);
          tl::tma_load(W_T_desc, mbarrier[(k & 3)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k & 3) * 16384) + 65536)])), (k * 128), (w * 128));
          tl::tma_load(W_T_desc, mbarrier[(k & 3)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k & 3) * 16384) + 73728)])), ((k * 128) + 64), (w * 128));
        }
        mbarrier[(k & 3)].arrive();
      }
    }
  } else {
    for (int w_1 = 0; w_1 < 9; ++w_1) {
      #pragma unroll
      for (int i = 0; i < 64; ++i) {
        *(float2*)(output_reg + (i * 2)) = make_float2(0x0p+0f/*0.000000e+00*/, 0x0p+0f/*0.000000e+00*/);
      }
      tl::fence_proxy_async();
      for (int k_1 = 0; k_1 < 12; ++k_1) {
        mbarrier[(k_1 & 3)].wait((((w_1 * 3) + (k_1 >> 2)) & 1));
        tl::gemm_ss<128, 128, 128, 2, 2, 0, 1, 0, 128, 128, 0, 0>((&(((bfloat16_t*)buf_dyn_shmem)[((k_1 & 3) * 16384)])), (&(((bfloat16_t*)buf_dyn_shmem)[(((k_1 & 3) * 16384) + 65536)])), (&(output_reg[0])));
        mbarrier[((k_1 & 3) + 4)].arrive();
      }
      #pragma unroll
      for (int i_1 = 0; i_1 < 128; ++i_1) {
        output_reg[i_1] = (output_reg[i_1] / (0x1p+0f/*1.000000e+00*/ + expf((output_reg[i_1] * -0x1p+0f/*-1.000000e+00*/))));
      }
      #pragma unroll
      for (int i_2 = 0; i_2 < 128; ++i_2) {
        squared_reg[i_2] = (output_reg[i_2] * output_reg[i_2]);
      }
      tl::__sync_thread_partial<3, 128>();
      #pragma unroll
      for (int i_3 = 0; i_3 < 8; ++i_3) {
        sum_reg[i_3] = 0x0p+0f/*0.000000e+00*/;
        #pragma unroll
        for (int rv = 0; rv < 16; ++rv) {
          sum_reg[i_3] = (sum_reg[i_3] + squared_reg[((((rv & 7) * 16) + (i_3 * 2)) + (rv >> 3))]);
        }
        sum_reg[i_3] = tl::AllReduce<tl::SumOp, 128, 64, 0>::run(sum_reg[i_3], (&(((float*)buf_dyn_shmem)[73728])));
        sum_reg[i_3] = tl::AllReduce<tl::SumOp, 4, 1, 0>::run(sum_reg[i_3]);
      }
      #pragma unroll
      for (int i_4 = 0; i_4 < 128; ++i_4) {
        output_reg[i_4] = (output_reg[i_4] * rsqrtf(sum_reg[((i_4 & 15) >> 1)]));
      }
      #pragma unroll
      for (int i_5 = 0; i_5 < 16; ++i_5) {
        tl::ptx_stmatrix_x4((&(((bfloat16_t*)buf_dyn_shmem)[((((((((((i_5 >> 3) * 8192) + ((i_5 & 1) * 4096)) + (((((int)threadIdx.x) & 31) >> 4) * 2048)) + (((((int)threadIdx.x) & 63) >> 5) * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + (((((i_5 & 7) >> 2) + ((((int)threadIdx.x) & 7) >> 2)) & 1) * 32)) + (((((i_5 & 3) >> 1) + ((((int)threadIdx.x) & 3) >> 1)) & 1) * 16)) + ((((((int)threadIdx.x) >> 6) + (((int)threadIdx.x) & 1)) & 1) * 8)) + 131072)])), __pack_half2(((bfloat16_t)output_reg[(i_5 * 8)]), ((bfloat16_t)output_reg[((i_5 * 8) + 1)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 2)]), ((bfloat16_t)output_reg[((i_5 * 8) + 3)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 4)]), ((bfloat16_t)output_reg[((i_5 * 8) + 5)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 6)]), ((bfloat16_t)output_reg[((i_5 * 8) + 7)])));
      }
      tl::fence_proxy_async();
      tl::__sync_thread_partial<3, 128>();
      if (tl::tl_shuffle_elect<128>()) {
        tl::tma_store(Output_desc, (&(((bfloat16_t*)buf_dyn_shmem)[131072])), (w_1 * 128), 0);
        tl::tma_store_arrive();
        tl::tma_store_wait<0>();
        tl::tma_store(Output_desc, (&(((bfloat16_t*)buf_dyn_shmem)[139264])), ((w_1 * 128) + 64), 0);
        tl::tma_store_arrive();
        tl::tma_store_wait<0>();
      }
    }
  }
}


#define ERROR_BUF_SIZE 1024
static char error_buf[ERROR_BUF_SIZE];

extern "C" const char* get_last_error() {
    return error_buf;
}

extern "C" int init() {
    error_buf[0] = '\0';
    
    cudaError_t result_main_kernel = cudaFuncSetAttribute(main_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, 295424);
    if (result_main_kernel != CUDA_SUCCESS) {
        snprintf(error_buf, ERROR_BUF_SIZE, "Failed to set the allowed dynamic shared memory size to %d with error: %s", 295424, cudaGetErrorString(result_main_kernel));
        return -1;
    }

    return 0;
}

extern "C" int call(bfloat16_t* __restrict__ Input, bfloat16_t* __restrict__ W_T, bfloat16_t* __restrict__ Output, cudaStream_t stream=cudaStreamDefault) {

	CUtensorMap Input_desc;
	CUtensorMapDataType Input_desc_type= (CUtensorMapDataType)9;
	cuuint32_t Input_desc_tensorRank= 2;
	void *Input_desc_globalAddress= Input;
	cuuint64_t Input_desc_globalDim[2]= {1536,8};
	cuuint64_t Input_desc_globalStride[2]= {2,3072};
	cuuint32_t Input_desc_boxDim[2]= {64,128};
	cuuint32_t Input_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave Input_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle Input_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion Input_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill Input_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult Input_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &Input_desc, Input_desc_type, Input_desc_tensorRank, Input_desc_globalAddress, Input_desc_globalDim, Input_desc_globalStride + 1, Input_desc_boxDim, Input_desc_elementStrides, Input_desc_interleave, Input_desc_swizzle, Input_desc_l2Promotion, Input_desc_oobFill);

	if (Input_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor Input_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}

	CUtensorMap Output_desc;
	CUtensorMapDataType Output_desc_type= (CUtensorMapDataType)9;
	cuuint32_t Output_desc_tensorRank= 2;
	void *Output_desc_globalAddress= Output;
	cuuint64_t Output_desc_globalDim[2]= {1152,8};
	cuuint64_t Output_desc_globalStride[2]= {2,2304};
	cuuint32_t Output_desc_boxDim[2]= {64,128};
	cuuint32_t Output_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave Output_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle Output_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion Output_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill Output_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult Output_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &Output_desc, Output_desc_type, Output_desc_tensorRank, Output_desc_globalAddress, Output_desc_globalDim, Output_desc_globalStride + 1, Output_desc_boxDim, Output_desc_elementStrides, Output_desc_interleave, Output_desc_swizzle, Output_desc_l2Promotion, Output_desc_oobFill);

	if (Output_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor Output_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}

	CUtensorMap W_T_desc;
	CUtensorMapDataType W_T_desc_type= (CUtensorMapDataType)9;
	cuuint32_t W_T_desc_tensorRank= 2;
	void *W_T_desc_globalAddress= W_T;
	cuuint64_t W_T_desc_globalDim[2]= {1536,1152};
	cuuint64_t W_T_desc_globalStride[2]= {2,3072};
	cuuint32_t W_T_desc_boxDim[2]= {64,128};
	cuuint32_t W_T_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave W_T_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle W_T_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion W_T_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill W_T_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult W_T_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &W_T_desc, W_T_desc_type, W_T_desc_tensorRank, W_T_desc_globalAddress, W_T_desc_globalDim, W_T_desc_globalStride + 1, W_T_desc_boxDim, W_T_desc_elementStrides, W_T_desc_interleave, W_T_desc_swizzle, W_T_desc_l2Promotion, W_T_desc_oobFill);

	if (W_T_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor W_T_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}
	main_kernel<<<dim3(1, 1, 1), dim3(256, 1, 1), 295424, stream>>>(Input_desc, Output_desc, W_T_desc);
	TILELANG_CHECK_LAST_ERROR("main_kernel");

	return 0;
}

2025-10-20 06:33:27,449 DEBUG:Compilation failed for config {'block_M': 128, 'block_K': 128, 'block_N': 128, 'num_stages': 4, 'threads': 256} at index 31 with error: Initialization failed: Failed to set the allowed dynamic shared memory size to 295936 with error: invalid argument
#include <tl_templates/cuda/gemm.h>
#include <tl_templates/cuda/copy.h>
#include <tl_templates/cuda/reduce.h>
#include <tl_templates/cuda/ldsm.h>
#include <tl_templates/cuda/threadblock_swizzle.h>
#include <tl_templates/cuda/debug.h>
#ifdef ENABLE_BF16
#include <tl_templates/cuda/cuda_bf16_fallbacks.cuh>
#endif

extern "C" __global__ void main_kernel(__grid_constant__ const CUtensorMap Input_desc, __grid_constant__ const CUtensorMap Output_desc, __grid_constant__ const CUtensorMap W_T_desc);
extern "C" __global__ void __launch_bounds__(384, 1) main_kernel(__grid_constant__ const CUtensorMap Input_desc, __grid_constant__ const CUtensorMap Output_desc, __grid_constant__ const CUtensorMap W_T_desc) {
  extern __shared__ __align__(1024) uchar buf_dyn_shmem[];
  float output_reg[64];
  float squared_reg[64];
  float sum_reg[8];
  __shared__ uint64_t mbarrier_mem[8];
  auto mbarrier = reinterpret_cast<Barrier*>(mbarrier_mem);
  if (tl::tl_shuffle_elect<0>()) {
    tl::prefetch_tma_descriptor(Input_desc);
    tl::prefetch_tma_descriptor(W_T_desc);
    tl::prefetch_tma_descriptor(Output_desc);
    mbarrier[0].init(128);
    mbarrier[1].init(128);
    mbarrier[2].init(128);
    mbarrier[3].init(128);
    mbarrier[4].init(256);
    mbarrier[5].init(256);
    mbarrier[6].init(256);
    mbarrier[7].init(256);
  }
  __syncthreads();
  if (256 <= ((int)threadIdx.x)) {
    for (int w = 0; w < 9; ++w) {
      for (int k = 0; k < 12; ++k) {
        mbarrier[((k & 3) + 4)].wait(((((w * 3) + (k >> 2)) & 1) ^ 1));
        if (tl::tl_shuffle_elect<128>()) {
          mbarrier[(k & 3)].expect_transaction(32768);
          tl::tma_load(Input_desc, mbarrier[(k & 3)], (&(((bfloat16_t*)buf_dyn_shmem)[((k & 3) * 16384)])), (k * 128), 0);
          tl::tma_load(Input_desc, mbarrier[(k & 3)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k & 3) * 16384) + 8192)])), ((k * 128) + 64), 0);
          mbarrier[(k & 3)].expect_transaction(32768);
          tl::tma_load(W_T_desc, mbarrier[(k & 3)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k & 3) * 16384) + 65536)])), (k * 128), (w * 128));
          tl::tma_load(W_T_desc, mbarrier[(k & 3)], (&(((bfloat16_t*)buf_dyn_shmem)[(((k & 3) * 16384) + 73728)])), ((k * 128) + 64), (w * 128));
        }
        mbarrier[(k & 3)].arrive();
      }
    }
  } else {
    for (int w_1 = 0; w_1 < 9; ++w_1) {
      #pragma unroll
      for (int i = 0; i < 32; ++i) {
        *(float2*)(output_reg + (i * 2)) = make_float2(0x0p+0f/*0.000000e+00*/, 0x0p+0f/*0.000000e+00*/);
      }
      tl::fence_proxy_async();
      for (int k_1 = 0; k_1 < 12; ++k_1) {
        mbarrier[(k_1 & 3)].wait((((w_1 * 3) + (k_1 >> 2)) & 1));
        tl::gemm_ss<128, 128, 128, 2, 4, 0, 1, 0, 128, 128, 0, 0>((&(((bfloat16_t*)buf_dyn_shmem)[((k_1 & 3) * 16384)])), (&(((bfloat16_t*)buf_dyn_shmem)[(((k_1 & 3) * 16384) + 65536)])), (&(output_reg[0])));
        mbarrier[((k_1 & 3) + 4)].arrive();
      }
      #pragma unroll
      for (int i_1 = 0; i_1 < 64; ++i_1) {
        output_reg[i_1] = (output_reg[i_1] / (0x1p+0f/*1.000000e+00*/ + expf((output_reg[i_1] * -0x1p+0f/*-1.000000e+00*/))));
      }
      #pragma unroll
      for (int i_2 = 0; i_2 < 64; ++i_2) {
        squared_reg[i_2] = (output_reg[i_2] * output_reg[i_2]);
      }
      tl::__sync_thread_partial<3, 256>();
      #pragma unroll
      for (int i_3 = 0; i_3 < 8; ++i_3) {
        sum_reg[i_3] = 0x0p+0f/*0.000000e+00*/;
        #pragma unroll
        for (int rv = 0; rv < 8; ++rv) {
          sum_reg[i_3] = (sum_reg[i_3] + squared_reg[((((rv & 3) * 16) + (i_3 * 2)) + (rv >> 2))]);
        }
        sum_reg[i_3] = tl::AllReduce<tl::SumOp, 256, 64, 0>::run(sum_reg[i_3], (&(((float*)buf_dyn_shmem)[73728])));
        sum_reg[i_3] = tl::AllReduce<tl::SumOp, 4, 1, 0>::run(sum_reg[i_3]);
      }
      #pragma unroll
      for (int i_4 = 0; i_4 < 64; ++i_4) {
        output_reg[i_4] = (output_reg[i_4] * rsqrtf(sum_reg[((i_4 & 15) >> 1)]));
      }
      #pragma unroll
      for (int i_5 = 0; i_5 < 8; ++i_5) {
        tl::ptx_stmatrix_x4((&(((bfloat16_t*)buf_dyn_shmem)[((((((((((i_5 >> 2) * 8192) + ((i_5 & 1) * 4096)) + (((((int)threadIdx.x) & 31) >> 4) * 2048)) + (((((int)threadIdx.x) & 63) >> 5) * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + (((((((int)threadIdx.x) & 7) >> 2) + ((i_5 & 3) >> 1)) & 1) * 32)) + ((((((int)threadIdx.x) >> 7) + ((((int)threadIdx.x) & 3) >> 1)) & 1) * 16)) + (((((((int)threadIdx.x) & 127) >> 6) + (((int)threadIdx.x) & 1)) & 1) * 8)) + 131072)])), __pack_half2(((bfloat16_t)output_reg[(i_5 * 8)]), ((bfloat16_t)output_reg[((i_5 * 8) + 1)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 2)]), ((bfloat16_t)output_reg[((i_5 * 8) + 3)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 4)]), ((bfloat16_t)output_reg[((i_5 * 8) + 5)])), __pack_half2(((bfloat16_t)output_reg[((i_5 * 8) + 6)]), ((bfloat16_t)output_reg[((i_5 * 8) + 7)])));
      }
      tl::fence_proxy_async();
      tl::__sync_thread_partial<3, 256>();
      if (tl::tl_shuffle_elect<256>()) {
        tl::tma_store(Output_desc, (&(((bfloat16_t*)buf_dyn_shmem)[131072])), (w_1 * 128), 0);
        tl::tma_store_arrive();
        tl::tma_store_wait<0>();
        tl::tma_store(Output_desc, (&(((bfloat16_t*)buf_dyn_shmem)[139264])), ((w_1 * 128) + 64), 0);
        tl::tma_store_arrive();
        tl::tma_store_wait<0>();
      }
    }
  }
}


#define ERROR_BUF_SIZE 1024
static char error_buf[ERROR_BUF_SIZE];

extern "C" const char* get_last_error() {
    return error_buf;
}

extern "C" int init() {
    error_buf[0] = '\0';
    
    cudaError_t result_main_kernel = cudaFuncSetAttribute(main_kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, 295936);
    if (result_main_kernel != CUDA_SUCCESS) {
        snprintf(error_buf, ERROR_BUF_SIZE, "Failed to set the allowed dynamic shared memory size to %d with error: %s", 295936, cudaGetErrorString(result_main_kernel));
        return -1;
    }

    return 0;
}

extern "C" int call(bfloat16_t* __restrict__ Input, bfloat16_t* __restrict__ W_T, bfloat16_t* __restrict__ Output, cudaStream_t stream=cudaStreamDefault) {

	CUtensorMap Input_desc;
	CUtensorMapDataType Input_desc_type= (CUtensorMapDataType)9;
	cuuint32_t Input_desc_tensorRank= 2;
	void *Input_desc_globalAddress= Input;
	cuuint64_t Input_desc_globalDim[2]= {1536,8};
	cuuint64_t Input_desc_globalStride[2]= {2,3072};
	cuuint32_t Input_desc_boxDim[2]= {64,128};
	cuuint32_t Input_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave Input_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle Input_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion Input_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill Input_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult Input_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &Input_desc, Input_desc_type, Input_desc_tensorRank, Input_desc_globalAddress, Input_desc_globalDim, Input_desc_globalStride + 1, Input_desc_boxDim, Input_desc_elementStrides, Input_desc_interleave, Input_desc_swizzle, Input_desc_l2Promotion, Input_desc_oobFill);

	if (Input_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor Input_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}

	CUtensorMap Output_desc;
	CUtensorMapDataType Output_desc_type= (CUtensorMapDataType)9;
	cuuint32_t Output_desc_tensorRank= 2;
	void *Output_desc_globalAddress= Output;
	cuuint64_t Output_desc_globalDim[2]= {1152,8};
	cuuint64_t Output_desc_globalStride[2]= {2,2304};
	cuuint32_t Output_desc_boxDim[2]= {64,128};
	cuuint32_t Output_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave Output_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle Output_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion Output_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill Output_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult Output_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &Output_desc, Output_desc_type, Output_desc_tensorRank, Output_desc_globalAddress, Output_desc_globalDim, Output_desc_globalStride + 1, Output_desc_boxDim, Output_desc_elementStrides, Output_desc_interleave, Output_desc_swizzle, Output_desc_l2Promotion, Output_desc_oobFill);

	if (Output_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor Output_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}

	CUtensorMap W_T_desc;
	CUtensorMapDataType W_T_desc_type= (CUtensorMapDataType)9;
	cuuint32_t W_T_desc_tensorRank= 2;
	void *W_T_desc_globalAddress= W_T;
	cuuint64_t W_T_desc_globalDim[2]= {1536,1152};
	cuuint64_t W_T_desc_globalStride[2]= {2,3072};
	cuuint32_t W_T_desc_boxDim[2]= {64,128};
	cuuint32_t W_T_desc_elementStrides[2]= {1,1};
	CUtensorMapInterleave W_T_desc_interleave= (CUtensorMapInterleave)0;
	CUtensorMapSwizzle W_T_desc_swizzle= (CUtensorMapSwizzle)3;
	CUtensorMapL2promotion W_T_desc_l2Promotion= (CUtensorMapL2promotion)2;
	CUtensorMapFloatOOBfill W_T_desc_oobFill= (CUtensorMapFloatOOBfill)0;

	CUresult W_T_desc_result = CUTLASS_CUDA_DRIVER_WRAPPER_CALL(cuTensorMapEncodeTiled)(
    &W_T_desc, W_T_desc_type, W_T_desc_tensorRank, W_T_desc_globalAddress, W_T_desc_globalDim, W_T_desc_globalStride + 1, W_T_desc_boxDim, W_T_desc_elementStrides, W_T_desc_interleave, W_T_desc_swizzle, W_T_desc_l2Promotion, W_T_desc_oobFill);

	if (W_T_desc_result != CUDA_SUCCESS) {
		std::stringstream ss;
		ss << "Error: Failed to initialize the TMA descriptor W_T_desc";
		snprintf(error_buf, ERROR_BUF_SIZE, "%s", ss.str().c_str());
		return -1;
	}
	main_kernel<<<dim3(1, 1, 1), dim3(384, 1, 1), 295936, stream>>>(Input_desc, Output_desc, W_T_desc);
	TILELANG_CHECK_LAST_ERROR("main_kernel");

	return 0;
}

