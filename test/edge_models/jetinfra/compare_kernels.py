import torch
import tilelang
from tilelang.autotuner import autotune
import tilelang.language as T
import itertools
from typing import Optional, Tuple
import time
import tilelang as tl

print("æ­£åœ¨å¯¼å…¥kernels...")

# ============================================================================
# Kernel 1: ä» tl_fusedrecurrent.py (edgeç‰ˆæœ¬)
# ============================================================================
@tilelang.jit(
    pass_configs={
        tilelang.PassConfigKey.TL_DISABLE_TMA_LOWER: True,
        tilelang.PassConfigKey.TL_DISABLE_WARP_SPECIALIZED: True,
    }
)
def fused_recurrent_edge(
    Batch, 
    Token, 
    Head, 
    Head_V, 
    K_Dim, 
    V_Dim, 
    USE_QK_L2NORM_IN_KERNEL=True, 
    STORE_FINAL_STATE=True,
    dtype = "bfloat16",
    accum_dtype = "float32",
    block_K=128, 
    block_V=128, 
    num_stages=2, 
    threads=128
):
    @T.macro
    def L2Norm_QK(
        QK: T.SharedBuffer([block_K],accum_dtype),
    ):
        shared_reg = T.alloc_fragment([block_K], accum_dtype)
        squared_reg = T.alloc_fragment([block_K], accum_dtype)
        sum_reg = T.alloc_fragment([1], accum_dtype)
        T.copy(QK, shared_reg)
        
        # è®¡ç®—å…ƒç´ çš„å¹³æ–¹ç”¨äºæ±‚norm
        for i in T.Parallel(block_K):
            squared_reg[i] = shared_reg[i] * shared_reg[i]
        T.reduce_sum(squared_reg, sum_reg, dim=0)
        sum_reg[0] = T.sqrt(sum_reg[0]) + 1e-6
        
        # ç”¨åŸå§‹å…ƒç´ é™¤ä»¥norm
        for i in T.Parallel(block_K):
            shared_reg[i] = shared_reg[i] / sum_reg[0]
        T.copy(shared_reg, QK)

    @T.prim_func
    def main(
        Q: T.Tensor([Batch, Token, Head, K_Dim], dtype),
        K: T.Tensor([Batch, Token, Head, K_Dim], dtype),
        V: T.Tensor([Batch, Token, Head_V, V_Dim], dtype),
        g: T.Tensor([Batch, Token, Head_V], accum_dtype),
        Beta: T.Tensor([Batch, Token, Head_V],dtype),
        O: T.Tensor([Batch, Token, Head_V, V_Dim], dtype),
        h0: T.Tensor([Batch, Head_V, K_Dim, V_Dim], accum_dtype),
        scale: T.Tensor([1], dtype),
    ):
        with T.Kernel(T.ceildiv(K_Dim, block_K), T.ceildiv(V_Dim, block_V), T.ceildiv(Batch*Head_V, 1), threads=threads) as (bx, by, bz):
            id_b = bz // Head_V
            id_hv = bz % Head_V
            id_h = id_hv // (Head_V // Head)
            Q_shared = T.alloc_shared([block_K], dtype)
            Q_fragment = T.alloc_fragment([block_K], dtype)
            K_shared = T.alloc_shared([block_K], dtype)
            V_shared = T.alloc_shared([block_V], dtype)
            V_fragment = T.alloc_fragment([block_V], dtype)
            h0_shared = T.alloc_shared([block_K, block_V], accum_dtype)
            o_shared = T.alloc_shared([block_V], dtype)
            
            h0_fragment = T.alloc_fragment([block_K, block_V], accum_dtype)
            K_fragment = T.alloc_fragment([block_K], accum_dtype)
            v_min_reg = T.alloc_fragment([block_V], accum_dtype)

            
            T.copy(h0[id_b, id_hv, bx * block_K, by * block_V], h0_shared)
  
            for t in T.serial(Token):
                T.copy(Q[id_b, t, id_h, bx * block_K], Q_shared)
                T.copy(K[id_b, t, id_h, bx * block_K], K_shared)
                T.copy(V[id_b, t, id_hv, by * block_V], V_shared)

                T.copy(Q_shared, Q_fragment)
                T.copy(K_shared, K_fragment)
                T.copy(V_shared, V_fragment)
                T.copy(h0_shared, h0_fragment)

                for i in T.Parallel(block_K):
                    Q_fragment[i] = Q_fragment[i] * scale[0]

                for i, j in T.Parallel(block_K, block_V):
                    h0_fragment[i, j] = h0_fragment[i, j] * T.exp(g[id_b, t, id_hv])
                
                T.copy(h0_fragment, h0_shared)
                for i, j in T.Parallel(block_K, block_V):
                    h0_fragment[i, j] = h0_fragment[i, j] * K_fragment[i]
                T.reduce_sum(h0_fragment, v_min_reg, dim=0)
                for i in T.Parallel(block_V):
                    V_fragment[i] = V_fragment[i] - v_min_reg[i]
                
                for i in T.Parallel(block_V):
                    V_fragment[i] = V_fragment[i] * Beta[id_b, t, id_hv]
                
                T.copy(h0_shared, h0_fragment)
                for i, j in T.Parallel(block_K, block_V):
                    h0_fragment[i, j] = h0_fragment[i, j] + K_fragment[i] * V_fragment[j]
                
                T.copy(h0_fragment, h0_shared)
                for i, j in T.Parallel(block_K, block_V):
                    h0_fragment[i, j] = h0_fragment[i, j] * Q_fragment[i]
                
                T.reduce_sum(h0_fragment, v_min_reg, dim=0)
                T.copy(v_min_reg, o_shared)
                
                T.copy(o_shared, O[id_b, t, id_hv, by * block_V])
                

            if STORE_FINAL_STATE:
                T.copy(h0_shared, h0[id_b, id_hv, bx * block_K, by * block_V])
        
    return main

# ============================================================================
# Kernel 2: ä» tl_fusedrecurrent_bf16.py (bf16ç‰ˆæœ¬)
# ============================================================================
@tilelang.jit(
    out_idx = [-1],
)
def fused_recurrent_bf16(
    Batch, 
    Token, 
    Head, 
    Head_V, 
    K_Dim, 
    V_Dim, 
    USE_QK_L2NORM_IN_KERNEL=True, 
    STORE_FINAL_STATE=True,
    dtype = "bfloat16",
    accum_dtype = "float32",
    block_K=128, 
    block_V=128, 
    num_stages=3, 
    threads=128
):
    @T.macro
    def L2Norm_QK(
        QK: T.FragmentBuffer([block_K],dtype),
    ):
        squared_reg = T.alloc_fragment([block_K], dtype)
        sum_reg = T.alloc_fragment([1], dtype)
        
        for i in T.Parallel(block_K):
            squared_reg[i] = QK[i] * QK[i]
        T.reduce_sum(squared_reg, sum_reg, dim=0)
        sum_reg[0] = T.sqrt(sum_reg[0]) + 1e-6
        
        for i in T.Parallel(block_K):
            QK[i] = QK[i] / sum_reg[0]


    @T.prim_func
    def main(
        Q: T.Tensor([Batch, Token, Head, K_Dim], dtype),
        K: T.Tensor([Batch, Token, Head, K_Dim], dtype),
        V: T.Tensor([Batch, Token, Head_V, V_Dim], dtype),
        g: T.Tensor([Batch, Token, Head_V], accum_dtype),
        Beta: T.Tensor([Batch, Token, Head_V],dtype),
        h0: T.Tensor([Batch, Head_V, K_Dim, V_Dim], accum_dtype),
        scale: T.Tensor([1], dtype),
        O: T.Tensor([Batch, Token, Head_V, V_Dim], dtype),
    ):
        with T.Kernel(T.ceildiv(K_Dim, block_K), T.ceildiv(V_Dim, block_V), T.ceildiv(Batch*Head_V, 1), threads=threads) as (bx, by, bz):
            id_b = bz // Head_V
            id_hv = bz % Head_V
            id_h = id_hv // (Head_V // Head)
            Q_shared = T.alloc_shared([block_K], dtype)
            Q_fragment = T.alloc_fragment([block_K], dtype)
            K_shared = T.alloc_shared([block_K], dtype)
            V_shared = T.alloc_shared([block_V], dtype)
            V_fragment = T.alloc_fragment([block_V], dtype)
            h0_shared = T.alloc_shared([block_K, block_V], accum_dtype)
            o_shared = T.alloc_shared([block_V], dtype)
            
            h0_fragment = T.alloc_fragment([block_K, block_V], accum_dtype)
            K_fragment = T.alloc_fragment([block_K], accum_dtype)
            v_min_reg = T.alloc_fragment([block_V], accum_dtype)
            
            T.copy(h0[id_b, id_hv, bx * block_K, by * block_V], h0_shared)
            for t in T.serial(Token):
                T.copy(Q[id_b, t, id_h, bx * block_K], Q_shared)
                T.copy(K[id_b, t, id_h, bx * block_K], K_shared)
                T.copy(V[id_b, t, id_hv, by * block_V], V_shared)
 
                T.copy(Q_shared, Q_fragment)
                T.copy(K_shared, K_fragment)
                T.copy(V_shared, V_fragment)
                T.copy(h0_shared, h0_fragment)
                
                for i in T.Parallel(block_K):
                    Q_fragment[i] = Q_fragment[i] * scale[0]
                
                for i, j in T.Parallel(block_K, block_V):
                    h0_fragment[i, j] = h0_fragment[i, j] * T.exp(g[id_b, t, id_hv])
                T.copy(h0_fragment, h0_shared)

                for i, j in T.Parallel(block_K, block_V):
                    h0_fragment[i, j] = h0_fragment[i, j] * K_fragment[i]
                T.reduce_sum(h0_fragment, v_min_reg, dim=0)
                for i in T.Parallel(block_V):
                    V_fragment[i] = V_fragment[i] - v_min_reg[i]
                
                for i in T.Parallel(block_V):
                    V_fragment[i] = V_fragment[i] * Beta[id_b, t, id_hv]
                
                T.copy(h0_shared, h0_fragment)
                for i, j in T.Parallel(block_K, block_V):
                    h0_fragment[i, j] = h0_fragment[i, j] + K_fragment[i] * V_fragment[j]
                
                T.copy(h0_fragment, h0_shared)
                for i, j in T.Parallel(block_K, block_V):
                    h0_fragment[i, j] = h0_fragment[i, j] * Q_fragment[i]
                
                T.reduce_sum(h0_fragment, v_min_reg, dim=0)
                T.copy(v_min_reg, o_shared)
                
                T.copy(o_shared, O[id_b, t, id_hv, by * block_V])
                

            if STORE_FINAL_STATE:
                T.copy(h0_shared, h0[id_b, id_hv, bx * block_K, by * block_V])
        
    return main


# ============================================================================
# è¾…åŠ©å‡½æ•°
# ============================================================================
def calculate_relative_errors(actual, expected, eps=1e-10):
    """
    è®¡ç®—å¹³å‡ç›¸å¯¹è¯¯å·®å’Œæœ€å¤§ç›¸å¯¹è¯¯å·®
    """
    actual_flat = actual.flatten()
    expected_flat = expected.flatten()
    
    abs_diff = torch.abs(actual_flat - expected_flat)
    expected_abs = torch.abs(expected_flat)
    
    non_zero_mask = expected_abs > eps
    relative_errors = torch.zeros_like(abs_diff)
    relative_errors[non_zero_mask] = abs_diff[non_zero_mask] / expected_abs[non_zero_mask]
    
    zero_mask = ~non_zero_mask
    actual_abs = torch.abs(actual_flat)
    relative_errors[zero_mask] = torch.where(
        actual_abs[zero_mask] <= eps, 
        torch.zeros_like(actual_abs[zero_mask]), 
        actual_abs[zero_mask]
    )
    
    mean_rel_error = torch.mean(relative_errors).item()
    max_rel_error = torch.max(relative_errors).item()
    
    return mean_rel_error, max_rel_error


# ============================================================================
# ä¸»æµ‹è¯•å‡½æ•°
# ============================================================================
def run_comparison():
    print("\n" + "="*80)
    print("å¼€å§‹å¯¹æ¯”ä¸¤ä¸ªKernelå®ç°")
    print("="*80)
    
    print("\nğŸ“‹ Kernelé…ç½®å¯¹æ¯”:")
    print("  ç›¸åŒç‚¹:")
    print("    - æ ¸å¿ƒè®¡ç®—é€»è¾‘å®Œå…¨ç›¸åŒ")
    print("    - accum_dtype: float32")
    print("    - h0ç±»å‹: float32 (accum_dtype)")
    print("    - h0_sharedç±»å‹: float32 (accum_dtype)")
    print("  å·®å¼‚ç‚¹:")
    print("    - è£…é¥°å™¨: Edgeä½¿ç”¨pass_configs, BF16ä½¿ç”¨out_idx=[-1]")
    print("    - å‚æ•°é¡ºåº: Edgeæ˜¯(Q,K,V,g,Beta,O,h0,scale), BF16æ˜¯(Q,K,V,g,Beta,h0,scale,O)")
    print("    - é»˜è®¤num_stages: Edge=2, BF16=3")
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    torch.cuda.manual_seed(42)
    torch.cuda.manual_seed_all(42)
    
    # æµ‹è¯•å‚æ•°
    B, Token, H, HV, K, V = 1, 10, 1, 1, 128, 128
    scale = K ** -0.5
    datatype = torch.bfloat16
    gdtype = torch.float32
    
    print(f"\næµ‹è¯•é…ç½®:")
    print(f"  Batch: {B}, Token: {Token}, Head: {H}, Head_V: {HV}")
    print(f"  K_Dim: {K}, V_Dim: {V}")
    print(f"  Scale: {scale:.6f}")
    print(f"  æ•°æ®ç±»å‹: {datatype}")
    print(f"  h0ç±»å‹: {gdtype} (ä¸¤ä¸ªkernelç»Ÿä¸€ä½¿ç”¨)")
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    q = torch.randn(B, Token, H, K, device='cuda', dtype=datatype)
    k = torch.randn(B, Token, H, K, device='cuda', dtype=datatype)
    v = torch.randn(B, Token, HV, V, device='cuda', dtype=datatype)
    g = torch.randn(B, Token, HV, device='cuda', dtype=gdtype).sigmoid()
    beta = torch.randn(B, Token, HV, device='cuda', dtype=datatype).sigmoid()
    
    # ä¸ºä¸¤ä¸ªkernelå‡†å¤‡ç›¸åŒé…ç½®çš„åˆå§‹çŠ¶æ€ï¼ˆéƒ½ä½¿ç”¨float32ï¼‰
    h0_edge = torch.zeros(B, HV, K, V, device='cuda', dtype=gdtype)
    h0_bf16 = torch.zeros(B, HV, K, V, device='cuda', dtype=gdtype)
    
    # é¢„åˆ†é…è¾“å‡ºå¼ é‡
    o_edge = torch.empty(B, Token, HV, V, device='cuda', dtype=datatype)
    o_bf16 = torch.empty(B, Token, HV, V, device='cuda', dtype=datatype)
    scale_tensor = torch.tensor([scale], device='cuda', dtype=datatype)
    
    # è¿è¡ŒEdgeç‰ˆæœ¬ kernel
    print("\n" + "-"*80)
    print("1. è¿è¡ŒEdgeç‰ˆæœ¬ Kernel (tl_fusedrecurrent.py)")
    print("-"*80)
    kernel_edge = fused_recurrent_edge(
        B, Token, H, HV, K, V, 
        USE_QK_L2NORM_IN_KERNEL=True, 
        STORE_FINAL_STATE=True,
        block_K=128, 
        block_V=128, 
        num_stages=2, 
        threads=128
    )
    
    torch.cuda.synchronize()
    start_edge = time.time()
    kernel_edge(q, k, v, g, beta, o_edge, h0_edge, scale_tensor)
    torch.cuda.synchronize()
    end_edge = time.time()
    
    print(f"  âœ“ æ‰§è¡Œå®Œæˆ")
    print(f"  æ‰§è¡Œæ—¶é—´: {(end_edge - start_edge) * 1000:.3f} ms")
    print(f"  è¾“å‡ºå½¢çŠ¶: {o_edge.shape}")
    print(f"  Final stateå½¢çŠ¶: {h0_edge.shape}")
    
    # è¿è¡ŒBF16ç‰ˆæœ¬ kernel
    print("\n" + "-"*80)
    print("2. è¿è¡ŒBF16ç‰ˆæœ¬ Kernel (tl_fusedrecurrent_bf16.py)")
    print("-"*80)
    kernel_bf16 = fused_recurrent_bf16(
        B, Token, H, HV, K, V, 
        USE_QK_L2NORM_IN_KERNEL=True, 
        STORE_FINAL_STATE=True,
        block_K=128, 
        block_V=128, 
        num_stages=3, 
        threads=128
    )
    
    torch.cuda.synchronize()
    start_bf16 = time.time()
    # æ³¨æ„ï¼šBF16ç‰ˆæœ¬å‚æ•°é¡ºåºæ˜¯ (q, k, v, g, beta, h0, scale) ä¸”Oé€šè¿‡è¿”å›å€¼
    o_bf16_result = kernel_bf16(q, k, v, g, beta, h0_bf16, scale_tensor)
    # å¦‚æœè¿”å›çš„æ˜¯å•ä¸ªå¼ é‡ï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦‚æœout_idxæœ‰æ•ˆï¼Œä¼šè¿”å›O
    o_bf16 = o_bf16_result if isinstance(o_bf16_result, torch.Tensor) else o_bf16_result
    torch.cuda.synchronize()
    end_bf16 = time.time()
    
    print(f"  âœ“ æ‰§è¡Œå®Œæˆ")
    print(f"  æ‰§è¡Œæ—¶é—´: {(end_bf16 - start_bf16) * 1000:.3f} ms")
    print(f"  è¾“å‡ºå½¢çŠ¶: {o_bf16.shape}")
    print(f"  Final stateå½¢çŠ¶: {h0_bf16.shape}")
    
    # å¯¹æ¯”ç»“æœ
    print("\n" + "="*80)
    print("ç»“æœå¯¹æ¯”")
    print("="*80)
    
    # è¾“å‡ºOçš„å¯¹æ¯”
    print("\nğŸ“Š è¾“å‡ºå¼ é‡ 'O' çš„è¯¯å·®åˆ†æ:")
    o_mean_err, o_max_err = calculate_relative_errors(o_edge, o_bf16)
    print(f"  å¹³å‡ç›¸å¯¹è¯¯å·®: {o_mean_err:.6e}")
    print(f"  æœ€å¤§ç›¸å¯¹è¯¯å·®: {o_max_err:.6e}")
    print(f"  ç»å¯¹è¯¯å·®ç»Ÿè®¡:")
    abs_diff_o = torch.abs(o_edge - o_bf16)
    print(f"    - å¹³å‡ç»å¯¹è¯¯å·®: {abs_diff_o.mean().item():.6e}")
    print(f"    - æœ€å¤§ç»å¯¹è¯¯å·®: {abs_diff_o.max().item():.6e}")
    print(f"    - æœ€å°ç»å¯¹è¯¯å·®: {abs_diff_o.min().item():.6e}")
    
    # Final stateçš„å¯¹æ¯” (ä¸¤è€…éƒ½æ˜¯float32ï¼Œæ— éœ€è½¬æ¢)
    print("\nğŸ“Š Final State 'h0' çš„è¯¯å·®åˆ†æ:")
    h0_mean_err, h0_max_err = calculate_relative_errors(
        h0_edge, h0_bf16
    )
    print(f"  å¹³å‡ç›¸å¯¹è¯¯å·®: {h0_mean_err:.6e}")
    print(f"  æœ€å¤§ç›¸å¯¹è¯¯å·®: {h0_max_err:.6e}")
    print(f"  ç»å¯¹è¯¯å·®ç»Ÿè®¡:")
    abs_diff_h0 = torch.abs(h0_edge - h0_bf16)
    print(f"    - å¹³å‡ç»å¯¹è¯¯å·®: {abs_diff_h0.mean().item():.6e}")
    print(f"    - æœ€å¤§ç»å¯¹è¯¯å·®: {abs_diff_h0.max().item():.6e}")
    print(f"    - æœ€å°ç»å¯¹è¯¯å·®: {abs_diff_h0.min().item():.6e}")
    
    # æ€§èƒ½å¯¹æ¯”
    print("\nâš¡ æ€§èƒ½å¯¹æ¯”:")
    time_edge_ms = (end_edge - start_edge) * 1000
    time_bf16_ms = (end_bf16 - start_bf16) * 1000
    speedup = time_edge_ms / time_bf16_ms
    print(f"  Edgeç‰ˆæœ¬: {time_edge_ms:.3f} ms")
    print(f"  BF16ç‰ˆæœ¬: {time_bf16_ms:.3f} ms")
    print(f"  é€Ÿåº¦æ¯”: {speedup:.2f}x {'(BF16æ›´å¿«)' if speedup > 1 else '(Edgeæ›´å¿«)'}")
    
    # åˆ¤æ–­æ˜¯å¦é€šè¿‡
    print("\n" + "="*80)
    print("æµ‹è¯•ç»“è®º")
    print("="*80)
    
    tolerance = 1e-3
    o_passed = o_mean_err < tolerance
    h0_passed = h0_mean_err < tolerance
    
    print(f"\nè¾“å‡º 'O' è¯¯å·®æµ‹è¯•: {'âœ“ é€šè¿‡' if o_passed else 'âœ— æœªé€šè¿‡'} (é˜ˆå€¼: {tolerance:.1e})")
    print(f"Final State 'h0' è¯¯å·®æµ‹è¯•: {'âœ“ é€šè¿‡' if h0_passed else 'âœ— æœªé€šè¿‡'} (é˜ˆå€¼: {tolerance:.1e})")
    
    if o_passed and h0_passed:
        print("\nğŸ‰ ä¸¤ä¸ªKernelå®ç°ç»“æœä¸€è‡´!")
    else:
        print("\nâš ï¸  ä¸¤ä¸ªKernelå®ç°å­˜åœ¨å·®å¼‚ï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥")
    
    print("\n" + "="*80)


if __name__ == '__main__':
    run_comparison()

